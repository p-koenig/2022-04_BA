{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span></li><li><span><a href=\"#Specification-of-Experiment-Settings\" data-toc-modified-id=\"Specification-of-Experiment-Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Specification of Experiment Settings</a></span></li><li><span><a href=\"#Data-Import-(from-Polynomial-Generation)\" data-toc-modified-id=\"Data-Import-(from-Polynomial-Generation)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Import (from Polynomial Generation)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generation-of-all-possible-Monomials\" data-toc-modified-id=\"Generation-of-all-possible-Monomials-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Generation of all possible Monomials</a></span></li></ul></li><li><span><a href=\"#Lambda-Network-Training-+-Weigh/Bias-saving\" data-toc-modified-id=\"Lambda-Network-Training-+-Weigh/Bias-saving-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Lambda Network Training + Weigh/Bias saving</a></span></li><li><span><a href=\"#Save-Lambda-Model-scores\" data-toc-modified-id=\"Save-Lambda-Model-scores-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save Lambda-Model scores</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "\n",
    "# Importing own helper library\n",
    "import sys\n",
    "sys.path.insert(0,'../_baselib')\n",
    "import general_helper as gh\n",
    "import polynom_helper as ph\n",
    "import lambdanet_helper as lh\n",
    "import net_helper as nh\n",
    "\n",
    "# Third-party imports\n",
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm.notebook import tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import random \n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import shutil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "#from livelossplot.keras import PlotLossesCallback\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# sympy for Polynomial support\n",
    "from sympy.polys import monomials\n",
    "from sympy.polys.orderings import monomial_key\n",
    "from sympy import symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static settings & directory preparation\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse settings from a configuration file ('config.ini')\n",
    "\n",
    "config_path = 'config.ini'\n",
    "\n",
    "try:\n",
    "    config = gh.parse_config(config_path)\n",
    "    locals().update(config)\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Parsing not possible due to Exception:')\n",
    "    print(e)\n",
    "    print('\\nContinue with manual specification.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual override of config and/or additional variable specification\n",
    "\n",
    "#d = 3  \n",
    "#n = 4\n",
    "#sparsity = 0.0\n",
    "#a_max = 10 \n",
    "#a_min = -10\n",
    "#a_step = 0.1\n",
    "#x_max = 1 \n",
    "#x_min = -1\n",
    "#x_step = 0.01\n",
    "#lambda_dataset_size = 100 \n",
    "#interpretation_dataset_size = 5000\n",
    "#same_training_all_polynomials = True\n",
    "\n",
    "n_jobs = -3\n",
    "\n",
    "# - lambda net specifications \n",
    "#batch_size = 64\n",
    "#epochs = 200\n",
    "#dropout = 0.0\n",
    "#optimizer='SGD'\n",
    "#each_epochs_save = 5\n",
    "\n",
    "# how many lambda nets should be generated (must be smaller or equal to interpretation_dataset_size)\n",
    "number_of_lambda_nets = interpretation_dataset_size\n",
    "\n",
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "\n",
    "# set derived attributes\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "if same_training_all_polynomials: \n",
    "    training_string = '_same'\n",
    "else: \n",
    "    training_string = '_diverse'\n",
    "\n",
    "if number_of_lambda_nets < interpretation_dataset_size:\n",
    "    generate_subset = True\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    generate_subset = False\n",
    "    data_size = interpretation_dataset_size   \n",
    "\n",
    "seed_method = True\n",
    "shuffle = True\n",
    "\n",
    "lambda_network_layers = [5 * gh.nCr(n+d, d)]\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleSeedMethod'\n",
    "elif not seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleNoSeedMethod'\n",
    "elif seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleSeedMethod'\n",
    "elif not seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleNoSeedMethod'\n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: 4 (abcd)\n",
      "Degree: 3\n",
      "Sparsity: 0.0%\n",
      "Lambda-Net Dataset Size: 1000\n",
      "I-Net Dataset Size: 1000\n",
      "Number of Lambda-Nets to Train: 1000\n",
      "Coefficient Range: [-10.0, 10.0]\n",
      "Variable Range: [-1.0, 1.0]\n",
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# inspect most important settings\n",
    "print('Variables: ' + str(n) + ' (' + variables + ')')\n",
    "print('Degree: ' + str(d))\n",
    "print('Sparsity: ' + str(sparsity*100) + '%') \n",
    "print('Lambda-Net Dataset Size: ' + str(lambda_dataset_size))\n",
    "print('I-Net Dataset Size: ' + str(interpretation_dataset_size))\n",
    "print('Number of Lambda-Nets to Train: ' + str(number_of_lambda_nets))\n",
    "      \n",
    "print('Coefficient Range: ' + '[' + str(a_min) + ', ' + str(a_max) + ']')\n",
    "print('Variable Range: ' + '[' + str(x_min) + ', ' + str(x_max) + ']')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "\n",
    "except FileExistsError:\n",
    "\n",
    "    if each_epochs_save != None:\n",
    "        for i in range(epochs//each_epochs_save):    \n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate() \n",
    "        \n",
    "    else:\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()\n",
    "\n",
    "gh.create_dir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def calcualate_function_with_data(coefficient_list, variable_values):\n",
    "#    \n",
    "#    global list_of_monomial_identifiers\n",
    "#    \n",
    "#    result = 0    \n",
    "#    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "#        partial_results = [variable_value**int(coefficient_multiplier) for coefficient_multiplier, variable_value in zip(coefficient_multipliers, variable_values)]\n",
    "#        \n",
    "#        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "#\n",
    "#    return result, variable_values\n",
    "# \n",
    "#def calculate_function_values_from_polynomial(true_value_test, evaluation_dataset):\n",
    "#\n",
    "#    if isinstance(true_value_test, pd.DataFrame):\n",
    "#        true_value_test = true_value_test.values\n",
    "#        \n",
    "#    true_value_fv = []\n",
    "#    true_value_coeff = []\n",
    "#        \n",
    "#    for evaluation in evaluation_dataset:\n",
    "#        true_function_value, true_coeff = calcualate_function_with_data(true_value_test, evaluation)\n",
    "#       \n",
    "#        true_value_fv.append(true_function_value) \n",
    "#        true_value_coeff.append(true_coeff)\n",
    "#        \n",
    "#    return np.array(true_value_coeff), np.array(true_value_fv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import (from Polynomial Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv imports\n",
    "path_polynomials_df = './data/saved_polynomial_lists/polynomials_df' + str(interpretation_dataset_size) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_df = gh.import_csv(path_polynomials_df)\n",
    "\n",
    "# Pickle imports\n",
    "path_polynomials_poly = './data/saved_polynomial_lists/polynomials_poly' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "polynomials_poly = gh.import_pickle(path_polynomials_poly)\n",
    "\n",
    "path_x_data = './data/saved_polynomial_lists/x_data' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "x_data = gh.import_pickle(path_x_data)\n",
    "\n",
    "path_y_data = './data/saved_polynomial_lists/y_data' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "y_data = gh.import_pickle(path_y_data)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_df = polynomials_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    x_data = random.sample(x_data, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data = random.sample(y_data, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Poly}{\\left( 0.2 a^{3} + 7.9 a^{2}b - 0.8 a^{2}c - 8.6 a^{2}d + 0.6 a^{2} - 2.9 ab^{2} + 8.8 abc - 8.0 abd + 0.2 ab + 2.1 ac^{2} - 2.6 acd - 1.3 ac + 1.6 ad^{2} - 0.1 ad + 0.3 a + 5.1 b^{3} + 3.0 b^{2}c + 4.9 b^{2}d - 4.8 b^{2} - 9.9 bc^{2} - 1.3 bcd + 5.7 bc - 6.3 bd^{2} + 2.9 bd + 9.1 b + 8.7 c^{3} - 8.0 c^{2}d + 6.0 c^{2} - 4.3 cd^{2} - 7.9 cd - 1.2 c - 5.2 d^{3} - 4.2 d^{2} + 6.9 d + 8.7, a, b, c, d, domain=\\mathbb{R} \\right)}$"
      ],
      "text/plain": [
       "Poly(0.2*a**3 + 7.9*a**2*b - 0.8*a**2*c - 8.6*a**2*d + 0.6*a**2 - 2.9*a*b**2 + 8.8*a*b*c - 8.0*a*b*d + 0.2*a*b + 2.1*a*c**2 - 2.6*a*c*d - 1.3*a*c + 1.6*a*d**2 - 0.1*a*d + 0.3*a + 5.1*b**3 + 3.0*b**2*c + 4.9*b**2*d - 4.8*b**2 - 9.9*b*c**2 - 1.3*b*c*d + 5.7*b*c - 6.3*b*d**2 + 2.9*b*d + 9.1*b + 8.7*c**3 - 8.0*c**2*d + 6.0*c**2 - 4.3*c*d**2 - 7.9*c*d - 1.2*c - 5.2*d**3 - 4.2*d**2 + 6.9*d + 8.7, a, b, c, d, domain='RR')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first polynomial of polynomials_poly\n",
    "polynomials_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.83, -0.43,  0.25, -0.28],\n",
       "       [ 0.09, -0.49, -0.55,  0.06],\n",
       "       [-0.88, -0.05, -0.84, -0.34],\n",
       "       ...,\n",
       "       [ 0.81,  0.12, -0.06, -0.23],\n",
       "       [ 0.92,  0.41,  0.04, -0.96],\n",
       "       [-0.06,  0.89,  0.28, -0.38]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first polynomial of x_data\n",
    "print(len(x_data[0]))\n",
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.4415906,   6.9130364,   7.0019939,  10.5460288,  15.0289468,\n",
       "         5.646763 ,   9.4060745,  26.26049  ,  -4.8021455,   7.0725501,\n",
       "         5.4122616,   4.7483147,   7.0366017,   0.0867052,  -8.0660826,\n",
       "         9.2829133,   5.7399021,  10.794185 ,   1.0960959,  -1.3450906,\n",
       "         6.8613714,  10.0378557,   4.1962761,  -5.6566008,   7.9067859,\n",
       "         3.5438344,   8.4100291,  12.9146392,  11.5057987,   7.945932 ,\n",
       "        20.3225657,   9.9106444,  17.6447399,  -5.9592127,   2.2235144,\n",
       "        14.0435376,  10.2109272,  -7.6108826,   0.2295006,   6.8622349,\n",
       "        12.7483371,   3.9355735,   5.6035846,   3.7557299,   5.900184 ,\n",
       "        23.8544613,   4.1100242,  -2.817271 ,  12.2000539,   1.2312174,\n",
       "        14.402697 ,  13.436746 ,  21.6348467,   1.8498805,   9.7133557,\n",
       "         1.8532793,   7.6079616,   5.5528012,   5.2736006,  17.9144739,\n",
       "        -1.630911 ,  -4.9461232,   9.832325 ,   3.3299857,  -5.8970591,\n",
       "         5.7383622,  11.1197398,  11.2604663,  -0.3710134,  -0.8284461,\n",
       "         2.875776 ,   5.976978 ,   5.4951404,   6.3506081,  -4.9842364,\n",
       "        -0.7791271,  -0.53065  ,  10.5266033,  14.7741921,  11.1867672,\n",
       "        11.9572581,   7.1466226,  17.5450293,   9.1138844,  17.4205245,\n",
       "        11.0659758,   1.7007959,  -9.3578637,  16.3618824,  -1.7411325,\n",
       "        10.9329025,   5.4695135,   8.6289889,  17.4112925,  13.2595586,\n",
       "         3.1380292,   5.9128488,   7.9662005,  11.9247015,  -5.9953936,\n",
       "        18.1939708,  10.8112129,   7.8771018,  10.7472934,   5.8150311,\n",
       "        -5.0305293,   5.8019366,  15.5026885,  17.458645 ,  11.2766174,\n",
       "        17.257916 ,  11.6425425,   6.2209784,   6.4820744,  12.776344 ,\n",
       "        -9.9805256,   0.6330205,   7.9510826,  13.5984779,  25.9716349,\n",
       "         9.2132498,  18.6997337,   8.851791 ,  11.6342041,   3.3945813,\n",
       "         6.0317568,   9.0348407,  19.3899413,   1.4689731,   7.4390366,\n",
       "       -10.8395482,  13.2254668,  -2.6806776,   8.2703012,  -3.6320269,\n",
       "        10.7192162,  11.1012773,   9.6649381,  13.3016201,  13.2820706,\n",
       "        10.0239095,   3.3054375,   8.8757819,  14.2370028,  12.8923416,\n",
       "        14.6512886,   4.913091 ,   4.7269747,  10.4735913,  10.3472989,\n",
       "         7.5771405,  15.4702261,   9.3855857,  13.8930886,  10.0504193,\n",
       "       -15.0006665,  15.3744516,  18.0175871,  21.7544738,  11.9603008,\n",
       "        -0.14097  ,  12.8438606,   2.9281283,  13.6832691,  23.5955992,\n",
       "         6.1694107,  15.4728372,  -0.9657942,  -2.5254064,  25.2110155,\n",
       "        15.4417647,  18.0252948,   4.9062522,  19.0225454,   8.7242679,\n",
       "         0.7100143,   3.7384265,   4.4097033,  -1.299195 ,  11.5679897,\n",
       "         7.0495145,   1.3641594,   9.7756664,   2.4941546,  13.9857256,\n",
       "        26.713083 ,   8.8848384,  11.6603511,   7.0880224,  18.8775313,\n",
       "         0.3466615,  16.709371 ,   3.7445376,   7.9147755,   4.559338 ,\n",
       "         9.7678499,   5.6903829,  -3.4504363,  10.8396313,   6.376339 ,\n",
       "         6.1883388,   0.6919192,  12.9049503,  15.3206687,  13.1281182,\n",
       "         5.6919143,   9.2062652,   3.480233 , -18.6587351,  -2.7068333,\n",
       "         9.5734822,  -0.2311628,   2.0920556,  14.280035 , -10.6208179,\n",
       "        -9.1840463,  12.0211037,   3.659634 ,   9.4211063,  11.7993272,\n",
       "        11.0634465,   1.0225773,  21.4520032,   9.7300715,  14.4702083,\n",
       "        16.6674744,  15.9460616,  13.8139264,  -0.0166628,   7.8534099,\n",
       "         9.436448 ,  -1.1019393,  -4.8875578,   8.566791 ,  22.233416 ,\n",
       "         9.3185968,  20.7835167,   8.8695194,  15.6995336,   5.1220251,\n",
       "        12.8233058,  -0.8497146,   1.7706759,  -4.0502705,  13.7963244,\n",
       "         2.863657 ,  14.8976513,  14.1766863,  18.8949834,  -8.3521536,\n",
       "         7.1739984,   3.2363387,   5.8023473,  -8.5480863,  -3.1924072,\n",
       "        -2.4906521,   0.463116 ,   7.6230296,   9.0058376,  -6.1698669,\n",
       "         5.8769389,  10.9110943,  11.2152629,   9.547703 ,  15.9071689,\n",
       "        13.1152106,   3.5764577,  13.0205094,  13.1902649,  14.804597 ,\n",
       "        12.5862206,  17.1445576,  10.5812743,  14.2410851,   7.1178368,\n",
       "        15.1563036,   0.4792789,  11.7027183,  12.7576424,  -1.4566576,\n",
       "        13.349618 ,   9.6220841,  -8.4777144,  23.8319439,  13.6989813,\n",
       "        -4.9902495,  10.2491238,  16.4939011,  13.9303721,   7.9585968,\n",
       "        13.1825695,   3.3902219,  13.2177373,   5.4160777,  18.2789273,\n",
       "        22.6515005,  14.4046147,  11.0099121,  13.3631535,   1.0616825,\n",
       "        20.9634788,   8.2879262,   6.1918081,   3.9152618,  16.7276639,\n",
       "         8.189478 ,  10.5961187,   4.8067255,   0.6239293,  15.7962572,\n",
       "         1.8728145,   3.7221097,  27.8795546,  31.4995932,  -1.8826941,\n",
       "        13.6970656,  21.570536 ,   4.7001528,  14.8458338,   5.330681 ,\n",
       "         3.9598352,   5.2163176,  12.43649  ,   7.6039966,   1.7801034,\n",
       "         2.3294934,   8.3582942,  11.6585749,  11.29792  , -16.8735379,\n",
       "         6.8235295,  -0.6131784,   8.0137959,   2.0532604,   7.1973248,\n",
       "       -10.7880063,  17.931519 ,  12.7780853,   9.9847071,  -1.8317971,\n",
       "        -3.4529141,   9.1993238,  27.6030843,  -2.9265246, -17.9116811,\n",
       "        12.0524374,  11.1484766,  -4.0008509,   5.5072788,  -8.2602173,\n",
       "        -5.542344 ,  21.6368922,   2.6195201,   6.9395913,  15.1206368,\n",
       "        12.3521077,  11.5822327,  12.0537535,  16.8248054,  19.4297619,\n",
       "        19.002039 ,  18.1757467,  22.097005 ,   6.3294699,  14.2899272,\n",
       "        10.9985578,   6.2340016,  -7.7180822,   8.5511882,  15.1693448,\n",
       "         7.8976153,  10.2670976,   9.4732224,   4.7641418,  13.8232103,\n",
       "        24.266497 ,  -2.6639419,  11.1760403,   2.3644041,  -5.1998944,\n",
       "         4.1511905,  19.2307738,  20.8158816,   8.7596291,   3.4032368,\n",
       "         6.6745553,   4.9527181,   6.0032097,  19.1196075,   3.5482343,\n",
       "         8.4097273,  12.0765196,   6.3951127,   6.2594758,  12.5901945,\n",
       "         0.3267344,  11.2615424,   7.765487 ,  -0.3726834,  10.9162894,\n",
       "         3.683719 ,   6.9626512,  11.6587449,  -5.1791647,   6.2864223,\n",
       "         5.2696927,  13.7219328,   1.8994604,  -4.3536074,   5.3842148,\n",
       "         1.5728105,  19.0161827,  15.6916679,  -0.7009921,   8.8916488,\n",
       "         1.868484 ,   9.0645148,   2.3892855,   9.009335 ,  -3.8074549,\n",
       "         8.5604896,  12.1133733,   5.6237633,   6.3516668,   8.9814342,\n",
       "        15.0228127,  16.7565399,   8.7176951,  26.6673069,  22.3980684,\n",
       "         2.4468724,  21.4363642,  18.6467907,  11.6577899,  -4.1100354,\n",
       "        16.9835443,  11.0033745,  -2.8742794,   3.134027 ,   5.6145825,\n",
       "         3.0438984,   5.3048921,   7.8595176,  -3.8587656,  12.4368848,\n",
       "         0.0901914,   3.9961349,  16.4208937,   7.2718028,   6.79581  ,\n",
       "        14.6614101,  14.6102544,   6.3996619,  10.8532755,  -8.1132257,\n",
       "         8.6698362,  13.0310195,  13.6518941,   9.3900264,  17.0205539,\n",
       "        14.5820647,   7.7977157,  -3.1127147,  21.0240591, -13.1600879,\n",
       "         7.4358844,  12.9485813,  28.0295785,   6.170044 ,  20.0185948,\n",
       "         5.4726434,   2.2698309,   4.5141759,  11.9234397,  12.4276895,\n",
       "         9.3803702,   7.7832752,   6.7433379,   9.498572 ,  19.6161647,\n",
       "        16.8333111, -12.9324352,  21.007803 ,  10.4375615,   3.2066324,\n",
       "         0.1775037,   6.8220248,  14.8953096,   7.2635536,   7.2676406,\n",
       "         5.6997508,   9.6430806,  13.2144   ,   3.5482157,   6.7453062,\n",
       "        12.936049 ,  26.4952469,  -6.3106967,  -6.7952908,  18.0603945,\n",
       "        -6.8441707,   4.2090962,  23.980008 ,  13.1501372, -12.6825059,\n",
       "        10.5249355,   5.9306872,   8.3325957,  13.6588121,  14.9043759,\n",
       "        11.4522038,   7.3220388,  16.7525708,   9.6070706,   9.73134  ,\n",
       "        11.8395896,   4.5621914,  12.1675094,   4.3065485,   1.4081963,\n",
       "         8.7025743,   1.7784389,   3.2525151,   3.1716476, -15.9604191,\n",
       "         2.7853447,  34.2029808,  10.7105905,  10.1960893,   8.228315 ,\n",
       "         4.4452628,  25.3726613,  17.4311676,   6.0720883,  -5.4144051,\n",
       "        19.2970663,   9.99812  ,  11.2547551,  -3.0560431,   7.0635779,\n",
       "         7.8396568,  16.2780447,   9.4494457,  10.2671197, -12.7152493,\n",
       "        10.7992499, -12.3347345,  21.0073758,   0.4620157,   9.2726655,\n",
       "        -1.280812 ,  13.3246621,  37.0577099,   2.1845509,  -5.8190659,\n",
       "         4.764633 ,  -0.851232 ,   1.909437 ,  16.0211378,  10.8255271,\n",
       "         3.7321208,  12.1427897,  -2.4356907,  16.9102101,  -0.5735777,\n",
       "        -4.5683848,   1.9880505,  -0.193984 ,  14.3347531,  10.3596075,\n",
       "        11.3404302,   4.6239624,  -2.6939286,  15.794084 ,  23.3042601,\n",
       "         1.3841888,  10.9569848,  12.1256931,  12.1333035,  -1.6134915,\n",
       "        13.2702332,  16.987889 ,   6.9070735,  14.8677124,  -0.904475 ,\n",
       "         8.1565321,  -6.1203536,  45.6523785,   2.8869131,  -4.4706008,\n",
       "       -10.88437  ,   3.7446786,  10.5103495,  -0.6922048, -12.5852387,\n",
       "        -1.0617511,   5.1246417,   4.3310608,  14.4008172,  -1.4724238,\n",
       "         3.7516409,   0.6623361,   9.5438881,  -2.9488991,  27.6540552,\n",
       "        -1.3641727,   2.8817954,  11.9281304,  22.6213082,  -0.8583968,\n",
       "         3.6437917,   9.4216713,   4.7563187,  22.0489348,  20.6825718,\n",
       "         7.7255147,  10.0328927,   3.0055273,  -3.4124414,  10.0229538,\n",
       "        -7.0793463,  13.4784181,   1.4604722,  15.6294212,   3.8339458,\n",
       "         5.6630311,  -1.7822899,   1.6211613,  -4.7637269,  -1.2513464,\n",
       "         4.6145502,  -3.6307432,  -7.5994245,   1.9626244,  13.5511137,\n",
       "         6.3157493,   8.4527254, -17.8828552,  12.8628445,  25.2107005,\n",
       "         8.3563502,   4.1937005,  26.7864938,   0.755857 ,   8.644224 ,\n",
       "        20.7300362,   5.1300322,  11.6202854,  -4.1194975,   7.6841978,\n",
       "         3.8330682,   7.9445329,   3.0957192,  12.5468754,  25.6846934,\n",
       "        14.4629462,  10.1390466,   2.0308405,   3.6213319,  12.9991915,\n",
       "        -0.2081535,   8.7263631,  -2.8450992,  -5.6520871,   7.8296119,\n",
       "         4.9639439,   8.672808 ,  12.2028607,   1.831608 ,  -0.8869135,\n",
       "         6.1676856,   7.5084836,   7.1588855,   9.3593729,   1.985648 ,\n",
       "         5.942855 ,  22.1802105,  -6.4258196,   2.8013363,  14.1340736,\n",
       "        -3.6037656,  27.6047836,  11.9844019,   2.7826518,  12.3533892,\n",
       "         3.8762558,   7.5814494,  15.4888022,   3.340016 ,  11.3071007,\n",
       "        12.1270914, -10.1353792,  17.6382697,   3.7539607, -10.9506706,\n",
       "         5.9944466,   3.9673109,   9.4541627,   6.4077695,  19.4183513,\n",
       "        12.4754877,   9.6055747,   5.343289 ,  15.0879144,   3.8265343,\n",
       "         9.8312105, -10.2218787,  -4.8644557,   9.549461 ,   6.5617339,\n",
       "        11.9934522,   2.3069305,  19.5258189,  21.0216237,   5.0604648,\n",
       "         5.0479294,  12.1110212,  15.3963214,   4.1449716,  13.4216875,\n",
       "        17.1212078,  19.4307104,  -7.0189901,  10.150348 ,  15.2296761,\n",
       "        20.5418618,  15.6050351,  15.2797976,  -1.6502446,  14.5371216,\n",
       "        14.7099215,  14.8160341,  17.0823867,   9.5749257,  -2.252475 ,\n",
       "        11.9643639,   4.5808254,   8.3098921,  -0.2808973,  19.759692 ,\n",
       "         4.5101733,  12.2605891,   5.1565153,  11.5161509,  13.8040952,\n",
       "        15.1623709,   6.4236192,   5.9230976,  27.515228 ,   2.5439391,\n",
       "        12.3380799,   7.0887369,   5.3035898,  -3.2024645,   6.0394829,\n",
       "        10.5910643,  11.6547581,  22.3679009,   3.5531869,   4.537917 ,\n",
       "         5.1668893,  11.2277259, -11.7271526,   3.9339576,  12.7435901,\n",
       "         3.1019222,  10.2234606,   1.9781574,  20.0502115,  14.5938968,\n",
       "         6.0918596,   9.6335702,  20.5599846,   3.1705158,   7.9189692,\n",
       "         0.2024328,  10.2030644,  -5.1741073,   5.9205612, -16.0741225,\n",
       "         4.8473057,   0.1089967,   7.9104108,  19.1914418,   2.3573575,\n",
       "       -15.9028872,  23.135432 ,   7.3888868,  12.1566495,   9.2896238,\n",
       "         6.7065891,   4.4115837,   4.5784566,  -6.0121159,   6.815881 ,\n",
       "        -6.7776465,  19.1666672,  24.1973084, -12.4274498,  12.6222688,\n",
       "        10.1534422,  -0.1709376,  15.9474711,   1.5338222,  10.2732729,\n",
       "        20.4770365,   2.1874134,  27.4870024,  10.5006003,  11.5608102,\n",
       "         4.9687245,  17.9210158,   5.3135593, -12.8306013,  15.6368338,\n",
       "        10.127599 ,  16.8895609,   9.3360237,   8.6175379,  20.1540421,\n",
       "        14.8130609,   5.9028521,  -7.5593104,   6.4044427,  25.3244731,\n",
       "        11.5957438,  17.3551053,   4.2845508,   6.310771 ,  12.6912659,\n",
       "         2.048516 ,  15.5541619,  -0.6143098, -14.1630149,  15.436701 ,\n",
       "        16.2917437,  10.586588 ,  26.7211958,   2.456275 ,   5.7937296,\n",
       "        18.6851645,   8.8864372,  -1.3947323,  10.9659705,   6.8741261,\n",
       "         1.1567601,  12.8345263,  13.2214079,  17.2268056, -13.1875797,\n",
       "         6.4203841,  17.3311237,  12.2951685,  -5.0015836,  15.5201874,\n",
       "         1.3082979,  -9.5254788,  11.3620144,  21.6294584,  21.733796 ,\n",
       "        -2.5787454,  14.0715687,  -3.8895622, -13.3678037,  17.497401 ,\n",
       "        -4.9093572,  21.4733264,  23.3920771,   7.9005855,  -0.0175111,\n",
       "        15.6752875,   5.4437926,   8.6243103,  -2.5123897,   4.6968558,\n",
       "         0.0070898,  -2.2601122,  20.2859368,  14.1458848,   7.3705635,\n",
       "         2.8545093,   5.4702615,  10.8605248,   7.4247758,   6.7305488,\n",
       "        14.7043255,  17.6299235,  23.5389102,   9.6404247,  15.1172073,\n",
       "         2.0991816,  14.9279141,   5.6697611,   4.0415224,   3.2078792,\n",
       "         3.7547889,   3.3587191,  18.7441132,   3.2579974,   7.0430527,\n",
       "        -0.2860729,  -3.8175599, -11.183837 ,   9.343536 ,   0.2887511,\n",
       "         7.7839425,   8.6936982,  -0.9655119,  -3.2417219,  20.297274 ,\n",
       "        12.8917428,   3.9700832,  14.8746934,  10.6846876,   6.8391702,\n",
       "        -7.785132 ,   4.6156431,   1.0997951,   0.970701 ,  14.5798041,\n",
       "        12.1611251,  10.2013437,  -6.3326442, -11.8277756,  -1.64503  ,\n",
       "        13.9694046,  15.8292288,   9.2265568,  -1.041687 ,   3.2549407,\n",
       "        18.8884511,   8.8218269,  10.6079985,   7.1083575,   3.6788615,\n",
       "        -8.6396668,  -4.3551014,   9.5376912,   4.3795645,  -6.7963398,\n",
       "        15.2720083,  -5.1276214,  17.5599715,  14.0741404,   3.6107273,\n",
       "         5.2685479,   9.4404077,  10.3001231,   7.4924752,   9.9031887,\n",
       "         9.191865 ,  11.216944 ,  20.2498712,   1.6743763,   7.2717753,\n",
       "        -6.7044481,  13.9403296,   5.637155 ,   4.5413864,   6.5911943,\n",
       "         2.6993208,   0.5698547,  13.211694 ,   8.8288308,   8.7707763,\n",
       "        21.5192495,   0.1827257,  19.6890884,  21.7523998,   2.6281487,\n",
       "        11.1677305,  17.4045192,  13.0069755,   9.2118611,  -5.0272667,\n",
       "        15.6282995,  14.4487748,   7.1893114,  13.492492 ,   3.9946202,\n",
       "         1.3198358,  -7.4452852,   3.1764791,  13.4744539,  13.4209536,\n",
       "        14.7757935,  15.4408811,  16.4443552,   6.7211838,   0.9593165,\n",
       "         4.2861866,   2.791661 ,  16.1509542,   6.439314 ,   7.3717192,\n",
       "        11.0468798,  13.3361702,  10.737191 ,  16.8062631,  12.9651819])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first polynomial of y_data\n",
    "print(len(y_data[0]))\n",
    "y_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of all possible Monomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "[a**3, a**2*b, a**2*c, a**2*d, a**2, a*b**2, a*b*c, a*b*d, a*b, a*c**2, a*c*d, a*c, a*d**2, a*d, a, b**3, b**2*c, b**2*d, b**2, b*c**2, b*c*d, b*c, b*d**2, b*d, b, c**3, c**2*d, c**2, c*d**2, c*d, c, d**3, d**2, d, 1]\n",
      "[(3, 0, 0, 0), (2, 1, 0, 0), (2, 0, 1, 0), (2, 0, 0, 1), (2, 0, 0, 0), (1, 2, 0, 0), (1, 1, 1, 0), (1, 1, 0, 1), (1, 1, 0, 0), (1, 0, 2, 0), (1, 0, 1, 1), (1, 0, 1, 0), (1, 0, 0, 2), (1, 0, 0, 1), (1, 0, 0, 0), (0, 3, 0, 0), (0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 0, 0), (0, 1, 2, 0), (0, 1, 1, 1), (0, 1, 1, 0), (0, 1, 0, 2), (0, 1, 0, 1), (0, 1, 0, 0), (0, 0, 3, 0), (0, 0, 2, 1), (0, 0, 2, 0), (0, 0, 1, 2), (0, 0, 1, 1), (0, 0, 1, 0), (0, 0, 0, 3), (0, 0, 0, 2), (0, 0, 0, 1), (0, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "polys_symbols = symbols(list(variables))\n",
    "polys_monomials = sorted(monomials.itermonomials(variables = polys_symbols, max_degrees = d, min_degrees = None), \n",
    "                                      reverse = True, key = monomial_key('lex', polys_symbols))\n",
    "monomials_count = monomials.monomial_count(n, d)\n",
    "\n",
    "tuples_monomials = [ph.monomial_to_power_tuple(mon, polys_symbols) for mon in polys_monomials]\n",
    "\n",
    "print('List length: ' + str(len(polys_monomials)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(monomials_count))\n",
    "print(polys_monomials)\n",
    "print(tuples_monomials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_lambda_net(x_data, y_data, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    # Preparation\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data = X_data.values\n",
    "    if isinstance(y_data, pd.DataFrame):\n",
    "        y_data = y_data.values\n",
    "    \n",
    "    # Split x_data and y_data into Train, Test, and Validation\n",
    "    data_dict = nh.split_input_data(x_data, y_data, train_ratio = 0.7, valid_ratio = 0.15, test_ratio = 0.15)\n",
    "  \n",
    "    # Create lambda neural network structure\n",
    "    model = lh.create_net_model(lambda_network_layers = lambda_network_layers, \n",
    "                                input_neurons = x_data.shape[1], \n",
    "                                dropout = dropout, \n",
    "                                optimizer = optimizer, \n",
    "                                loss_function = 'mae', #huber_loss(val_min, val_max), #'mape',#'mean_absolute_error',#root_mean_squared_error \n",
    "                                metrics = [lh.mean_absolute_percentage_error_keras, lh.root_mean_squared_error])\n",
    "    \n",
    "    weights = []\n",
    "    polynomial_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "     \n",
    "    if each_epochs_save == None:\n",
    "        \n",
    "        # training of NN\n",
    "        model_history = model.fit(data_dict['x_train'], \n",
    "                                  data_dict['y_train'], \n",
    "                                  epochs=epochs, \n",
    "                                  batch_size=batch_size, \n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=(data_dict['x_valid'], data_dict['y_valid']),\n",
    "                                  verbose=0,\n",
    "                                  workers=0)\n",
    "        \n",
    "        weights.append(model.get_weights())\n",
    "        history = model_history.history\n",
    "        \n",
    "        # predict input validation and test values\n",
    "        data_dict['y_pred_valid'] = model.predict(data_dict['x_valid'])                \n",
    "        data_dict['y_pred_test'] = model.predict(data_dict['x_test'])\n",
    "\n",
    "        # ????????????\n",
    "        term_list_all = []\n",
    "        y = 0\n",
    "        for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "            term_list = [int(value_mult) for value_mult in term]\n",
    "            term_list_all.append(term_list)\n",
    "\n",
    "        #print(trm_list_all)\n",
    "\n",
    "        #generate separate arrays for each variable combination\n",
    "        terms_matrix = []\n",
    "        for unknowns in data_dict['x_valid']:\n",
    "            terms = []\n",
    "            for term_multipliers in term_list_all:\n",
    "                term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                terms.append(term_value)\n",
    "            terms_matrix.append(np.array(terms))\n",
    "\n",
    "        terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "        polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_pred_list.append(polynomial_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_test_polynomial = []\n",
    "        y_test_lstsq = []\n",
    "        for entry in X_test:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            \n",
    "            y_test_polynomial.append(true_function_value_pred)\n",
    "            y_test_lstsq.append(true_function_value_lstsq)\n",
    "        y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "        y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "        y_valid_polynomial = []  \n",
    "        y_valid_lstsq = []\n",
    "        for entry in X_valid:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_valid_polynomial.append(true_function_value_pred)     \n",
    "            y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "        y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "        y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)     \n",
    "\n",
    "        pred_list = (y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test)\n",
    "\n",
    "        # TO BE ADJUSTED \n",
    "        # data_dict not fully filled\n",
    "        result_dict_list = []\n",
    "        result_dict_list.append(lh.calc_metrics(data_dict, advanced_size))         \n",
    "                            \n",
    "    else:\n",
    "        result_dict_list = []\n",
    "        pred_list = []\n",
    "        for i in range(epochs//each_epochs_save):\n",
    "            if i == 0:\n",
    "                history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "                history = history.history\n",
    "            else:\n",
    "                model_history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "\n",
    "                for key_1 in history.keys():\n",
    "                    for key_2 in model_history.history.keys():\n",
    "                        if key_1 == key_2:\n",
    "                            history[key_1] += model_history.history[key_2]  \n",
    "            \n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)                \n",
    "            y_pred_test = model.predict(X_test)        \n",
    "\n",
    "            term_list_all = []\n",
    "            y = 0\n",
    "            for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "                term_list = [int(value_mult) for value_mult in term]\n",
    "                term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "            terms_matrix = []\n",
    "            for unknowns in X_valid:\n",
    "                terms = []\n",
    "                for term_multipliers in term_list_all:\n",
    "                    term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                    terms.append(term_value)\n",
    "                terms_matrix.append(np.array(terms))\n",
    "\n",
    "            terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "            polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_pred_list.append(polynomial_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            y_test_polynomial = []\n",
    "            if i == 0:\n",
    "                y_test_lstsq = []\n",
    "            for entry in X_test:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_test_polynomial.append(true_function_value_pred)\n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_test_lstsq.append(true_function_value_lstsq)\n",
    "\n",
    "            y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "            if i == 0:\n",
    "                y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "            y_valid_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = []\n",
    "            for entry in X_valid:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_valid_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "            y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)    \n",
    "                \n",
    "            pred_list.append((y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test))\n",
    "\n",
    "            \n",
    "            mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "            mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "            mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "            rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "            rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "            mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "            mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "            r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "            r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "            r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "            raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "            raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "            rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "            rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "            dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "            dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "            if i == 0:\n",
    "                dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "            mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "            mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "            mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "            rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "            rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "            mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "            mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "            r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "            r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "            raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "            raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "            rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "            rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "            if i == 0:\n",
    "                rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "\n",
    "            fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "            dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "            dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "            std_train_real = np.std(y_train)\n",
    "            std_valid_real = np.std(y_valid)\n",
    "            std_test_real = np.std(y_test)\n",
    "\n",
    "            std_valid_pred = np.std(y_pred_valid)\n",
    "            std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "            std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "            std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "                std_test_lstsq = np.std(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            mean_train_real = np.mean(y_train)\n",
    "            mean_valid_real = np.mean(y_valid)\n",
    "            mean_test_real = np.mean(y_test)\n",
    "\n",
    "            mean_valid_pred = np.mean(y_pred_valid)\n",
    "            mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "            mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "            mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "                mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            result_dict_list_single_epoch =  [{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]        \n",
    "                  \n",
    "            result_dict_list.append(result_dict_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save != None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, #polynomial_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute lambda-net training (and evaluation) in parallel over chunks of functions\n",
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "x_data_chunks = list(gh.chunks(x_data, chunksize))\n",
    "y_data_chunks = list(gh.chunks(y_data, chunksize))\n",
    "\n",
    "for x_data_chunk, y_data_chunk in tqdm(zip(x_data_chunks, y_data_chunks), total=max(len(x_data_chunks), len(y_data_chunks))):\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_lambda_net)(x_dat, y_dat, x_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for x_dat, y_dat in zip(x_data_chunk, y_data_chunk))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random lambda-net with prints\n",
    "# \n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_lambda_net(x_data[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesCallback(skip_first=0)], return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    dict_list_valid = [clf[1][0] for clf in clf_list]\n",
    "    dict_list_test = [clf[1][1] for clf in clf_list]\n",
    "    dict_list_stds = [clf[1][2] for clf in clf_list]\n",
    "    dict_list_means = [clf[1][3] for clf in clf_list]\n",
    "\n",
    "    dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_dict_list_by_epochs = [[] for i in range(epochs//each_epochs_save)]\n",
    "    for scores_dict_list in scores_list:   \n",
    "        for index, scores_dict in enumerate(scores_dict_list):\n",
    "            scores_dict_list_by_epochs[index].append(scores_dict)\n",
    "            \n",
    "        \n",
    "    for index, scores_dict_list_single_epoch in enumerate(scores_dict_list_by_epochs):\n",
    "        index = (index+1)*each_epochs_save\n",
    "        dict_list_valid = [dict_list[0] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_test = [dict_list[1] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_stds = [dict_list[2] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_means = [dict_list[3] for dict_list in scores_dict_list_single_epoch]\n",
    "        \n",
    "        dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()  \n",
    "        dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1*each_epochs_save:\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data = [clf[2] for clf in clf_list]\n",
    "\n",
    "    pred_evaluation_dataset_valid_real = pred_evaluation_dataset_valid_real.ravel()\n",
    "    pred_evaluation_dataset_test_real = pred_evaluation_dataset_test_real.ravel()\n",
    "    pred_evaluation_dataset_valid = pred_evaluation_dataset_valid.ravel()\n",
    "    pred_evaluation_dataset_test = pred_evaluation_dataset_test.ravel()\n",
    "    pred_evaluation_dataset_valid_polynomial = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "    pred_evaluation_dataset_test_polynomial = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial, columns=[str(test_data) for test_data in X_test_data])    \n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "    pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "    pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "       \n",
    "    path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]))) for i in range(epochs//each_epochs_save)]\n",
    "    \n",
    "    for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data) in enumerate(pred_evaluation_dataset_per_epoch):\n",
    "            pred_evaluation_dataset_valid_real_list[index][i] = pred_evaluation_dataset_valid_real.ravel()\n",
    "            pred_evaluation_dataset_valid_list[index][i] = pred_evaluation_dataset_valid.ravel()\n",
    "            pred_evaluation_dataset_valid_polynomial_list[index][i] = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_test_real_list[index][i] = pred_evaluation_dataset_test_real.ravel()\n",
    "            pred_evaluation_dataset_test_list[index][i] = pred_evaluation_dataset_test.ravel()\n",
    "            pred_evaluation_dataset_test_polynomial_list[index][i] = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    for index, (pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n",
    "        pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])  \n",
    "        pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial_by_epoch, columns=[str(test_data) for test_data in X_test_data])   \n",
    "        \n",
    "        pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "        pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "        pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "\n",
    "        path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)         \n",
    "        pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)    \n",
    "        pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_real_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable_values = []\n",
    "#for column in pred_evaluation_dataset_df.columns:\n",
    "#    variable_values.append(np.array(column[1:-1].split()).astype('float'))\n",
    "#variable_values = np.array(variable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv_with_vv = []\n",
    "#for function_values in tqdm(pred_evaluation_dataset_df.values):\n",
    "#    fv_with_vv.append(np.array([np.append(vv, fv) for fv, vv in zip(function_values, variable_values)]))\n",
    "#fv_with_vv = np.array(fv_with_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 760.85,
   "position": {
    "height": "782.85px",
    "left": "1409px",
    "right": "20px",
    "top": "120px",
    "width": "251px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
