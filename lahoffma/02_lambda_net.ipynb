{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:10.645942Z",
     "start_time": "2020-09-17T08:06:10.638221Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:10.667841Z",
     "start_time": "2020-09-17T08:06:10.655021Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 50000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 5  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:10.690437Z",
     "start_time": "2020-09-17T08:06:10.670592Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "same_training_all_polynomials = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "seed_method = True\n",
    "shuffle = True\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_polynomials:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleSeedMethod'\n",
    "elif not seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleNoSeedMethod'\n",
    "elif seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleSeedMethod'\n",
    "elif not seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleNoSeedMethod'\n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.829500Z",
     "start_time": "2020-09-17T08:06:10.694428Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.841998Z",
     "start_time": "2020-09-17T08:06:51.833227Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.858891Z",
     "start_time": "2020-09-17T08:06:51.844799Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.872001Z",
     "start_time": "2020-09-17T08:06:51.861463Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcualate_function_with_data(coefficient_list, variable_values):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [variable_value**int(coefficient_multiplier) for coefficient_multiplier, variable_value in zip(coefficient_multipliers, variable_values)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "\n",
    "    return result, variable_values\n",
    " \n",
    "def calculate_function_values_from_polynomial(true_value_test, evaluation_dataset):\n",
    "\n",
    "    if isinstance(true_value_test, pd.DataFrame):\n",
    "        true_value_test = true_value_test.values\n",
    "        \n",
    "    true_value_fv = []\n",
    "    true_value_coeff = []\n",
    "        \n",
    "    for evaluation in evaluation_dataset:\n",
    "        true_function_value, true_coeff = calcualate_function_with_data(true_value_test, evaluation)\n",
    "       \n",
    "        true_value_fv.append(true_function_value) \n",
    "        true_value_coeff.append(true_coeff)\n",
    "        \n",
    "    return np.array(true_value_coeff), np.array(true_value_fv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.921165Z",
     "start_time": "2020-09-17T08:06:51.874824Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.015141Z",
     "start_time": "2020-09-17T08:06:51.925042Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= n:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.469880Z",
     "start_time": "2020-09-17T08:06:52.018648Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.702250Z",
     "start_time": "2020-09-17T08:06:52.472447Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        for i in range(epochs//each_epochs_save):    \n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.725867Z",
     "start_time": "2020-09-17T08:06:52.706076Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.737995Z",
     "start_time": "2020-09-17T08:06:52.728292Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.746904Z",
     "start_time": "2020-09-17T08:06:52.740246Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.755585Z",
     "start_time": "2020-09-17T08:06:52.749478Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:53.085851Z",
     "start_time": "2020-09-17T08:06:52.758561Z"
    },
    "code_folding": [
     32
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data, y_data, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data = X_data.values\n",
    "    if isinstance(y_data, pd.DataFrame):\n",
    "        y_data = y_data.values\n",
    "        \n",
    "    X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "            \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae', #huber_loss(val_min, val_max), #'mape',#'mean_absolute_error',#root_mean_squared_error,\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    weights = []\n",
    "    polynomial_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "        \n",
    "        \n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:        \n",
    "        model_history = model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "\n",
    "        y_pred_valid = model.predict(X_valid)                \n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        term_list_all = []\n",
    "        y = 0\n",
    "        for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "            term_list = [int(value_mult) for value_mult in term]\n",
    "            term_list_all.append(term_list)\n",
    "\n",
    "        #print(trm_list_all)\n",
    "\n",
    "        #generate separate arrays for each variable combination\n",
    "        terms_matrix = []\n",
    "        for unknowns in X_valid:\n",
    "            terms = []\n",
    "            for term_multipliers in term_list_all:\n",
    "                term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                terms.append(term_value)\n",
    "            terms_matrix.append(np.array(terms))\n",
    "\n",
    "        terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "        polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_pred_list.append(polynomial_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_test_polynomial = []\n",
    "        y_test_lstsq = []\n",
    "        for entry in X_test:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            \n",
    "            y_test_polynomial.append(true_function_value_pred)\n",
    "            y_test_lstsq.append(true_function_value_lstsq)\n",
    "        y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "        y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "        y_valid_polynomial = []  \n",
    "        y_valid_lstsq = []\n",
    "        for entry in X_valid:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_valid_polynomial.append(true_function_value_pred)     \n",
    "            y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "        y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "        y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)     \n",
    "\n",
    "        pred_list = (y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test)\n",
    "\n",
    "        mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "        mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "        mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "        mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "        rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "        rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "        rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "        rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "        \n",
    "        mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "        mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "        mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "        mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "        r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "        r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "        r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "        r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "        raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "        raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "        rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "        rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "        dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "        dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "        dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "        mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "        mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "        mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "        rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "        rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "        mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "        mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "        r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "        r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "        r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "        raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "        raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "        rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "        rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "        rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "        \n",
    "        fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "        dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "        dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "        dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "        \n",
    "        std_train_real = np.std(y_train)\n",
    "        std_valid_real = np.std(y_valid)\n",
    "        std_test_real = np.std(y_test)\n",
    "\n",
    "        std_valid_pred = np.std(y_pred_valid)\n",
    "        std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "        std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "        std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "        std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "        std_test_lstsq = np.std(y_test_lstsq)\n",
    "        \n",
    "        \n",
    "        mean_train_real = np.mean(y_train)\n",
    "        mean_valid_real = np.mean(y_valid)\n",
    "        mean_test_real = np.mean(y_test)\n",
    "\n",
    "        mean_valid_pred = np.mean(y_pred_valid)\n",
    "        mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "        mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "        mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "        mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "        mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "\n",
    "        result_dict_list =  [{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]            \n",
    "                            \n",
    "    else:\n",
    "        result_dict_list = []\n",
    "        pred_list = []\n",
    "        for i in range(epochs//each_epochs_save):\n",
    "            if i == 0:\n",
    "                history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "                history = history.history\n",
    "            else:\n",
    "                model_history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "\n",
    "                for key_1 in history.keys():\n",
    "                    for key_2 in model_history.history.keys():\n",
    "                        if key_1 == key_2:\n",
    "                            history[key_1] += model_history.history[key_2]  \n",
    "            \n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)                \n",
    "            y_pred_test = model.predict(X_test)        \n",
    "\n",
    "            term_list_all = []\n",
    "            y = 0\n",
    "            for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "                term_list = [int(value_mult) for value_mult in term]\n",
    "                term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "            terms_matrix = []\n",
    "            for unknowns in X_valid:\n",
    "                terms = []\n",
    "                for term_multipliers in term_list_all:\n",
    "                    term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                    terms.append(term_value)\n",
    "                terms_matrix.append(np.array(terms))\n",
    "\n",
    "            terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "            polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_pred_list.append(polynomial_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            y_test_polynomial = []\n",
    "            if i == 0:\n",
    "                y_test_lstsq = []\n",
    "            for entry in X_test:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_test_polynomial.append(true_function_value_pred)\n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_test_lstsq.append(true_function_value_lstsq)\n",
    "\n",
    "            y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "            if i == 0:\n",
    "                y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "            y_valid_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = []\n",
    "            for entry in X_valid:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_valid_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "            y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)    \n",
    "                \n",
    "            pred_list.append((y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test))\n",
    "\n",
    "            \n",
    "            mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "            mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "            mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "            rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "            rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "            mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "            mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "            r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "            r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "            r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "            raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "            raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "            rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "            rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "            dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "            dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "            if i == 0:\n",
    "                dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "            mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "            mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "            mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "            rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "            rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "            mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "            mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "            r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "            r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "            raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "            raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "            rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "            rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "            if i == 0:\n",
    "                rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "\n",
    "            fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "            dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "            dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "            std_train_real = np.std(y_train)\n",
    "            std_valid_real = np.std(y_valid)\n",
    "            std_test_real = np.std(y_test)\n",
    "\n",
    "            std_valid_pred = np.std(y_pred_valid)\n",
    "            std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "            std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "            std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "                std_test_lstsq = np.std(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            mean_train_real = np.mean(y_train)\n",
    "            mean_valid_real = np.mean(y_valid)\n",
    "            mean_test_real = np.mean(y_test)\n",
    "\n",
    "            mean_valid_pred = np.mean(y_pred_valid)\n",
    "            mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "            mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "            mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "                mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            result_dict_list_single_epoch =  [{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]        \n",
    "                  \n",
    "            result_dict_list.append(result_dict_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save != None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, #polynomial_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:02.580716Z",
     "start_time": "2020-09-17T08:06:53.088393Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesCallback(skip_first=0)], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.693701Z",
     "start_time": "2020-09-17T08:17:02.583223Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    dict_list_valid = [clf[1][0] for clf in clf_list]\n",
    "    dict_list_test = [clf[1][1] for clf in clf_list]\n",
    "    dict_list_stds = [clf[1][2] for clf in clf_list]\n",
    "    dict_list_means = [clf[1][3] for clf in clf_list]\n",
    "\n",
    "    dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_dict_list_by_epochs = [[] for i in range(epochs//each_epochs_save)]\n",
    "    for scores_dict_list in scores_list:   \n",
    "        for index, scores_dict in enumerate(scores_dict_list):\n",
    "            scores_dict_list_by_epochs[index].append(scores_dict)\n",
    "            \n",
    "        \n",
    "    for index, scores_dict_list_single_epoch in enumerate(scores_dict_list_by_epochs):\n",
    "        index = (index+1)*each_epochs_save\n",
    "        dict_list_valid = [dict_list[0] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_test = [dict_list[1] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_stds = [dict_list[2] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_means = [dict_list[3] for dict_list in scores_dict_list_single_epoch]\n",
    "        \n",
    "        dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()  \n",
    "        dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1*each_epochs_save:\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.723163Z",
     "start_time": "2020-09-17T08:17:03.696067Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.751338Z",
     "start_time": "2020-09-17T08:17:03.725279Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.779243Z",
     "start_time": "2020-09-17T08:17:03.753450Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.807209Z",
     "start_time": "2020-09-17T08:17:03.781408Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.168836Z",
     "start_time": "2020-09-17T08:17:03.809359Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data = [clf[2] for clf in clf_list]\n",
    "\n",
    "    pred_evaluation_dataset_valid_real = pred_evaluation_dataset_valid_real.ravel()\n",
    "    pred_evaluation_dataset_test_real = pred_evaluation_dataset_test_real.ravel()\n",
    "    pred_evaluation_dataset_valid = pred_evaluation_dataset_valid.ravel()\n",
    "    pred_evaluation_dataset_test = pred_evaluation_dataset_test.ravel()\n",
    "    pred_evaluation_dataset_valid_polynomial = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "    pred_evaluation_dataset_test_polynomial = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial, columns=[str(test_data) for test_data in X_test_data])    \n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "    pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "    pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "       \n",
    "    path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]))) for i in range(epochs//each_epochs_save)]\n",
    "    \n",
    "    for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data) in enumerate(pred_evaluation_dataset_per_epoch):\n",
    "            pred_evaluation_dataset_valid_real_list[index][i] = pred_evaluation_dataset_valid_real.ravel()\n",
    "            pred_evaluation_dataset_valid_list[index][i] = pred_evaluation_dataset_valid.ravel()\n",
    "            pred_evaluation_dataset_valid_polynomial_list[index][i] = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_test_real_list[index][i] = pred_evaluation_dataset_test_real.ravel()\n",
    "            pred_evaluation_dataset_test_list[index][i] = pred_evaluation_dataset_test.ravel()\n",
    "            pred_evaluation_dataset_test_polynomial_list[index][i] = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    for index, (pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n",
    "        pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])  \n",
    "        pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial_by_epoch, columns=[str(test_data) for test_data in X_test_data])   \n",
    "        \n",
    "        pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "        pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "        pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "\n",
    "        path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)         \n",
    "        pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)    \n",
    "        pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.209395Z",
     "start_time": "2020-09-17T08:18:00.176321Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_real_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.239323Z",
     "start_time": "2020-09-17T08:18:00.212545Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.268837Z",
     "start_time": "2020-09-17T08:18:00.241795Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.274581Z",
     "start_time": "2020-09-17T08:18:00.271355Z"
    }
   },
   "outputs": [],
   "source": [
    "#variable_values = []\n",
    "#for column in pred_evaluation_dataset_df.columns:\n",
    "#    variable_values.append(np.array(column[1:-1].split()).astype('float'))\n",
    "#variable_values = np.array(variable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.283212Z",
     "start_time": "2020-09-17T08:18:00.277813Z"
    }
   },
   "outputs": [],
   "source": [
    "#fv_with_vv = []\n",
    "#for function_values in tqdm(pred_evaluation_dataset_df.values):\n",
    "#    fv_with_vv.append(np.array([np.append(vv, fv) for fv, vv in zip(function_values, variable_values)]))\n",
    "#fv_with_vv = np.array(fv_with_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.532950Z",
     "start_time": "2020-09-17T08:18:00.285738Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:01.152123Z",
     "start_time": "2020-09-17T08:18:00.535436Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:02.003084Z",
     "start_time": "2020-09-17T08:18:01.154461Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:02.664470Z",
     "start_time": "2020-09-17T08:18:02.005271Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:03.331777Z",
     "start_time": "2020-09-17T08:18:02.666846Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:03.994633Z",
     "start_time": "2020-09-17T08:18:03.333795Z"
    }
   },
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:05.622225Z",
     "start_time": "2020-09-17T08:18:03.996958Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:07.191097Z",
     "start_time": "2020-09-17T08:18:05.624357Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
