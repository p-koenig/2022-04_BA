{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "\n",
    "# Importing own helper library\n",
    "import sys\n",
    "sys.path.insert(0,'../_baselib')\n",
    "import general_helper as gh\n",
    "import polynom_helper as ph\n",
    "import lambdanet_helper as lh\n",
    "\n",
    "# Third-party imports\n",
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm.notebook import tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import random \n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import shutil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# sympy for Polynomial support\n",
    "from sympy.polys import monomials\n",
    "from sympy.polys.orderings import monomial_key\n",
    "from sympy import symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static settings & directory preparation\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Specification of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse settings from a configuration file ('config.ini')\n",
    "\n",
    "config_path = 'config.ini'\n",
    "\n",
    "try:\n",
    "    config = gh.parse_config(config_path)\n",
    "    locals().update(config)\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Parsing not possible due to Exception:')\n",
    "    print(e)\n",
    "    print('\\nContinue with manual specification.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual override of config and/or additional variable specification\n",
    "\n",
    "#d = 3  \n",
    "#n = 4\n",
    "#sparsity = 0.0\n",
    "#a_max = 10 \n",
    "#a_min = -10\n",
    "#a_step = 0.1\n",
    "#x_max = 1 \n",
    "#x_min = -1\n",
    "#x_step = 0.01\n",
    "#lambda_dataset_size = 100 \n",
    "#interpretation_dataset_size = 5000\n",
    "#same_training_all_polynomials = True\n",
    "\n",
    "n_jobs = -3\n",
    "\n",
    "# - lambda net specifications \n",
    "#batch_size = 64\n",
    "#epochs = 200\n",
    "#dropout = 0.0\n",
    "#optimizer='SGD'\n",
    "#each_epochs_save = 5\n",
    "\n",
    "# how many lambda nets should be generated (must be smaller or equal to interpretation_dataset_size)\n",
    "number_of_lambda_nets = interpretation_dataset_size\n",
    "\n",
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "\n",
    "# set derived attributes\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "if same_training_all_polynomials: \n",
    "    training_string = '_same'\n",
    "else: \n",
    "    training_string = '_diverse'\n",
    "\n",
    "if number_of_lambda_nets < interpretation_dataset_size:\n",
    "    generate_subset = True\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    generate_subset = False\n",
    "    data_size = interpretation_dataset_size   \n",
    "\n",
    "seed_method = True\n",
    "shuffle = True\n",
    "\n",
    "lambda_network_layers = [5 * gh.nCr(n+d, d)]\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleSeedMethod'\n",
    "elif not seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleNoSeedMethod'\n",
    "elif seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleSeedMethod'\n",
    "elif not seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleNoSeedMethod'\n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: 4 (abcd)\n",
      "Degree: 3\n",
      "Sparsity: 0.0%\n",
      "Lambda-Net Dataset Size: 100\n",
      "I-Net Dataset Size: 5000\n",
      "Number of Lambda-Nets to Train: 5000\n",
      "Coefficient Range: [-10.0, 10.0]\n",
      "Variable Range: [-1.0, 1.0]\n",
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# inspect most important settings\n",
    "print('Variables: ' + str(n) + ' (' + variables + ')')\n",
    "print('Degree: ' + str(d))\n",
    "print('Sparsity: ' + str(sparsity*100) + '%') \n",
    "print('Lambda-Net Dataset Size: ' + str(lambda_dataset_size))\n",
    "print('I-Net Dataset Size: ' + str(interpretation_dataset_size))\n",
    "print('Number of Lambda-Nets to Train: ' + str(number_of_lambda_nets))\n",
    "      \n",
    "print('Coefficient Range: ' + '[' + str(a_min) + ', ' + str(a_max) + ']')\n",
    "print('Variable Range: ' + '[' + str(x_min) + ', ' + str(x_max) + ']')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:51.872001Z",
     "start_time": "2020-09-17T08:06:51.861463Z"
    }
   },
   "outputs": [],
   "source": [
    "#def calcualate_function_with_data(coefficient_list, variable_values):\n",
    "#    \n",
    "#    global list_of_monomial_identifiers\n",
    "#    \n",
    "#    result = 0    \n",
    "#    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "#        partial_results = [variable_value**int(coefficient_multiplier) for coefficient_multiplier, variable_value in zip(coefficient_multipliers, variable_values)]\n",
    "#        \n",
    "#        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "#\n",
    "#    return result, variable_values\n",
    "# \n",
    "#def calculate_function_values_from_polynomial(true_value_test, evaluation_dataset):\n",
    "#\n",
    "#    if isinstance(true_value_test, pd.DataFrame):\n",
    "#        true_value_test = true_value_test.values\n",
    "#        \n",
    "#    true_value_fv = []\n",
    "#    true_value_coeff = []\n",
    "#        \n",
    "#    for evaluation in evaluation_dataset:\n",
    "#        true_function_value, true_coeff = calcualate_function_with_data(true_value_test, evaluation)\n",
    "#       \n",
    "#        true_value_fv.append(true_function_value) \n",
    "#        true_value_coeff.append(true_coeff)\n",
    "#        \n",
    "#    return np.array(true_value_coeff), np.array(true_value_fv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Import (from Polynomial Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.469880Z",
     "start_time": "2020-09-17T08:06:52.018648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Csv imports\n",
    "path_polynomials_df = './data/saved_polynomial_lists/polynomials_df' + str(interpretation_dataset_size) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_df = gh.import_csv(path_polynomials_df)\n",
    "\n",
    "# Pickle imports\n",
    "path_polynomials_poly = './data/saved_polynomial_lists/polynomials_poly' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "polynomials_poly = gh.import_pickle(path_polynomials_poly)\n",
    "\n",
    "path_x_data = './data/saved_polynomial_lists/x_data' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "x_data = gh.import_pickle(path_x_data)\n",
    "\n",
    "path_y_data = './data/saved_polynomial_lists/y_data' + str(interpretation_dataset_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.pkl'\n",
    "y_data = gh.import_pickle(path_y_data)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_df = polynomials_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    x_data = random.sample(x_data, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data = random.sample(y_data, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.725867Z",
     "start_time": "2020-09-17T08:06:52.706076Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.39,  0.98,  0.6 , -0.97],\n",
       "       [ 0.35,  0.14,  0.61, -0.27],\n",
       "       [-0.65,  0.56, -0.33, -0.66],\n",
       "       [-0.73, -0.79, -0.43,  0.16],\n",
       "       [-0.5 , -0.75,  0.33, -0.77],\n",
       "       [-0.33, -0.55,  0.98,  0.34],\n",
       "       [-0.64, -0.33,  0.23,  0.5 ],\n",
       "       [ 0.59, -0.47,  0.82, -0.16],\n",
       "       [ 0.38, -0.21, -0.61,  0.48],\n",
       "       [ 0.96, -0.54,  0.22,  0.26],\n",
       "       [ 0.17, -0.66,  0.74, -0.53],\n",
       "       [-0.75, -0.72,  0.86, -0.86],\n",
       "       [ 0.98, -0.2 ,  0.  ,  0.35],\n",
       "       [-0.6 ,  0.28,  0.59, -0.98],\n",
       "       [ 0.6 , -0.21, -0.73,  0.88],\n",
       "       [ 0.84, -0.43, -0.77,  0.22],\n",
       "       [-0.01, -0.03, -0.27, -0.51],\n",
       "       [ 0.53,  0.76,  0.99, -0.89],\n",
       "       [ 0.69,  0.28,  0.7 , -0.54],\n",
       "       [ 0.88,  0.54, -0.19, -0.37],\n",
       "       [ 0.77,  0.77, -0.25,  0.69],\n",
       "       [ 0.7 ,  0.16,  0.08,  0.76],\n",
       "       [-0.21,  0.3 ,  0.9 , -0.89],\n",
       "       [-0.92,  0.76,  0.21,  0.16],\n",
       "       [ 0.8 , -0.55, -0.38, -0.89],\n",
       "       [ 0.91, -0.15,  0.67,  0.04],\n",
       "       [-0.78, -0.2 ,  0.43, -0.18],\n",
       "       [-0.23,  0.52,  0.69,  0.15],\n",
       "       [-0.32, -0.38,  0.29,  0.17],\n",
       "       [-0.49,  0.53, -0.69,  0.75],\n",
       "       [ 0.34,  0.11,  0.24,  0.47],\n",
       "       [ 0.3 ,  0.02, -0.32, -0.16],\n",
       "       [ 0.31,  0.38, -0.8 ,  0.7 ],\n",
       "       [-0.91,  0.06, -0.18, -0.62],\n",
       "       [ 0.52,  0.12, -0.08, -0.37],\n",
       "       [ 0.34,  0.25,  0.61, -0.45],\n",
       "       [-0.33,  0.8 ,  0.47,  0.19],\n",
       "       [ 0.16,  0.22,  0.42, -0.55],\n",
       "       [ 0.14, -0.13,  0.64, -0.52],\n",
       "       [ 0.27,  0.37, -0.6 ,  0.32],\n",
       "       [ 0.81,  0.77,  0.69, -0.91],\n",
       "       [ 0.15,  0.29,  0.8 , -0.19],\n",
       "       [ 0.9 , -0.19,  0.49,  0.26],\n",
       "       [-0.73,  0.78,  0.74,  0.31],\n",
       "       [-0.17,  0.01, -0.1 ,  0.23],\n",
       "       [ 0.28, -0.24,  0.81, -0.78],\n",
       "       [ 0.63,  0.62, -0.3 ,  0.82],\n",
       "       [-0.61,  0.76,  0.88,  0.15],\n",
       "       [-0.27, -0.82, -0.55, -0.33],\n",
       "       [-0.4 , -0.31,  0.84, -0.87],\n",
       "       [ 0.03, -0.7 ,  0.08, -0.49],\n",
       "       [-0.86, -0.4 , -0.83, -0.79],\n",
       "       [ 0.16, -0.33, -0.8 ,  0.2 ],\n",
       "       [-0.9 ,  0.04,  0.84, -0.87],\n",
       "       [-0.89, -0.18,  0.39, -0.93],\n",
       "       [ 0.29, -0.04,  0.27, -0.73],\n",
       "       [-0.11, -0.05, -0.53,  0.49],\n",
       "       [ 0.4 ,  0.95,  0.32, -0.93],\n",
       "       [-0.61, -0.74, -0.17,  0.83],\n",
       "       [-0.15, -0.05,  0.46, -0.61],\n",
       "       [ 0.63, -0.37,  0.57, -0.08],\n",
       "       [ 0.01,  0.2 ,  0.31,  0.21],\n",
       "       [-0.73,  0.4 , -0.81,  0.65],\n",
       "       [ 0.51,  0.61,  0.38,  0.94],\n",
       "       [ 0.02, -0.47, -0.6 , -0.35],\n",
       "       [-0.42,  0.09, -0.33,  0.85],\n",
       "       [-0.88,  0.52, -0.72,  0.97],\n",
       "       [-0.56,  0.98,  0.92, -0.46],\n",
       "       [-0.41, -0.39,  0.44, -0.05],\n",
       "       [-0.99,  0.89, -0.54, -0.78],\n",
       "       [-0.63,  0.91, -0.14, -0.91],\n",
       "       [ 0.55,  0.3 , -0.07,  0.63],\n",
       "       [-0.64,  0.06,  0.15, -0.2 ],\n",
       "       [-0.19, -0.14,  0.16, -0.34],\n",
       "       [-0.61, -0.59, -0.39,  0.73],\n",
       "       [ 0.97,  0.79, -0.41, -0.36],\n",
       "       [ 0.77,  0.14, -0.77,  0.15],\n",
       "       [ 0.95,  0.96,  0.09, -0.95],\n",
       "       [-0.5 , -0.99,  1.  ,  0.87],\n",
       "       [-0.77, -0.69, -0.47, -0.41],\n",
       "       [ 0.7 ,  0.31,  0.81, -0.04],\n",
       "       [-0.62, -0.99, -0.24,  0.23],\n",
       "       [-0.35, -0.9 , -0.82, -0.39],\n",
       "       [-0.18,  0.99,  0.76,  0.85],\n",
       "       [ 0.85,  0.  ,  0.4 , -0.41],\n",
       "       [-0.64,  0.18,  0.18,  0.19],\n",
       "       [ 0.61,  0.13,  0.37,  0.68],\n",
       "       [ 0.92, -0.86, -1.  , -0.4 ],\n",
       "       [ 0.7 ,  0.03, -0.35, -0.04],\n",
       "       [-0.31, -0.07, -0.75, -0.16],\n",
       "       [ 0.19,  0.25,  0.37, -0.34],\n",
       "       [-0.82,  0.53,  0.19,  0.7 ],\n",
       "       [ 0.58,  0.99,  0.49,  0.92],\n",
       "       [-0.6 ,  0.19, -0.3 ,  0.28],\n",
       "       [ 0.92, -0.44,  0.6 ,  0.71],\n",
       "       [-0.53,  0.28, -0.07,  0.01],\n",
       "       [-0.84, -0.18,  0.72, -0.26],\n",
       "       [-0.56, -0.03, -0.05,  0.82],\n",
       "       [ 0.79,  0.24,  0.32, -0.74],\n",
       "       [ 0.02,  0.7 , -0.19,  0.07]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first polynomial of x_data\n",
    "print(len(x_data[0]))\n",
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.737995Z",
     "start_time": "2020-09-17T08:06:52.728292Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  6.5466964,  14.1552117,   6.2386106,  -5.1563538,  -0.3081001,\n",
       "        14.2110167,   2.7051367,   9.8477365,  13.3632263,  -3.7052808,\n",
       "         6.8799222,  21.431129 ,   5.5486424,  10.2749953,  14.4934895,\n",
       "        12.0238791,   4.7583472,  36.0919691,  21.6437686,  14.6957146,\n",
       "        14.033763 ,   7.48636  ,  20.1434354,  22.4756171,   2.4145583,\n",
       "        10.1207996,   8.0853943,  17.3275651,   4.8414354,  13.8312942,\n",
       "        10.0359621,   8.5623464,  11.232144 ,   8.3449984,   8.26664  ,\n",
       "        14.9745371,  19.6612157,   9.5912396,  11.5108923,  12.6079121,\n",
       "        31.8877648,  19.0505869,   5.1019445,  21.3153018,  10.3172595,\n",
       "        17.2387127,  12.0904572,  22.7792788,  -2.8126045,  19.4617989,\n",
       "        -4.7880812,   8.1800906,  10.9686977,  21.3805084,  10.5591725,\n",
       "         5.7823363,  12.4230728,  10.5239151,  -5.3532647,   7.8290411,\n",
       "         5.3607768,  11.6056111,  11.0576213,   5.9079895,   4.3057739,\n",
       "        10.1469917,  10.3934912,  23.213084 ,   5.7650491,  10.0388477,\n",
       "         1.7746277,  11.8731869,   8.6720263,   5.3335864,   0.0190484,\n",
       "        15.1137254,  12.2259315,  22.3759945,  -0.9815239,  -1.5854494,\n",
       "        21.9632117, -10.4784587,  -2.3763802,  15.1437751,  11.6886032,\n",
       "        11.2661008,   6.5393054,   5.7887176,  10.7143337,   7.435021 ,\n",
       "        10.4659886,  14.0128355,  11.4581501,  12.4236489,  -5.6950228,\n",
       "        11.7706597,  14.0667416,   5.9256311,  14.735869 ,  14.3733816])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first polynomial of y_data\n",
    "print(len(y_data[0]))\n",
    "y_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generation of all possible Monomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.015141Z",
     "start_time": "2020-09-17T08:06:51.925042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "[a**3, a**2*b, a**2*c, a**2*d, a**2, a*b**2, a*b*c, a*b*d, a*b, a*c**2, a*c*d, a*c, a*d**2, a*d, a, b**3, b**2*c, b**2*d, b**2, b*c**2, b*c*d, b*c, b*d**2, b*d, b, c**3, c**2*d, c**2, c*d**2, c*d, c, d**3, d**2, d, 1]\n",
      "[(3, 0, 0, 0), (2, 1, 0, 0), (2, 0, 1, 0), (2, 0, 0, 1), (2, 0, 0, 0), (1, 2, 0, 0), (1, 1, 1, 0), (1, 1, 0, 1), (1, 1, 0, 0), (1, 0, 2, 0), (1, 0, 1, 1), (1, 0, 1, 0), (1, 0, 0, 2), (1, 0, 0, 1), (1, 0, 0, 0), (0, 3, 0, 0), (0, 2, 1, 0), (0, 2, 0, 1), (0, 2, 0, 0), (0, 1, 2, 0), (0, 1, 1, 1), (0, 1, 1, 0), (0, 1, 0, 2), (0, 1, 0, 1), (0, 1, 0, 0), (0, 0, 3, 0), (0, 0, 2, 1), (0, 0, 2, 0), (0, 0, 1, 2), (0, 0, 1, 1), (0, 0, 1, 0), (0, 0, 0, 3), (0, 0, 0, 2), (0, 0, 0, 1), (0, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "polys_symbols = symbols(list(variables))\n",
    "polys_monomials = sorted(monomials.itermonomials(variables = polys_symbols, max_degrees = d, min_degrees = None), \n",
    "                                      reverse = True, key = monomial_key('lex', polys_symbols))\n",
    "monomials_count = monomials.monomial_count(n, d)\n",
    "\n",
    "tuples_monomials = [ph.monomial_to_power_tuple(mon, polys_symbols) for mon in polys_monomials]\n",
    "\n",
    "print('List length: ' + str(len(polys_monomials)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(monomials_count))\n",
    "print(polys_monomials)\n",
    "print(tuples_monomials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:52.702250Z",
     "start_time": "2020-09-17T08:06:52.472447Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        for i in range(epochs//each_epochs_save):    \n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:06:53.085851Z",
     "start_time": "2020-09-17T08:06:52.758561Z"
    },
    "code_folding": [
     32
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data, y_data, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data = X_data.values\n",
    "    if isinstance(y_data, pd.DataFrame):\n",
    "        y_data = y_data.values\n",
    "        \n",
    "    X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "            \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae', #huber_loss(val_min, val_max), #'mape',#'mean_absolute_error',#root_mean_squared_error,\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    weights = []\n",
    "    polynomial_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "        \n",
    "        \n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:        \n",
    "        model_history = model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "\n",
    "        y_pred_valid = model.predict(X_valid)                \n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        term_list_all = []\n",
    "        y = 0\n",
    "        for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "            term_list = [int(value_mult) for value_mult in term]\n",
    "            term_list_all.append(term_list)\n",
    "\n",
    "        #print(trm_list_all)\n",
    "\n",
    "        #generate separate arrays for each variable combination\n",
    "        terms_matrix = []\n",
    "        for unknowns in X_valid:\n",
    "            terms = []\n",
    "            for term_multipliers in term_list_all:\n",
    "                term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                terms.append(term_value)\n",
    "            terms_matrix.append(np.array(terms))\n",
    "\n",
    "        terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "        polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_pred_list.append(polynomial_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_test_polynomial = []\n",
    "        y_test_lstsq = []\n",
    "        for entry in X_test:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            \n",
    "            y_test_polynomial.append(true_function_value_pred)\n",
    "            y_test_lstsq.append(true_function_value_lstsq)\n",
    "        y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "        y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "        y_valid_polynomial = []  \n",
    "        y_valid_lstsq = []\n",
    "        for entry in X_valid:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_valid_polynomial.append(true_function_value_pred)     \n",
    "            y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "        y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "        y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)     \n",
    "\n",
    "        pred_list = (y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test)\n",
    "\n",
    "        mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "        mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "        mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "        mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "        rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "        rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "        rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "        rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "        \n",
    "        mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "        mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "        mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "        mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "        r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "        r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "        r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "        r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "        raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "        raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "        rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "        rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "        dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "        dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "        dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "        mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "        mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "        mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "        rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "        rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "        mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "        mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "        r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "        r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "        r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "        raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "        raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "        rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "        rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "        rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "        \n",
    "        fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "        dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "        dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "        dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "        \n",
    "        std_train_real = np.std(y_train)\n",
    "        std_valid_real = np.std(y_valid)\n",
    "        std_test_real = np.std(y_test)\n",
    "\n",
    "        std_valid_pred = np.std(y_pred_valid)\n",
    "        std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "        std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "        std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "        std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "        std_test_lstsq = np.std(y_test_lstsq)\n",
    "        \n",
    "        \n",
    "        mean_train_real = np.mean(y_train)\n",
    "        mean_valid_real = np.mean(y_valid)\n",
    "        mean_test_real = np.mean(y_test)\n",
    "\n",
    "        mean_valid_pred = np.mean(y_pred_valid)\n",
    "        mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "        mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "        mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "        mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "        mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "\n",
    "        result_dict_list =  [{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]            \n",
    "                            \n",
    "    else:\n",
    "        result_dict_list = []\n",
    "        pred_list = []\n",
    "        for i in range(epochs//each_epochs_save):\n",
    "            if i == 0:\n",
    "                history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "                history = history.history\n",
    "            else:\n",
    "                model_history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "\n",
    "                for key_1 in history.keys():\n",
    "                    for key_2 in model_history.history.keys():\n",
    "                        if key_1 == key_2:\n",
    "                            history[key_1] += model_history.history[key_2]  \n",
    "            \n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)                \n",
    "            y_pred_test = model.predict(X_test)        \n",
    "\n",
    "            term_list_all = []\n",
    "            y = 0\n",
    "            for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "                term_list = [int(value_mult) for value_mult in term]\n",
    "                term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "            terms_matrix = []\n",
    "            for unknowns in X_valid:\n",
    "                terms = []\n",
    "                for term_multipliers in term_list_all:\n",
    "                    term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                    terms.append(term_value)\n",
    "                terms_matrix.append(np.array(terms))\n",
    "\n",
    "            terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "            polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_valid.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_valid.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_pred_list.append(polynomial_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            y_test_polynomial = []\n",
    "            if i == 0:\n",
    "                y_test_lstsq = []\n",
    "            for entry in X_test:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_test_polynomial.append(true_function_value_pred)\n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_test_lstsq.append(true_function_value_lstsq)\n",
    "\n",
    "            y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "            if i == 0:\n",
    "                y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "            y_valid_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = []\n",
    "            for entry in X_valid:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_valid_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "            y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)    \n",
    "                \n",
    "            pred_list.append((y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test))\n",
    "\n",
    "            \n",
    "            mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "            mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "            mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "            rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "            rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "            mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "            mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "            r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "            r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "            r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "            raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "            raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "            rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "            rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "            dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "            dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "            if i == 0:\n",
    "                dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "            mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "            mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "            mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "            rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "            rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "            mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "            mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "            r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "            r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "            raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "            raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "            rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "            rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "            if i == 0:\n",
    "                rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "\n",
    "            fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "            dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "            dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "            std_train_real = np.std(y_train)\n",
    "            std_valid_real = np.std(y_valid)\n",
    "            std_test_real = np.std(y_test)\n",
    "\n",
    "            std_valid_pred = np.std(y_pred_valid)\n",
    "            std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "            std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "            std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "                std_test_lstsq = np.std(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            mean_train_real = np.mean(y_train)\n",
    "            mean_valid_real = np.mean(y_valid)\n",
    "            mean_test_real = np.mean(y_test)\n",
    "\n",
    "            mean_valid_pred = np.mean(y_pred_valid)\n",
    "            mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "            mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "            mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "                mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            result_dict_list_single_epoch =  [{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]        \n",
    "                  \n",
    "            result_dict_list.append(result_dict_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save != None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, #polynomial_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:02.580716Z",
     "start_time": "2020-09-17T08:06:53.088393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fad765599c241dfa50306c919a01634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_percentage_error_keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/lahoffma/Programs/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/home/lahoffma/Programs/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/lahoffma/Programs/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/lahoffma/Programs/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/lahoffma/Programs/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 252, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"<ipython-input-20-30c8e51d7ce1>\", line 34, in train_nn\nNameError: name 'mean_absolute_percentage_error_keras' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-95cd4648a8b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX_data_list_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data_list_split\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data_list_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data_list_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data_list_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data_list_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loky'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclf_sublist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_epochs_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meach_epochs_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data_list_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data_list_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mclf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_sublist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_percentage_error_keras' is not defined"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(gh.chunks(x_data, chunksize))\n",
    "y_data_list_splits = list(gh.chunks(y_data, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data, y_data, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesCallback(skip_first=0)], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.693701Z",
     "start_time": "2020-09-17T08:17:02.583223Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    dict_list_valid = [clf[1][0] for clf in clf_list]\n",
    "    dict_list_test = [clf[1][1] for clf in clf_list]\n",
    "    dict_list_stds = [clf[1][2] for clf in clf_list]\n",
    "    dict_list_means = [clf[1][3] for clf in clf_list]\n",
    "\n",
    "    dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_dict_list_by_epochs = [[] for i in range(epochs//each_epochs_save)]\n",
    "    for scores_dict_list in scores_list:   \n",
    "        for index, scores_dict in enumerate(scores_dict_list):\n",
    "            scores_dict_list_by_epochs[index].append(scores_dict)\n",
    "            \n",
    "        \n",
    "    for index, scores_dict_list_single_epoch in enumerate(scores_dict_list_by_epochs):\n",
    "        index = (index+1)*each_epochs_save\n",
    "        dict_list_valid = [dict_list[0] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_test = [dict_list[1] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_stds = [dict_list[2] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_means = [dict_list[3] for dict_list in scores_dict_list_single_epoch]\n",
    "        \n",
    "        dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()  \n",
    "        dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1*each_epochs_save:\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.723163Z",
     "start_time": "2020-09-17T08:17:03.696067Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.751338Z",
     "start_time": "2020-09-17T08:17:03.725279Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.779243Z",
     "start_time": "2020-09-17T08:17:03.753450Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:17:03.807209Z",
     "start_time": "2020-09-17T08:17:03.781408Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.168836Z",
     "start_time": "2020-09-17T08:17:03.809359Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data = [clf[2] for clf in clf_list]\n",
    "\n",
    "    pred_evaluation_dataset_valid_real = pred_evaluation_dataset_valid_real.ravel()\n",
    "    pred_evaluation_dataset_test_real = pred_evaluation_dataset_test_real.ravel()\n",
    "    pred_evaluation_dataset_valid = pred_evaluation_dataset_valid.ravel()\n",
    "    pred_evaluation_dataset_test = pred_evaluation_dataset_test.ravel()\n",
    "    pred_evaluation_dataset_valid_polynomial = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "    pred_evaluation_dataset_test_polynomial = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial, columns=[str(test_data) for test_data in X_test_data])    \n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "    pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "    pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "       \n",
    "    path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]))) for i in range(epochs//each_epochs_save)]\n",
    "    \n",
    "    for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data) in enumerate(pred_evaluation_dataset_per_epoch):\n",
    "            pred_evaluation_dataset_valid_real_list[index][i] = pred_evaluation_dataset_valid_real.ravel()\n",
    "            pred_evaluation_dataset_valid_list[index][i] = pred_evaluation_dataset_valid.ravel()\n",
    "            pred_evaluation_dataset_valid_polynomial_list[index][i] = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_test_real_list[index][i] = pred_evaluation_dataset_test_real.ravel()\n",
    "            pred_evaluation_dataset_test_list[index][i] = pred_evaluation_dataset_test.ravel()\n",
    "            pred_evaluation_dataset_test_polynomial_list[index][i] = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    for index, (pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n",
    "        pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])  \n",
    "        pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial_by_epoch, columns=[str(test_data) for test_data in X_test_data])   \n",
    "        \n",
    "        pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "        pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "        pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "\n",
    "        path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)         \n",
    "        pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)    \n",
    "        pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.209395Z",
     "start_time": "2020-09-17T08:18:00.176321Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_real_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.239323Z",
     "start_time": "2020-09-17T08:18:00.212545Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.268837Z",
     "start_time": "2020-09-17T08:18:00.241795Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_evaluation_dataset_test_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.274581Z",
     "start_time": "2020-09-17T08:18:00.271355Z"
    }
   },
   "outputs": [],
   "source": [
    "#variable_values = []\n",
    "#for column in pred_evaluation_dataset_df.columns:\n",
    "#    variable_values.append(np.array(column[1:-1].split()).astype('float'))\n",
    "#variable_values = np.array(variable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.283212Z",
     "start_time": "2020-09-17T08:18:00.277813Z"
    }
   },
   "outputs": [],
   "source": [
    "#fv_with_vv = []\n",
    "#for function_values in tqdm(pred_evaluation_dataset_df.values):\n",
    "#    fv_with_vv.append(np.array([np.append(vv, fv) for fv, vv in zip(function_values, variable_values)]))\n",
    "#fv_with_vv = np.array(fv_with_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:00.532950Z",
     "start_time": "2020-09-17T08:18:00.285738Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:01.152123Z",
     "start_time": "2020-09-17T08:18:00.535436Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:02.003084Z",
     "start_time": "2020-09-17T08:18:01.154461Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:02.664470Z",
     "start_time": "2020-09-17T08:18:02.005271Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:03.331777Z",
     "start_time": "2020-09-17T08:18:02.666846Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:03.994633Z",
     "start_time": "2020-09-17T08:18:03.333795Z"
    }
   },
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:05.622225Z",
     "start_time": "2020-09-17T08:18:03.996958Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T08:18:07.191097Z",
     "start_time": "2020-09-17T08:18:05.624357Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
