{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Specitication-of-Experiment-Settings\" data-toc-modified-id=\"Specitication-of-Experiment-Settings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Specitication of Experiment Settings</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Utility-functions\" data-toc-modified-id=\"Utility-functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Utility functions</a></span></li><li><span><a href=\"#Loss/Error-functions\" data-toc-modified-id=\"Loss/Error-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loss/Error functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-List-of-Monomial-Identifiers\" data-toc-modified-id=\"Generate-List-of-Monomial-Identifiers-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Generate List of Monomial Identifiers</a></span></li></ul></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Data-Inspection\" data-toc-modified-id=\"Data-Inspection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data Inspection</a></span></li><li><span><a href=\"#Lambda-Network-Training-+-Weigh/Bias-saving\" data-toc-modified-id=\"Lambda-Network-Training-+-Weigh/Bias-saving-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Lambda Network Training + Weigh/Bias saving</a></span></li><li><span><a href=\"#Save-Lambda-Model-scores\" data-toc-modified-id=\"Save-Lambda-Model-scores-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Save Lambda-Model scores</a></span></li><li><span><a href=\"#Mail-Notification\" data-toc-modified-id=\"Mail-Notification-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Mail Notification</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext email_notify_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:03.438269Z",
     "start_time": "2020-11-10T19:21:03.433555Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:03.453326Z",
     "start_time": "2020-11-10T19:21:03.441657Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 500 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 5  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:03.468604Z",
     "start_time": "2020-11-10T19:21:03.456318Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "same_training_all_polynomials = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "seed_method = True\n",
    "shuffle = True\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_polynomials:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleSeedMethod'\n",
    "elif not seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleNoSeedMethod'\n",
    "elif seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleSeedMethod'\n",
    "elif not seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleNoSeedMethod'\n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.132327Z",
     "start_time": "2020-11-10T19:21:03.471218Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.167543Z",
     "start_time": "2020-11-10T19:21:46.134828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.202749Z",
     "start_time": "2020-11-10T19:21:46.169840Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T11:11:38.705661Z",
     "start_time": "2020-11-12T11:11:38.698087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pairwise([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.214736Z",
     "start_time": "2020-11-10T19:21:46.206190Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcualate_function_with_data(coefficient_list, variable_values):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [variable_value**int(coefficient_multiplier) for coefficient_multiplier, variable_value in zip(coefficient_multipliers, variable_values)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "\n",
    "    return result, variable_values\n",
    " \n",
    "def calculate_function_values_from_polynomial(true_value_test, evaluation_dataset):\n",
    "\n",
    "    if isinstance(true_value_test, pd.DataFrame):\n",
    "        true_value_test = true_value_test.values\n",
    "        \n",
    "    true_value_fv = []\n",
    "    true_value_coeff = []\n",
    "        \n",
    "    for evaluation in evaluation_dataset:\n",
    "        true_function_value, true_coeff = calcualate_function_with_data(true_value_test, evaluation)\n",
    "       \n",
    "        true_value_fv.append(true_function_value) \n",
    "        true_value_coeff.append(true_coeff)\n",
    "        \n",
    "    return np.array(true_value_coeff), np.array(true_value_fv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.250338Z",
     "start_time": "2020-11-10T19:21:46.217422Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:46.339337Z",
     "start_time": "2020-11-10T19:21:46.252708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-661ec6648fcc>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range((d+1)**n)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2884ba0ea063432a8552cf2fa55785a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-661ec6648fcc>:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7387cc23b1d46c0b7f37f8c4f81816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.268910Z",
     "start_time": "2020-11-10T19:21:46.341595Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.347726Z",
     "start_time": "2020-11-10T19:21:47.271579Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        for i in range(epochs//each_epochs_save):    \n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.391172Z",
     "start_time": "2020-11-10T19:21:47.351265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.340 -0.230  0.570  0.310\n",
       "1  0.380 -0.380  0.770 -0.880\n",
       "2 -0.050  0.460  0.210 -0.450\n",
       "3  0.940  0.110 -0.300  0.910\n",
       "4  0.760  0.990 -0.530 -0.300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.404616Z",
     "start_time": "2020-11-10T19:21:47.393647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.340 -0.230  0.570  0.310\n",
       "1  0.380 -0.380  0.770 -0.880\n",
       "2 -0.050  0.460  0.210 -0.450\n",
       "3  0.940  0.110 -0.300  0.910\n",
       "4  0.760  0.990 -0.530 -0.300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.415838Z",
     "start_time": "2020-11-10T19:21:47.407225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.427316Z",
     "start_time": "2020-11-10T19:21:47.420886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add train to pred and save pred_eval_dataset_train containing train data and use for loss function in interpretation network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.705440Z",
     "start_time": "2020-11-10T19:21:47.431576Z"
    },
    "code_folding": [
     48
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data, y_data, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data = X_data.values\n",
    "    if isinstance(y_data, pd.DataFrame):\n",
    "        y_data = y_data.values\n",
    "        \n",
    "    #split train test valid\n",
    "        \n",
    "    X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "    #create neural network structure       \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "            \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae', #huber_loss(val_min, val_max), #'mape',#'mean_absolute_error',#root_mean_squared_error,\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "        \n",
    "        \n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        #training neurl net\n",
    "        model_history = model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_pred_train = model.predict(X_train) \n",
    "        y_pred_valid = model.predict(X_valid)                \n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        term_list_all = []\n",
    "        y = 0\n",
    "        for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "            term_list = [int(value_mult) for value_mult in term]\n",
    "            term_list_all.append(term_list)\n",
    "\n",
    "        #print(trm_list_all)\n",
    "\n",
    "        #generate separate arrays for each variable combination\n",
    "        terms_matrix = []\n",
    "        for unknowns in X_train:\n",
    "            terms = []\n",
    "            for term_multipliers in term_list_all:\n",
    "                term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                terms.append(term_value)\n",
    "            terms_matrix.append(np.array(terms))\n",
    "\n",
    "        terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "        polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_train.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_pred_list.append(polynomial_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_test_polynomial = []\n",
    "        y_test_lstsq = []\n",
    "        for entry in X_test:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)   \n",
    "            y_test_polynomial.append(true_function_value_pred)\n",
    "            y_test_lstsq.append(true_function_value_lstsq)\n",
    "        y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "        y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "        y_valid_polynomial = []  \n",
    "        y_valid_lstsq = []\n",
    "        for entry in X_valid:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_valid_polynomial.append(true_function_value_pred)     \n",
    "            y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "        y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "        y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)     \n",
    "        \n",
    "        y_train_polynomial = []  \n",
    "        y_train_lstsq = []\n",
    "        for entry in X_train:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_train_polynomial.append(true_function_value_pred)     \n",
    "            y_train_lstsq.append(true_function_value_lstsq)     \n",
    "        y_train_polynomial = np.array(y_train_polynomial).reshape(len(y_train_polynomial), 1)     \n",
    "        y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)    \n",
    "        \n",
    "        \n",
    "        pred_list = (y_train, y_pred_train, y_train_polynomial, X_train, y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test)\n",
    "\n",
    "        mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "        mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "        mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "        mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "        rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "        rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "        rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "        rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "        \n",
    "        mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "        mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "        mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "        mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "        r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "        r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "        r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "        r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "        raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "        raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "        rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "        rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "        dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "        dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "        dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "        mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "        mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "        mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "        rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "        rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "        mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "        mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "        r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "        r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "        r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "        raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "        raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "        rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "        rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "        rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "        \n",
    "        fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "        dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "        dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "        dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    #CHANGE TRAIN\n",
    "        mae_train_pred = np.round(mean_absolute_error(y_train, y_pred_train), 4)\n",
    "        mae_train_polynomial = np.round(mean_absolute_error(y_train, y_train_polynomial), 4)\n",
    "        mae_train_polynomial_pred = np.round(mean_absolute_error(y_train_polynomial, y_pred_train), 4)\n",
    "        mae_train_lstsq = np.round(mean_absolute_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        rmse_train_pred = np.round(root_mean_squared_error(y_train, y_pred_train), 4)\n",
    "        rmse_train_polynomial = np.round(root_mean_squared_error(y_train, y_train_polynomial), 4)\n",
    "        rmse_train_polynomial_pred = np.round(root_mean_squared_error(y_train_polynomial, y_pred_train), 4)\n",
    "        rmse_train_lstsq = np.round(root_mean_squared_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        mape_train_pred = np.round(mean_absolute_percentage_error_keras(y_train, y_pred_train), 4)\n",
    "        mape_train_polynomial = np.round(mean_absolute_percentage_error_keras(y_train, y_train_polynomial), 4)\n",
    "        mape_train_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_train_polynomial, y_pred_train), 4)\n",
    "        mape_train_lstsq = np.round(mean_absolute_percentage_error_keras(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        r2_train_pred = np.round(r2_score(y_train, y_pred_train), 4)\n",
    "        r2_train_polynomial = np.round(r2_score(y_train, y_train_polynomial), 4)\n",
    "        r2_train_polynomial_pred = np.round(r2_score(y_train_polynomial, y_pred_train), 4)\n",
    "        r2_train_lstsq = np.round(r2_score(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        raae_train_pred = np.round(relative_absolute_average_error(y_train, y_pred_train), 4)\n",
    "        raae_train_polynomial = np.round(relative_absolute_average_error(y_train, y_train_polynomial), 4)\n",
    "        raae_train_polynomial_pred = np.round(relative_absolute_average_error(y_train_polynomial, y_pred_train), 4)\n",
    "        raae_train_lstsq = np.round(relative_absolute_average_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        rmae_train_pred = np.round(relative_maximum_average_error(y_train, y_pred_train), 4) \n",
    "        rmae_train_polynomial = np.round(relative_maximum_average_error(y_train, y_train_polynomial), 4) \n",
    "        rmae_train_polynomial_pred = np.round(relative_maximum_average_error(y_train_polynomial, y_pred_train), 4) \n",
    "        rmae_train_lstsq = np.round(relative_maximum_average_error(y_train, y_train_lstsq), 4) \n",
    "        \n",
    "        fd_train_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_polynomial = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_polynomial_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_lstsq = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_train_pred, dtw_complete_train_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "        dtw_train_pred = np.round(dtw_train_pred, 4)    \n",
    "        dtw_train_polynomial, dtw_complete_train_polynomial = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_train_polynomial = np.round(dtw_train_polynomial, 4)       \n",
    "        dtw_train_polynomial_pred, dtw_complete_train_polynomial_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "        dtw_train_polynomial_pred = np.round(dtw_train_polynomial_pred, 4)   \n",
    "        dtw_train_lstsq, dtw_complete_train_lstsq = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_train_lstsq = np.round(dtw_train_lstsq, 4)\n",
    "\n",
    "        \n",
    "        std_train_real = np.std(y_train)\n",
    "        std_valid_real = np.std(y_valid)\n",
    "        std_test_real = np.std(y_test)\n",
    "\n",
    "        std_train_pred = np.std(y_pred_train)\n",
    "        std_valid_pred = np.std(y_pred_valid)\n",
    "        std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "        std_train_polynomial = np.std(y_train_polynomial)\n",
    "        std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "        std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "        std_train_lstsq = np.std(y_train_lstsq)\n",
    "        std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "        std_test_lstsq = np.std(y_test_lstsq)\n",
    "        \n",
    "        \n",
    "        mean_train_real = np.mean(y_train)\n",
    "        mean_valid_real = np.mean(y_valid)\n",
    "        mean_test_real = np.mean(y_test)\n",
    "\n",
    "        mean_train_pred = np.mean(y_pred_train)\n",
    "        mean_valid_pred = np.mean(y_pred_valid)\n",
    "        mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "        mean_train_polynomial = np.mean(y_train_polynomial)\n",
    "        mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "        mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "        mean_train_lstsq = np.mean(y_train_lstsq)\n",
    "        mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "        mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "\n",
    "        result_dict_list =  [{\n",
    "                         'MAE FV TRAIN PRED': mae_valid_pred,\n",
    "                         'MAE FV TRAIN POLY': mae_valid_polynomial,\n",
    "                         'MAE FV TRAIN POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV TRAIN LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV TRAIN PRED': rmse_valid_pred,\n",
    "                         'RMSE FV TRAIN POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV TRAIN POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV TRAIN LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV TRAIN PRED': mape_valid_pred,\n",
    "                         'MAPE FV TRAIN POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV TRAIN POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV TRAIN LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV TRAIN PRED': r2_valid_pred,\n",
    "                         'R2 FV TRAIN POLY': r2_valid_polynomial,\n",
    "                         'R2 FV TRAIN POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV TRAIN LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED TRAIN': raae_valid_pred,\n",
    "                         'RAAE FV TRAIN POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV TRAIN POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV TRAIN LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV TRAIN PRED': rmae_valid_pred,\n",
    "                         'RMAE FV TRAIN POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV TRAIN POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV TRAIN LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV TRAIN PRED': fd_valid_pred,   \n",
    "                         'FD FV TRAIN POLY': fd_valid_polynomial,   \n",
    "                         'FD FV TRAIN POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV TRAIN LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV TRAIN PRED': dtw_valid_pred, \n",
    "                         'DTW FV TRAIN POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV TRAIN POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV TRAIN LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV TRAIN PRED': std_train_pred,\n",
    "                         'STD FV TRAIN POLY': std_train_polynomial, \n",
    "                         'STD FV TRAIN LSTSQ': std_train_lstsq, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV TRAIN PRED': mean_train_pred,\n",
    "                         'MEAN FV TRAIN POLY': mean_train_polynomial, \n",
    "                         'MEAN FV TRAIN LSTSQ': mean_train_lstsq, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]            \n",
    "                            \n",
    "    else:\n",
    "        result_dict_list = []\n",
    "        pred_list = []\n",
    "        for i in range(epochs//each_epochs_save):\n",
    "            if i == 0:\n",
    "                history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "                history = history.history\n",
    "            else:\n",
    "                model_history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "\n",
    "                for key_1 in history.keys():\n",
    "                    for key_2 in model_history.history.keys():\n",
    "                        if key_1 == key_2:\n",
    "                            history[key_1] += model_history.history[key_2]  \n",
    "            \n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)                \n",
    "            y_pred_valid = model.predict(X_valid)                \n",
    "            y_pred_test = model.predict(X_test)        \n",
    "\n",
    "            term_list_all = []\n",
    "            y = 0\n",
    "            for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "                term_list = [int(value_mult) for value_mult in term]\n",
    "                term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "            terms_matrix = []\n",
    "            for unknowns in X_train:\n",
    "                terms = []\n",
    "                for term_multipliers in term_list_all:\n",
    "                    term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                    terms.append(term_value)\n",
    "                terms_matrix.append(np.array(terms))\n",
    "\n",
    "            terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "            polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_train.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_pred_list.append(polynomial_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            y_test_polynomial = []\n",
    "            if i == 0:\n",
    "                y_test_lstsq = []\n",
    "            for entry in X_test:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_test_polynomial.append(true_function_value_pred)\n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_test_lstsq.append(true_function_value_lstsq)\n",
    "\n",
    "            y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "            if i == 0:\n",
    "                y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "            y_valid_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = []\n",
    "            for entry in X_valid:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_valid_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "            y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1) \n",
    "                \n",
    "                \n",
    "            y_train_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_train_lstsq = []\n",
    "            for entry in X_train:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_train_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_train_lstsq.append(true_function_value_lstsq)     \n",
    "            y_train_polynomial = np.array(y_train_polynomial).reshape(len(y_train_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)    \n",
    "                \n",
    "                \n",
    "            pred_list.append((y_train, y_pred_train, y_train_polynomial, X_train, y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test))\n",
    "\n",
    "            \n",
    "            mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "            mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "            mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "            rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "            rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "            mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "            mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "            r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "            r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "            r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "            raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "            raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "            rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "            rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "            dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "            dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "            if i == 0:\n",
    "                dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "            mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "            mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "            mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "            rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "            rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "            mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "            mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "            r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "            r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "            raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "            raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "            rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "            rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "            if i == 0:\n",
    "                rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "\n",
    "            fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "            dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "            dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "                \n",
    "            mae_train_pred = np.round(mean_absolute_error(y_train, y_pred_train), 4)\n",
    "            mae_train_polynomial = np.round(mean_absolute_error(y_train, y_train_polynomial), 4)\n",
    "            mae_train_polynomial_pred = np.round(mean_absolute_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                mae_train_lstsq = np.round(mean_absolute_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            rmse_train_pred = np.round(root_mean_squared_error(y_train, y_pred_train), 4)\n",
    "            rmse_train_polynomial = np.round(root_mean_squared_error(y_train, y_train_polynomial), 4)\n",
    "            rmse_train_polynomial_pred = np.round(root_mean_squared_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                rmse_train_lstsq = np.round(root_mean_squared_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            mape_train_pred = np.round(mean_absolute_percentage_error_keras(y_train, y_pred_train), 4)\n",
    "            mape_train_polynomial = np.round(mean_absolute_percentage_error_keras(y_train, y_train_polynomial), 4)\n",
    "            mape_train_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                mape_train_lstsq = np.round(mean_absolute_percentage_error_keras(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            r2_train_pred = np.round(r2_score(y_train, y_pred_train), 4)\n",
    "            r2_train_polynomial = np.round(r2_score(y_train, y_train_polynomial), 4)\n",
    "            r2_train_polynomial_pred = np.round(r2_score(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                r2_train_lstsq = np.round(r2_score(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            raae_train_pred = np.round(relative_absolute_average_error(y_train, y_pred_train), 4)\n",
    "            raae_train_polynomial = np.round(relative_absolute_average_error(y_train, y_train_polynomial), 4)\n",
    "            raae_train_polynomial_pred = np.round(relative_absolute_average_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                raae_train_lstsq = np.round(relative_absolute_average_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            rmae_train_pred = np.round(relative_maximum_average_error(y_train, y_pred_train), 4) \n",
    "            rmae_train_polynomial = np.round(relative_maximum_average_error(y_train, y_train_polynomial), 4) \n",
    "            rmae_train_polynomial_pred = np.round(relative_maximum_average_error(y_train_polynomial, y_pred_train), 4) \n",
    "            if i == 0:\n",
    "                rmae_train_lstsq = np.round(relative_maximum_average_error(y_train, y_train_lstsq), 4) \n",
    "\n",
    "            fd_train_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_train_polynomial = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_train_polynomial_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_train_lstsq = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_train_pred, dtw_complete_train_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "            dtw_train_pred = np.round(dtw_train_pred, 4)    \n",
    "            dtw_train_polynomial, dtw_complete_train_polynomial = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_train_polynomial = np.round(dtw_train_polynomial, 4)       \n",
    "            dtw_train_polynomial_pred, dtw_complete_train_polynomial_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "            dtw_train_polynomial_pred = np.round(dtw_train_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_train_lstsq, dtw_complete_train_lstsq = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_train_lstsq = np.round(dtw_train_lstsq, 4)\n",
    "          \n",
    "            std_train_real = np.std(y_train)\n",
    "            std_valid_real = np.std(y_valid)\n",
    "            std_test_real = np.std(y_test)\n",
    "\n",
    "            std_train_pred = np.std(y_pred_train)\n",
    "            std_valid_pred = np.std(y_pred_valid)\n",
    "            std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "            std_train_polynomial = np.std(y_train_polynomial)\n",
    "            std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "            std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                std_train_lstsq = np.std(y_train_lstsq)\n",
    "                std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "                std_test_lstsq = np.std(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            mean_train_real = np.mean(y_train)\n",
    "            mean_valid_real = np.mean(y_valid)\n",
    "            mean_test_real = np.mean(y_test)\n",
    "\n",
    "            mean_train_pred = np.mean(y_pred_train)\n",
    "            mean_valid_pred = np.mean(y_pred_valid)\n",
    "            mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "            mean_train_polynomial = np.mean(y_train_polynomial)\n",
    "            mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "            mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                mean_train_lstsq = np.mean(y_train_lstsq)\n",
    "                mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "                mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            result_dict_list_single_epoch =  [{\n",
    "                         'MAE FV TRAIN PRED': mae_train_pred,\n",
    "                         'MAE FV TRAIN POLY': mae_train_polynomial,\n",
    "                         'MAE FV TRAIN POLY PRED': mae_train_polynomial_pred,\n",
    "                         'MAE FV TRAIN LSTSQ': mae_train_lstsq,\n",
    "                         'RMSE FV TRAIN PRED': rmse_train_pred,\n",
    "                         'RMSE FV TRAIN POLY': rmse_train_polynomial,\n",
    "                         'RMSE FV TRAIN POLY PRED': rmse_train_polynomial_pred,\n",
    "                         'RMSE FV TRAIN LSTSQ': rmse_train_lstsq,\n",
    "                         'MAPE FV TRAIN PRED': mape_train_pred,\n",
    "                         'MAPE FV TRAIN POLY': mape_train_polynomial,\n",
    "                         'MAPE FV TRAIN POLY PRED': mape_train_polynomial_pred,\n",
    "                         'MAPE FV TRAIN LSTSQ': mape_train_lstsq,\n",
    "                         'R2 FV TRAIN PRED': r2_train_pred,\n",
    "                         'R2 FV TRAIN POLY': r2_train_polynomial,\n",
    "                         'R2 FV TRAIN POLY PRED': r2_train_polynomial_pred,\n",
    "                         'R2 FV TRAIN LSTSQ': r2_train_lstsq,\n",
    "                         'RAAE FV PRED TRAIN': raae_train_pred,\n",
    "                         'RAAE FV TRAIN POLY': raae_train_polynomial,\n",
    "                         'RAAE FV TRAIN POLY PRED': raae_train_polynomial_pred,\n",
    "                         'RAAE FV TRAIN LSTSQ': raae_train_lstsq,\n",
    "                         'RMAE FV TRAIN PRED': rmae_train_pred,\n",
    "                         'RMAE FV TRAIN POLY': rmae_train_polynomial,\n",
    "                         'RMAE FV TRAIN POLY PRED': rmae_train_polynomial_pred,\n",
    "                         'RMAE FV TRAIN LSTSQ': rmae_train_lstsq,\n",
    "                         'FD FV TRAIN PRED': fd_train_pred,   \n",
    "                         'FD FV TRAIN POLY': fd_train_polynomial,   \n",
    "                         'FD FV TRAIN POLY PRED': fd_train_polynomial_pred,   \n",
    "                         'FD FV TRAIN LSTSQ': fd_train_lstsq,   \n",
    "                         'DTW FV TRAIN PRED': dtw_train_pred, \n",
    "                         'DTW FV TRAIN POLY': dtw_train_polynomial, \n",
    "                         'DTW FV TRAIN POLY PRED': dtw_train_polynomial_pred, \n",
    "                         'DTW FV TRAIN LSTSQ': dtw_train_lstsq, \n",
    "        },{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]        \n",
    "                  \n",
    "            result_dict_list.append(result_dict_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save != None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, #polynomial_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:21:47.737976Z",
     "start_time": "2020-11-10T19:21:47.708065Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "polynomial = X_data_list[0][0]\n",
    "\n",
    "X_data = X_data_list[0][1].values\n",
    "y_data = y_data_list[0][1].values\n",
    "\n",
    "X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "  \n",
    "\n",
    "term_list_all = []\n",
    "for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "    term_list = [int(value_mult) for value_mult in term]\n",
    "    term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "terms_matrix = []\n",
    "for unknowns in X_train:\n",
    "    terms = []\n",
    "    for term_multipliers in term_list_all:\n",
    "        term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "        terms.append(term_value)\n",
    "    terms_matrix.append(np.array(terms))\n",
    "\n",
    "terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "\n",
    "polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1]\n",
    "\n",
    "y_train_lstsq = []\n",
    "for entry in X_train:\n",
    "    print(entry)\n",
    "    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "    y_train_lstsq.append(true_function_value_lstsq)     \n",
    "\n",
    "y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)  \n",
    "\n",
    "y_train_poly = []\n",
    "for entry in X_train:\n",
    "    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial.values, entry)\n",
    "    y_train_poly.append(true_function_value_lstsq)     \n",
    "\n",
    "y_train_poly = np.array(y_train_poly).reshape(len(y_train_lstsq), 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:18.931506Z",
     "start_time": "2020-11-10T19:21:47.742015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    3.598, max:    9.512, cur:    3.598)\n",
      "\tvalidation       \t (min:    4.387, max:   10.479, cur:    4.387)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.004, max:    3.232, cur:    2.340)\n",
      "\tvalidation       \t (min:    0.946, max:    1.969, cur:    1.471)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    5.215, max:   11.968, cur:    5.215)\n",
      "\tvalidation       \t (min:    6.238, max:   13.493, cur:    6.238)\n"
     ]
    }
   ],
   "source": [
    "%%email lahoffma.ines@gmail.com --to hoffmann@es.uni-mannheim.de --s '[PyMail] Lambda-Net Training Finished' --keep-password\n",
    "\n",
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.697749Z",
     "start_time": "2020-11-10T19:56:18.934068Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    dict_list_train = [clf[1][0] for clf in clf_list]\n",
    "    dict_list_valid = [clf[1][1] for clf in clf_list]\n",
    "    dict_list_test = [clf[1][2] for clf in clf_list]\n",
    "    dict_list_stds = [clf[1][3] for clf in clf_list]\n",
    "    dict_list_means = [clf[1][4] for clf in clf_list]\n",
    "\n",
    "    dict_list_train_mean = pd.DataFrame(dict_list_train, columns=dict_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_dict_list_by_epochs = [[] for i in range(epochs//each_epochs_save)]\n",
    "    for scores_dict_list in scores_list:   \n",
    "        for index, scores_dict in enumerate(scores_dict_list):\n",
    "            scores_dict_list_by_epochs[index].append(scores_dict)\n",
    "            \n",
    "        \n",
    "    for index, scores_dict_list_single_epoch in enumerate(scores_dict_list_by_epochs):\n",
    "        index = (index+1)*each_epochs_save\n",
    "        dict_list_train = [dict_list[0] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_valid = [dict_list[1] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_test = [dict_list[2] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_stds = [dict_list[3] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_means = [dict_list[4] for dict_list in scores_dict_list_single_epoch]\n",
    "        \n",
    "        dict_list_train_mean = pd.DataFrame(dict_list_train, columns=dict_list_train[0].keys()).mean()  \n",
    "        dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()  \n",
    "        dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1*each_epochs_save:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.734711Z",
     "start_time": "2020-11-10T19:56:20.701887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E5</th>\n",
       "      <th>TRAIN POLY E5</th>\n",
       "      <th>TRAIN POLY PRED E5</th>\n",
       "      <th>TRAIN LSTSQ E5</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E15</th>\n",
       "      <th>TRAIN POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E195</th>\n",
       "      <th>TRAIN POLY E195</th>\n",
       "      <th>TRAIN POLY PRED E195</th>\n",
       "      <th>TRAIN LSTSQ E195</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>9.845</td>\n",
       "      <td>9.845</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.566</td>\n",
       "      <td>9.566</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.226</td>\n",
       "      <td>9.227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.134</td>\n",
       "      <td>4.178</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.089</td>\n",
       "      <td>4.133</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.631</td>\n",
       "      <td>12.631</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.343</td>\n",
       "      <td>12.343</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.992</td>\n",
       "      <td>11.992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.862</td>\n",
       "      <td>5.835</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.805</td>\n",
       "      <td>5.776</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.123</td>\n",
       "      <td>1.124</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.243</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.397</td>\n",
       "      <td>1.401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.376</td>\n",
       "      <td>2.476</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.352</td>\n",
       "      <td>2.456</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.427</td>\n",
       "      <td>3.426</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.408</td>\n",
       "      <td>3.406</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.380</td>\n",
       "      <td>3.379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.214</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.265</td>\n",
       "      <td>2.196</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.243</td>\n",
       "      <td>24.241</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.825</td>\n",
       "      <td>23.823</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.297</td>\n",
       "      <td>23.293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.828</td>\n",
       "      <td>10.748</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.739</td>\n",
       "      <td>10.658</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>99.889</td>\n",
       "      <td>99.892</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.145</td>\n",
       "      <td>97.152</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>93.799</td>\n",
       "      <td>93.807</td>\n",
       "      <td>...</td>\n",
       "      <td>3.707</td>\n",
       "      <td>0.000</td>\n",
       "      <td>40.633</td>\n",
       "      <td>41.158</td>\n",
       "      <td>3.823</td>\n",
       "      <td>0.000</td>\n",
       "      <td>40.183</td>\n",
       "      <td>40.723</td>\n",
       "      <td>3.938</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E5  TRAIN POLY E5  TRAIN POLY PRED E5  TRAIN LSTSQ E5  \\\n",
       "MAE FV           9.845          9.845               0.013           0.000   \n",
       "RMSE FV         12.631         12.631               0.016           0.000   \n",
       "MAPE FV          1.123          1.124               0.210           0.000   \n",
       "R2 FV           -0.369         -0.369               0.982           1.000   \n",
       "RAAE FV          0.901          0.901               0.100           0.000   \n",
       "RMAE FV          3.427          3.426               0.524           0.000   \n",
       "FD FV           24.243         24.241               0.028           0.000   \n",
       "DTW FV          99.889         99.892               0.102           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.566           9.566                0.014            0.000   \n",
       "RMSE FV          12.343          12.343                0.018            0.000   \n",
       "MAPE FV           1.243           1.245                0.132            0.000   \n",
       "R2 FV            -0.297          -0.297                0.993            1.000   \n",
       "RAAE FV           0.874           0.874                0.062            0.000   \n",
       "RMAE FV           3.408           3.406                0.318            0.000   \n",
       "FD FV            23.825          23.823                0.031            0.000   \n",
       "DTW FV           97.145          97.152                0.113            0.000   \n",
       "\n",
       "         TRAIN PRED E15  TRAIN POLY E15  ...  TRAIN POLY PRED E190  \\\n",
       "MAE FV            9.226           9.227  ...                 0.402   \n",
       "RMSE FV          11.992          11.992  ...                 0.514   \n",
       "MAPE FV           1.397           1.401  ...                 0.553   \n",
       "R2 FV            -0.213          -0.213  ...                 0.995   \n",
       "RAAE FV           0.841           0.841  ...                 0.054   \n",
       "RMAE FV           3.380           3.379  ...                 0.257   \n",
       "FD FV            23.297          23.293  ...                 0.867   \n",
       "DTW FV           93.799          93.807  ...                 3.707   \n",
       "\n",
       "         TRAIN LSTSQ E190  TRAIN PRED E195  TRAIN POLY E195  \\\n",
       "MAE FV              0.000            4.134            4.178   \n",
       "RMSE FV             0.000            5.862            5.835   \n",
       "MAPE FV             0.000            2.376            2.476   \n",
       "R2 FV               1.000            0.686            0.689   \n",
       "RAAE FV             0.000            0.387            0.391   \n",
       "RMAE FV             0.000            2.281            2.214   \n",
       "FD FV               0.000           10.828           10.748   \n",
       "DTW FV              0.000           40.633           41.158   \n",
       "\n",
       "         TRAIN POLY PRED E195  TRAIN LSTSQ E195  TRAIN PRED E200  \\\n",
       "MAE FV                  0.415             0.000            4.089   \n",
       "RMSE FV                 0.530             0.000            5.805   \n",
       "MAPE FV                 0.442             0.000            2.352   \n",
       "R2 FV                   0.994             1.000            0.692   \n",
       "RAAE FV                 0.055             0.000            0.383   \n",
       "RMAE FV                 0.262             0.000            2.265   \n",
       "FD FV                   0.894             0.000           10.739   \n",
       "DTW FV                  3.823             0.000           40.183   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.133                 0.427             0.000  \n",
       "RMSE FV            5.776                 0.546             0.000  \n",
       "MAPE FV            2.456                 0.443             0.000  \n",
       "R2 FV              0.695                 0.994             1.000  \n",
       "RAAE FV            0.387                 0.057             0.000  \n",
       "RMAE FV            2.196                 0.268             0.000  \n",
       "FD FV             10.658                 0.921             0.000  \n",
       "DTW FV            40.723                 3.938             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.767105Z",
     "start_time": "2020-11-10T19:56:20.737862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E5</th>\n",
       "      <th>VALID POLY E5</th>\n",
       "      <th>VALID POLY PRED E5</th>\n",
       "      <th>VALID LSTSQ E5</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E15</th>\n",
       "      <th>VALID POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E195</th>\n",
       "      <th>VALID POLY E195</th>\n",
       "      <th>VALID POLY PRED E195</th>\n",
       "      <th>VALID LSTSQ E195</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>9.986</td>\n",
       "      <td>9.986</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.710</td>\n",
       "      <td>9.710</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.374</td>\n",
       "      <td>9.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.373</td>\n",
       "      <td>4.388</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.330</td>\n",
       "      <td>4.343</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.754</td>\n",
       "      <td>12.754</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.469</td>\n",
       "      <td>12.469</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.122</td>\n",
       "      <td>12.121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.087</td>\n",
       "      <td>6.033</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.031</td>\n",
       "      <td>5.974</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.194</td>\n",
       "      <td>1.191</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.345</td>\n",
       "      <td>1.344</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.535</td>\n",
       "      <td>1.533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.711</td>\n",
       "      <td>2.801</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.671</td>\n",
       "      <td>2.767</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.906</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>2.929</td>\n",
       "      <td>2.929</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.911</td>\n",
       "      <td>2.910</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.884</td>\n",
       "      <td>2.884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.889</td>\n",
       "      <td>1.836</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.876</td>\n",
       "      <td>1.821</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>22.535</td>\n",
       "      <td>22.532</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.059</td>\n",
       "      <td>22.056</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.455</td>\n",
       "      <td>21.450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.901</td>\n",
       "      <td>9.867</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.812</td>\n",
       "      <td>9.783</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>94.289</td>\n",
       "      <td>94.298</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>91.423</td>\n",
       "      <td>91.436</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>87.962</td>\n",
       "      <td>87.976</td>\n",
       "      <td>...</td>\n",
       "      <td>3.892</td>\n",
       "      <td>0.000</td>\n",
       "      <td>38.115</td>\n",
       "      <td>38.583</td>\n",
       "      <td>4.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>37.742</td>\n",
       "      <td>38.210</td>\n",
       "      <td>4.125</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E5  VALID POLY E5  VALID POLY PRED E5  VALID LSTSQ E5  \\\n",
       "MAE FV           9.986          9.986               0.014           0.000   \n",
       "RMSE FV         12.754         12.754               0.017           0.000   \n",
       "MAPE FV          1.194          1.191               0.187           0.000   \n",
       "R2 FV           -0.372         -0.372               0.978           1.000   \n",
       "RAAE FV          0.906          0.906               0.112           0.000   \n",
       "RMAE FV          2.929          2.929               0.454           0.000   \n",
       "FD FV           22.535         22.532               0.029           0.000   \n",
       "DTW FV          94.289         94.298               0.134           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.710           9.710                0.014            0.000   \n",
       "RMSE FV          12.469          12.469                0.018            0.000   \n",
       "MAPE FV           1.345           1.344                0.127            0.000   \n",
       "R2 FV            -0.303          -0.302                0.992            1.000   \n",
       "RAAE FV           0.880           0.880                0.066            0.000   \n",
       "RMAE FV           2.911           2.910                0.271            0.000   \n",
       "FD FV            22.059          22.056                0.032            0.000   \n",
       "DTW FV           91.423          91.436                0.147            0.000   \n",
       "\n",
       "         VALID PRED E15  VALID POLY E15  ...  VALID POLY PRED E190  \\\n",
       "MAE FV            9.374           9.374  ...                 0.426   \n",
       "RMSE FV          12.122          12.121  ...                 0.545   \n",
       "MAPE FV           1.535           1.533  ...                 0.427   \n",
       "R2 FV            -0.220          -0.220  ...                 0.994   \n",
       "RAAE FV           0.848           0.848  ...                 0.057   \n",
       "RMAE FV           2.884           2.884  ...                 0.226   \n",
       "FD FV            21.455          21.450  ...                 0.900   \n",
       "DTW FV           87.962          87.976  ...                 3.892   \n",
       "\n",
       "         VALID LSTSQ E190  VALID PRED E195  VALID POLY E195  \\\n",
       "MAE FV              0.000            4.373            4.388   \n",
       "RMSE FV             0.000            6.087            6.033   \n",
       "MAPE FV             0.000            2.711            2.801   \n",
       "R2 FV               1.000            0.666            0.671   \n",
       "RAAE FV             0.000            0.406            0.408   \n",
       "RMAE FV             0.000            1.889            1.836   \n",
       "FD FV               0.000            9.901            9.867   \n",
       "DTW FV              0.000           38.115           38.583   \n",
       "\n",
       "         VALID POLY PRED E195  VALID LSTSQ E195  VALID PRED E200  \\\n",
       "MAE FV                  0.440             0.000            4.330   \n",
       "RMSE FV                 0.562             0.000            6.031   \n",
       "MAPE FV                 0.561             0.000            2.671   \n",
       "R2 FV                   0.994             1.000            0.672   \n",
       "RAAE FV                 0.059             0.000            0.402   \n",
       "RMAE FV                 0.231             0.000            1.876   \n",
       "FD FV                   0.927             0.000            9.812   \n",
       "DTW FV                  4.010             0.000           37.742   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.343                 0.453             0.000  \n",
       "RMSE FV            5.974                 0.579             0.000  \n",
       "MAPE FV            2.767                 1.248             0.000  \n",
       "R2 FV              0.677                 0.993             1.000  \n",
       "RAAE FV            0.404                 0.060             0.000  \n",
       "RMAE FV            1.821                 0.236             0.000  \n",
       "FD FV              9.783                 0.954             0.000  \n",
       "DTW FV            38.210                 4.125             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.797209Z",
     "start_time": "2020-11-10T19:56:20.770220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E5</th>\n",
       "      <th>TEST POLY E5</th>\n",
       "      <th>TEST POLY PRED E5</th>\n",
       "      <th>TEST LSTSQ E5</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E15</th>\n",
       "      <th>TEST POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E195</th>\n",
       "      <th>TEST POLY E195</th>\n",
       "      <th>TEST POLY PRED E195</th>\n",
       "      <th>TEST LSTSQ E195</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>9.864</td>\n",
       "      <td>9.864</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.587</td>\n",
       "      <td>9.587</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.251</td>\n",
       "      <td>9.251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.218</td>\n",
       "      <td>4.243</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.175</td>\n",
       "      <td>4.200</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.567</td>\n",
       "      <td>12.567</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.278</td>\n",
       "      <td>12.277</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.925</td>\n",
       "      <td>11.924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.860</td>\n",
       "      <td>5.821</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.805</td>\n",
       "      <td>5.763</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.157</td>\n",
       "      <td>1.155</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.332</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.584</td>\n",
       "      <td>1.584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.062</td>\n",
       "      <td>3.108</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.037</td>\n",
       "      <td>3.076</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.911</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.068</td>\n",
       "      <td>3.067</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.048</td>\n",
       "      <td>3.047</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.020</td>\n",
       "      <td>3.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.967</td>\n",
       "      <td>1.912</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.953</td>\n",
       "      <td>1.896</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.642</td>\n",
       "      <td>24.639</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.181</td>\n",
       "      <td>24.173</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.596</td>\n",
       "      <td>23.586</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.441</td>\n",
       "      <td>10.332</td>\n",
       "      <td>1.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.349</td>\n",
       "      <td>10.237</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>100.029</td>\n",
       "      <td>100.019</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.262</td>\n",
       "      <td>97.238</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>93.920</td>\n",
       "      <td>93.892</td>\n",
       "      <td>...</td>\n",
       "      <td>4.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>40.205</td>\n",
       "      <td>40.433</td>\n",
       "      <td>4.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>39.786</td>\n",
       "      <td>40.018</td>\n",
       "      <td>4.382</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E5  TEST POLY E5  TEST POLY PRED E5  TEST LSTSQ E5  \\\n",
       "MAE FV          9.864         9.864              0.013          0.000   \n",
       "RMSE FV        12.567        12.567              0.017          0.000   \n",
       "MAPE FV         1.157         1.155              0.232          0.000   \n",
       "R2 FV          -0.380        -0.380              0.982          1.000   \n",
       "RAAE FV         0.911         0.911              0.099          0.000   \n",
       "RMAE FV         3.068         3.067              0.516          0.000   \n",
       "FD FV          24.642        24.639              0.026          0.000   \n",
       "DTW FV        100.029       100.019              0.129          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.587          9.587               0.014           0.000   \n",
       "RMSE FV         12.278         12.277               0.018           0.000   \n",
       "MAPE FV          1.333          1.332               0.117           0.000   \n",
       "R2 FV           -0.308         -0.307               0.993           1.000   \n",
       "RAAE FV          0.884          0.884               0.063           0.000   \n",
       "RMAE FV          3.048          3.047               0.331           0.000   \n",
       "FD FV           24.181         24.173               0.033           0.000   \n",
       "DTW FV          97.262         97.238               0.145           0.000   \n",
       "\n",
       "         TEST PRED E15  TEST POLY E15  ...  TEST POLY PRED E190  \\\n",
       "MAE FV           9.251          9.251  ...                0.424   \n",
       "RMSE FV         11.925         11.924  ...                0.543   \n",
       "MAPE FV          1.584          1.584  ...                0.426   \n",
       "R2 FV           -0.222         -0.222  ...                0.994   \n",
       "RAAE FV          0.851          0.851  ...                0.057   \n",
       "RMAE FV          3.020          3.019  ...                0.250   \n",
       "FD FV           23.596         23.586  ...                1.081   \n",
       "DTW FV          93.920         93.892  ...                4.124   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E195  TEST POLY E195  TEST POLY PRED E195  \\\n",
       "MAE FV             0.000           4.218           4.243                0.438   \n",
       "RMSE FV            0.000           5.860           5.821                0.559   \n",
       "MAPE FV            0.000           3.062           3.108                0.527   \n",
       "R2 FV              1.000           0.680           0.683                0.994   \n",
       "RAAE FV            0.000           0.399           0.401                0.058   \n",
       "RMAE FV            0.000           1.967           1.912                0.255   \n",
       "FD FV              0.000          10.441          10.332                1.114   \n",
       "DTW FV             0.000          40.205          40.433                4.253   \n",
       "\n",
       "         TEST LSTSQ E195  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.175           4.200                0.451   \n",
       "RMSE FV            0.000           5.805           5.763                0.576   \n",
       "MAPE FV            0.000           3.037           3.076                0.534   \n",
       "R2 FV              1.000           0.685           0.689                0.993   \n",
       "RAAE FV            0.000           0.395           0.397                0.060   \n",
       "RMAE FV            0.000           1.953           1.896                0.261   \n",
       "FD FV              0.000          10.349          10.237                1.148   \n",
       "DTW FV             0.000          39.786          40.018                4.382   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.828553Z",
     "start_time": "2020-11-10T19:56:20.800785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E5</th>\n",
       "      <th>E10</th>\n",
       "      <th>E15</th>\n",
       "      <th>E20</th>\n",
       "      <th>E25</th>\n",
       "      <th>E30</th>\n",
       "      <th>E35</th>\n",
       "      <th>E40</th>\n",
       "      <th>E45</th>\n",
       "      <th>E50</th>\n",
       "      <th>...</th>\n",
       "      <th>E155</th>\n",
       "      <th>E160</th>\n",
       "      <th>E165</th>\n",
       "      <th>E170</th>\n",
       "      <th>E175</th>\n",
       "      <th>E180</th>\n",
       "      <th>E185</th>\n",
       "      <th>E190</th>\n",
       "      <th>E195</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL</th>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>...</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "      <td>11.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL</th>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>...</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED</th>\n",
       "      <td>0.132</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.907</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.852</td>\n",
       "      <td>2.418</td>\n",
       "      <td>2.994</td>\n",
       "      <td>3.555</td>\n",
       "      <td>...</td>\n",
       "      <td>7.705</td>\n",
       "      <td>7.748</td>\n",
       "      <td>7.790</td>\n",
       "      <td>7.831</td>\n",
       "      <td>7.871</td>\n",
       "      <td>7.911</td>\n",
       "      <td>7.949</td>\n",
       "      <td>7.987</td>\n",
       "      <td>8.025</td>\n",
       "      <td>8.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID POLY</th>\n",
       "      <td>0.130</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.907</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.852</td>\n",
       "      <td>2.419</td>\n",
       "      <td>2.994</td>\n",
       "      <td>3.555</td>\n",
       "      <td>...</td>\n",
       "      <td>7.698</td>\n",
       "      <td>7.740</td>\n",
       "      <td>7.782</td>\n",
       "      <td>7.822</td>\n",
       "      <td>7.861</td>\n",
       "      <td>7.900</td>\n",
       "      <td>7.937</td>\n",
       "      <td>7.974</td>\n",
       "      <td>8.011</td>\n",
       "      <td>8.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID LSTSQ</th>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>...</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "      <td>11.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL</th>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>...</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED</th>\n",
       "      <td>0.135</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1.856</td>\n",
       "      <td>2.425</td>\n",
       "      <td>3.003</td>\n",
       "      <td>3.566</td>\n",
       "      <td>...</td>\n",
       "      <td>7.707</td>\n",
       "      <td>7.748</td>\n",
       "      <td>7.789</td>\n",
       "      <td>7.829</td>\n",
       "      <td>7.868</td>\n",
       "      <td>7.907</td>\n",
       "      <td>7.944</td>\n",
       "      <td>7.981</td>\n",
       "      <td>8.018</td>\n",
       "      <td>8.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST POLY</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.911</td>\n",
       "      <td>1.338</td>\n",
       "      <td>1.857</td>\n",
       "      <td>2.426</td>\n",
       "      <td>3.004</td>\n",
       "      <td>3.568</td>\n",
       "      <td>...</td>\n",
       "      <td>7.704</td>\n",
       "      <td>7.745</td>\n",
       "      <td>7.784</td>\n",
       "      <td>7.823</td>\n",
       "      <td>7.861</td>\n",
       "      <td>7.898</td>\n",
       "      <td>7.935</td>\n",
       "      <td>7.970</td>\n",
       "      <td>8.006</td>\n",
       "      <td>8.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST LSTSQ</th>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>...</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "      <td>10.946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       E5    E10    E15    E20    E25    E30    E35    E40  \\\n",
       "STD FV TRAIN REAL  11.047 11.047 11.047 11.047 11.047 11.047 11.047 11.047   \n",
       "STD FV VALID REAL  11.132 11.132 11.132 11.132 11.132 11.132 11.132 11.132   \n",
       "STD FV VALID PRED   0.132  0.232  0.379  0.594  0.907  1.333  1.852  2.418   \n",
       "STD FV VALID POLY   0.130  0.231  0.379  0.594  0.907  1.333  1.852  2.419   \n",
       "STD FV VALID LSTSQ 11.132 11.132 11.132 11.132 11.132 11.132 11.132 11.132   \n",
       "STD FV TEST REAL   10.946 10.946 10.946 10.946 10.946 10.946 10.946 10.946   \n",
       "STD FV TEST PRED    0.135  0.235  0.382  0.597  0.910  1.337  1.856  2.425   \n",
       "STD FV TEST POLY    0.136  0.236  0.383  0.598  0.911  1.338  1.857  2.426   \n",
       "STD FV TEST LSTSQ  10.946 10.946 10.946 10.946 10.946 10.946 10.946 10.946   \n",
       "\n",
       "                      E45    E50  ...   E155   E160   E165   E170   E175  \\\n",
       "STD FV TRAIN REAL  11.047 11.047  ... 11.047 11.047 11.047 11.047 11.047   \n",
       "STD FV VALID REAL  11.132 11.132  ... 11.132 11.132 11.132 11.132 11.132   \n",
       "STD FV VALID PRED   2.994  3.555  ...  7.705  7.748  7.790  7.831  7.871   \n",
       "STD FV VALID POLY   2.994  3.555  ...  7.698  7.740  7.782  7.822  7.861   \n",
       "STD FV VALID LSTSQ 11.132 11.132  ... 11.132 11.132 11.132 11.132 11.132   \n",
       "STD FV TEST REAL   10.946 10.946  ... 10.946 10.946 10.946 10.946 10.946   \n",
       "STD FV TEST PRED    3.003  3.566  ...  7.707  7.748  7.789  7.829  7.868   \n",
       "STD FV TEST POLY    3.004  3.568  ...  7.704  7.745  7.784  7.823  7.861   \n",
       "STD FV TEST LSTSQ  10.946 10.946  ... 10.946 10.946 10.946 10.946 10.946   \n",
       "\n",
       "                     E180   E185   E190   E195   E200  \n",
       "STD FV TRAIN REAL  11.047 11.047 11.047 11.047 11.047  \n",
       "STD FV VALID REAL  11.132 11.132 11.132 11.132 11.132  \n",
       "STD FV VALID PRED   7.911  7.949  7.987  8.025  8.063  \n",
       "STD FV VALID POLY   7.900  7.937  7.974  8.011  8.049  \n",
       "STD FV VALID LSTSQ 11.132 11.132 11.132 11.132 11.132  \n",
       "STD FV TEST REAL   10.946 10.946 10.946 10.946 10.946  \n",
       "STD FV TEST PRED    7.907  7.944  7.981  8.018  8.055  \n",
       "STD FV TEST POLY    7.898  7.935  7.970  8.006  8.042  \n",
       "STD FV TEST LSTSQ  10.946 10.946 10.946 10.946 10.946  \n",
       "\n",
       "[9 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:56:20.860076Z",
     "start_time": "2020-11-10T19:56:20.831822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E5</th>\n",
       "      <th>E10</th>\n",
       "      <th>E15</th>\n",
       "      <th>E20</th>\n",
       "      <th>E25</th>\n",
       "      <th>E30</th>\n",
       "      <th>E35</th>\n",
       "      <th>E40</th>\n",
       "      <th>E45</th>\n",
       "      <th>E50</th>\n",
       "      <th>...</th>\n",
       "      <th>E155</th>\n",
       "      <th>E160</th>\n",
       "      <th>E165</th>\n",
       "      <th>E170</th>\n",
       "      <th>E175</th>\n",
       "      <th>E180</th>\n",
       "      <th>E185</th>\n",
       "      <th>E190</th>\n",
       "      <th>E195</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL</th>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL</th>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID POLY</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID LSTSQ</th>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST POLY</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST LSTSQ</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       E5   E10   E15   E20   E25   E30   E35   E40   E45  \\\n",
       "MEAN FV TRAIN REAL  0.137 0.137 0.137 0.137 0.137 0.137 0.137 0.137 0.137   \n",
       "MEAN FV VALID REAL  0.112 0.112 0.112 0.112 0.112 0.112 0.112 0.112 0.112   \n",
       "MEAN FV VALID PRED  0.198 0.232 0.298 0.403 0.512 0.567 0.550 0.489 0.424   \n",
       "MEAN FV VALID POLY  0.197 0.231 0.297 0.402 0.511 0.566 0.549 0.488 0.423   \n",
       "MEAN FV VALID LSTSQ 0.112 0.112 0.112 0.112 0.112 0.112 0.112 0.112 0.112   \n",
       "MEAN FV TEST REAL   0.136 0.136 0.136 0.136 0.136 0.136 0.136 0.136 0.136   \n",
       "MEAN FV TEST PRED   0.197 0.232 0.299 0.406 0.518 0.575 0.561 0.501 0.437   \n",
       "MEAN FV TEST POLY   0.197 0.232 0.300 0.407 0.518 0.575 0.561 0.501 0.437   \n",
       "MEAN FV TEST LSTSQ  0.136 0.136 0.136 0.136 0.136 0.136 0.136 0.136 0.136   \n",
       "\n",
       "                      E50  ...  E155  E160  E165  E170  E175  E180  E185  \\\n",
       "MEAN FV TRAIN REAL  0.137  ... 0.137 0.137 0.137 0.137 0.137 0.137 0.137   \n",
       "MEAN FV VALID REAL  0.112  ... 0.112 0.112 0.112 0.112 0.112 0.112 0.112   \n",
       "MEAN FV VALID PRED  0.368  ... 0.193 0.194 0.194 0.194 0.195 0.197 0.198   \n",
       "MEAN FV VALID POLY  0.367  ... 0.191 0.192 0.192 0.192 0.194 0.195 0.196   \n",
       "MEAN FV VALID LSTSQ 0.112  ... 0.112 0.112 0.112 0.112 0.112 0.112 0.112   \n",
       "MEAN FV TEST REAL   0.136  ... 0.136 0.136 0.136 0.136 0.136 0.136 0.136   \n",
       "MEAN FV TEST PRED   0.383  ... 0.219 0.220 0.220 0.220 0.222 0.223 0.224   \n",
       "MEAN FV TEST POLY   0.382  ... 0.217 0.217 0.217 0.218 0.219 0.220 0.221   \n",
       "MEAN FV TEST LSTSQ  0.136  ... 0.136 0.136 0.136 0.136 0.136 0.136 0.136   \n",
       "\n",
       "                     E190  E195  E200  \n",
       "MEAN FV TRAIN REAL  0.137 0.137 0.137  \n",
       "MEAN FV VALID REAL  0.112 0.112 0.112  \n",
       "MEAN FV VALID PRED  0.198 0.198 0.197  \n",
       "MEAN FV VALID POLY  0.197 0.196 0.195  \n",
       "MEAN FV VALID LSTSQ 0.112 0.112 0.112  \n",
       "MEAN FV TEST REAL   0.136 0.136 0.136  \n",
       "MEAN FV TEST PRED   0.225 0.225 0.224  \n",
       "MEAN FV TEST POLY   0.222 0.222 0.221  \n",
       "MEAN FV TEST LSTSQ  0.136 0.136 0.136  \n",
       "\n",
       "[9 rows x 40 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:21.571835Z",
     "start_time": "2020-11-10T19:56:20.863436Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-59a7d38c93eb>:78: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b6536e77cb4593b7f845592597a08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-59a7d38c93eb>:93: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, (pred_evaluation_dataset_train_real_by_epoch, pred_evaluation_dataset_train_by_epoch, pred_evaluation_dataset_train_polynomial_by_epoch, pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_train_real_list, pred_evaluation_dataset_train_list, pred_evaluation_dataset_train_polynomial_list, pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84deaac0d996440f8d5b624e05c5d598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_train_real, pred_evaluation_dataset_train, pred_evaluation_dataset_train_polynomial, X_train_data, pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data = [clf[2] for clf in clf_list]\n",
    "\n",
    "    pred_evaluation_dataset_train_real = pred_evaluation_dataset_train_real.ravel()\n",
    "    pred_evaluation_dataset_valid_real = pred_evaluation_dataset_valid_real.ravel()\n",
    "    pred_evaluation_dataset_test_real = pred_evaluation_dataset_test_real.ravel()\n",
    "    pred_evaluation_dataset_train = pred_evaluation_dataset_train.ravel()\n",
    "    pred_evaluation_dataset_valid = pred_evaluation_dataset_valid.ravel()\n",
    "    pred_evaluation_dataset_test = pred_evaluation_dataset_test.ravel()\n",
    "    pred_evaluation_dataset_train_polynomial = pred_evaluation_dataset_train_polynomial.ravel()\n",
    "    pred_evaluation_dataset_valid_polynomial = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "    pred_evaluation_dataset_test_polynomial = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_train_real, columns=[str(train_data) for train_data in X_train_data])\n",
    "    pred_evaluation_dataset_train_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_train_df = pd.DataFrame(pred_evaluation_dataset_train, columns=[str(train_data) for train_data in X_vtrain_data])\n",
    "    pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_train_polynomial_df = pd.DataFrame(pred_evaluation_dataset_train_polynomial, columns=[str(train_data) for train_data in X_train_data])\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial, columns=[str(test_data) for test_data in X_test_data])    \n",
    "    \n",
    "    pred_evaluation_dataset_train_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_real_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "    pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "    pred_evaluation_dataset_train_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "    pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "    pred_evaluation_dataset_train_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "       \n",
    "    path_pred_evaluation_dataset_train_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_train_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    pred_evaluation_dataset_train_real_df.to_csv(path_pred_evaluation_dataset_train_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_train_df.to_csv(path_pred_evaluation_dataset_train, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)\n",
    "    pred_evaluation_dataset_train_polynomial_df.to_csv(path_pred_evaluation_dataset_train_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_train_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_train_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_train_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][8]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]))) for i in range(epochs//each_epochs_save)]\n",
    "    \n",
    "    for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (pred_evaluation_dataset_train_real, pred_evaluation_dataset_train, pred_evaluation_dataset_train_polynomial, X_train_data, pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data) in enumerate(pred_evaluation_dataset_per_epoch):\n",
    "            pred_evaluation_dataset_train_real_list[index][i] = pred_evaluation_dataset_train_real.ravel()\n",
    "            pred_evaluation_dataset_train_list[index][i] = pred_evaluation_dataset_train.ravel()\n",
    "            pred_evaluation_dataset_train_polynomial_list[index][i] = pred_evaluation_dataset_train_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_valid_real_list[index][i] = pred_evaluation_dataset_valid_real.ravel()\n",
    "            pred_evaluation_dataset_valid_list[index][i] = pred_evaluation_dataset_valid.ravel()\n",
    "            pred_evaluation_dataset_valid_polynomial_list[index][i] = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_test_real_list[index][i] = pred_evaluation_dataset_test_real.ravel()\n",
    "            pred_evaluation_dataset_test_list[index][i] = pred_evaluation_dataset_test.ravel()\n",
    "            pred_evaluation_dataset_test_polynomial_list[index][i] = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    for index, (pred_evaluation_dataset_train_real_by_epoch, pred_evaluation_dataset_train_by_epoch, pred_evaluation_dataset_train_polynomial_by_epoch, pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_train_real_list, pred_evaluation_dataset_train_list, pred_evaluation_dataset_train_polynomial_list, pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n",
    "        \n",
    "        pred_evaluation_dataset_train_real_df = pd.DataFrame(pred_evaluation_dataset_train_real_by_epoch, columns=[str(train_data) for train_data in X_train_data])\n",
    "        pred_evaluation_dataset_train_df = pd.DataFrame(pred_evaluation_dataset_train_by_epoch, columns=[str(train_data) for train_data in X_train_data])\n",
    "        pred_evaluation_dataset_train_polynomial_df = pd.DataFrame(pred_evaluation_dataset_train_polynomial_by_epoch, columns=[str(train_data) for train_data in X_train_data])  \n",
    "        pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])  \n",
    "        pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial_by_epoch, columns=[str(test_data) for test_data in X_test_data])   \n",
    "        \n",
    "        pred_evaluation_dataset_train_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_real_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "        pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "        pred_evaluation_dataset_train_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "        pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "        pred_evaluation_dataset_train_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "\n",
    "        path_pred_evaluation_dataset_train_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_train_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        pred_evaluation_dataset_train_real_df.to_csv(path_pred_evaluation_dataset_train_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)         \n",
    "        pred_evaluation_dataset_train_df.to_csv(path_pred_evaluation_dataset_train, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)    \n",
    "        pred_evaluation_dataset_train_polynomial_df.to_csv(path_pred_evaluation_dataset_train_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T17:14:53.835427Z",
     "start_time": "2020-11-17T17:14:53.789638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.54 -0.36 -0.65 -0.15]</th>\n",
       "      <th>[ 0.19  0.09  0.49 -0.57]</th>\n",
       "      <th>[ 0.31 -0.14 -0.14 -0.53]</th>\n",
       "      <th>[ 0.63  0.3  -0.16  0.07]</th>\n",
       "      <th>[ 0.53  0.2  -0.94  0.75]</th>\n",
       "      <th>[-0.42  0.68 -0.47 -0.49]</th>\n",
       "      <th>[ 0.09  0.01  0.24 -0.88]</th>\n",
       "      <th>[ 0.68 -0.84  0.97 -0.29]</th>\n",
       "      <th>[ 0.44 -0.38  0.65  0.78]</th>\n",
       "      <th>[-0.86 -0.01  0.96  0.45]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>4.667</td>\n",
       "      <td>7.981</td>\n",
       "      <td>17.877</td>\n",
       "      <td>10.108</td>\n",
       "      <td>-2.843</td>\n",
       "      <td>-35.324</td>\n",
       "      <td>-22.465</td>\n",
       "      <td>1.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.242</td>\n",
       "      <td>-17.358</td>\n",
       "      <td>-15.759</td>\n",
       "      <td>-4.426</td>\n",
       "      <td>-11.355</td>\n",
       "      <td>-28.500</td>\n",
       "      <td>-27.527</td>\n",
       "      <td>-24.286</td>\n",
       "      <td>1.330</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.951</td>\n",
       "      <td>-18.182</td>\n",
       "      <td>-14.672</td>\n",
       "      <td>1.023</td>\n",
       "      <td>6.637</td>\n",
       "      <td>-14.289</td>\n",
       "      <td>-25.483</td>\n",
       "      <td>-27.545</td>\n",
       "      <td>-10.465</td>\n",
       "      <td>-37.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.982</td>\n",
       "      <td>8.327</td>\n",
       "      <td>7.513</td>\n",
       "      <td>2.569</td>\n",
       "      <td>-10.336</td>\n",
       "      <td>11.018</td>\n",
       "      <td>13.244</td>\n",
       "      <td>3.942</td>\n",
       "      <td>8.238</td>\n",
       "      <td>15.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>7.215</td>\n",
       "      <td>3.719</td>\n",
       "      <td>1.881</td>\n",
       "      <td>3.211</td>\n",
       "      <td>5.916</td>\n",
       "      <td>-6.406</td>\n",
       "      <td>1.493</td>\n",
       "      <td>1.742</td>\n",
       "      <td>-7.037</td>\n",
       "      <td>4.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.100</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-5.400</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.255</td>\n",
       "      <td>9.222</td>\n",
       "      <td>5.440</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-6.967</td>\n",
       "      <td>-9.329</td>\n",
       "      <td>10.388</td>\n",
       "      <td>-1.525</td>\n",
       "      <td>-14.721</td>\n",
       "      <td>-19.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2.900</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>2.400</td>\n",
       "      <td>6.700</td>\n",
       "      <td>7.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>6.252</td>\n",
       "      <td>5.249</td>\n",
       "      <td>6.221</td>\n",
       "      <td>11.532</td>\n",
       "      <td>19.397</td>\n",
       "      <td>18.871</td>\n",
       "      <td>7.631</td>\n",
       "      <td>-1.640</td>\n",
       "      <td>16.115</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-8.000</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>8.100</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-3.400</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>3.100</td>\n",
       "      <td>5.700</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.505</td>\n",
       "      <td>-3.818</td>\n",
       "      <td>-6.962</td>\n",
       "      <td>-8.111</td>\n",
       "      <td>-42.547</td>\n",
       "      <td>-2.831</td>\n",
       "      <td>-8.025</td>\n",
       "      <td>2.819</td>\n",
       "      <td>-16.974</td>\n",
       "      <td>8.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-2.500</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-9.600</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>6.200</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>5.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>6.021</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-8.297</td>\n",
       "      <td>-1.868</td>\n",
       "      <td>7.185</td>\n",
       "      <td>41.620</td>\n",
       "      <td>-4.349</td>\n",
       "      <td>-3.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>3.400</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-9.000</td>\n",
       "      <td>-8.100</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>5.500</td>\n",
       "      <td>...</td>\n",
       "      <td>3.306</td>\n",
       "      <td>-2.098</td>\n",
       "      <td>2.499</td>\n",
       "      <td>4.268</td>\n",
       "      <td>8.814</td>\n",
       "      <td>5.028</td>\n",
       "      <td>4.647</td>\n",
       "      <td>-24.776</td>\n",
       "      <td>-18.616</td>\n",
       "      <td>-19.202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  \\\n",
       "0    6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400   \n",
       "1   -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500   \n",
       "2   -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900   \n",
       "3    4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000   \n",
       "4    0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "495  1.100 -7.700 -1.400 -0.200  6.700 -5.400  0.800 -6.200 -2.800 -2.100   \n",
       "496  2.900 -0.200  7.500  1.600 -9.800  2.400  6.700  7.100  0.200  0.800   \n",
       "497 -8.000 -6.200 -9.900 -1.400  8.100  5.100 -3.400 -5.000  3.100  5.700   \n",
       "498 -2.500 -0.900  0.600 -9.600  1.400 -1.700 -2.400  6.200 -9.100  5.200   \n",
       "499  3.400  4.500  1.700 -9.000 -8.100 -1.700 -0.500 -9.100 -6.300  5.500   \n",
       "\n",
       "     ...  [ 0.54 -0.36 -0.65 -0.15]  [ 0.19  0.09  0.49 -0.57]  \\\n",
       "0    ...                      2.053                     -1.037   \n",
       "1    ...                    -12.242                    -17.358   \n",
       "2    ...                    -12.951                    -18.182   \n",
       "3    ...                     -3.982                      8.327   \n",
       "4    ...                      7.215                      3.719   \n",
       "..   ...                        ...                        ...   \n",
       "495  ...                     -2.255                      9.222   \n",
       "496  ...                      6.252                      5.249   \n",
       "497  ...                    -22.505                     -3.818   \n",
       "498  ...                     -1.368                      6.021   \n",
       "499  ...                      3.306                     -2.098   \n",
       "\n",
       "     [ 0.31 -0.14 -0.14 -0.53]  [ 0.63  0.3  -0.16  0.07]  \\\n",
       "0                        4.667                      7.981   \n",
       "1                      -15.759                     -4.426   \n",
       "2                      -14.672                      1.023   \n",
       "3                        7.513                      2.569   \n",
       "4                        1.881                      3.211   \n",
       "..                         ...                        ...   \n",
       "495                      5.440                      0.093   \n",
       "496                      6.221                     11.532   \n",
       "497                     -6.962                     -8.111   \n",
       "498                      1.170                     -0.508   \n",
       "499                      2.499                      4.268   \n",
       "\n",
       "     [ 0.53  0.2  -0.94  0.75]  [-0.42  0.68 -0.47 -0.49]  \\\n",
       "0                       17.877                     10.108   \n",
       "1                      -11.355                    -28.500   \n",
       "2                        6.637                    -14.289   \n",
       "3                      -10.336                     11.018   \n",
       "4                        5.916                     -6.406   \n",
       "..                         ...                        ...   \n",
       "495                     -6.967                     -9.329   \n",
       "496                     19.397                     18.871   \n",
       "497                    -42.547                     -2.831   \n",
       "498                     -8.297                     -1.868   \n",
       "499                      8.814                      5.028   \n",
       "\n",
       "     [ 0.09  0.01  0.24 -0.88]  [ 0.68 -0.84  0.97 -0.29]  \\\n",
       "0                       -2.843                    -35.324   \n",
       "1                      -27.527                    -24.286   \n",
       "2                      -25.483                    -27.545   \n",
       "3                       13.244                      3.942   \n",
       "4                        1.493                      1.742   \n",
       "..                         ...                        ...   \n",
       "495                     10.388                     -1.525   \n",
       "496                      7.631                     -1.640   \n",
       "497                     -8.025                      2.819   \n",
       "498                      7.185                     41.620   \n",
       "499                      4.647                    -24.776   \n",
       "\n",
       "     [ 0.44 -0.38  0.65  0.78]  [-0.86 -0.01  0.96  0.45]  \n",
       "0                      -22.465                      1.251  \n",
       "1                        1.330                      0.626  \n",
       "2                      -10.465                    -37.682  \n",
       "3                        8.238                     15.669  \n",
       "4                       -7.037                      4.807  \n",
       "..                         ...                        ...  \n",
       "495                    -14.721                    -19.644  \n",
       "496                     16.115                      0.938  \n",
       "497                    -16.974                      8.344  \n",
       "498                     -4.349                     -3.170  \n",
       "499                    -18.616                    -19.202  \n",
       "\n",
       "[500 rows x 223 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_valid_real_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T17:14:54.231538Z",
     "start_time": "2020-11-17T17:14:54.191683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.54 -0.36 -0.65 -0.15]</th>\n",
       "      <th>[ 0.19  0.09  0.49 -0.57]</th>\n",
       "      <th>[ 0.31 -0.14 -0.14 -0.53]</th>\n",
       "      <th>[ 0.63  0.3  -0.16  0.07]</th>\n",
       "      <th>[ 0.53  0.2  -0.94  0.75]</th>\n",
       "      <th>[-0.42  0.68 -0.47 -0.49]</th>\n",
       "      <th>[ 0.09  0.01  0.24 -0.88]</th>\n",
       "      <th>[ 0.68 -0.84  0.97 -0.29]</th>\n",
       "      <th>[ 0.44 -0.38  0.65  0.78]</th>\n",
       "      <th>[-0.86 -0.01  0.96  0.45]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>3.692</td>\n",
       "      <td>-2.203</td>\n",
       "      <td>2.486</td>\n",
       "      <td>5.450</td>\n",
       "      <td>9.484</td>\n",
       "      <td>8.773</td>\n",
       "      <td>1.008</td>\n",
       "      <td>-18.103</td>\n",
       "      <td>-19.749</td>\n",
       "      <td>-5.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.617</td>\n",
       "      <td>-20.127</td>\n",
       "      <td>-20.626</td>\n",
       "      <td>-8.356</td>\n",
       "      <td>-4.783</td>\n",
       "      <td>-29.874</td>\n",
       "      <td>-27.733</td>\n",
       "      <td>-10.627</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-8.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.894</td>\n",
       "      <td>-21.447</td>\n",
       "      <td>-19.140</td>\n",
       "      <td>-3.476</td>\n",
       "      <td>8.628</td>\n",
       "      <td>-13.647</td>\n",
       "      <td>-24.337</td>\n",
       "      <td>-24.695</td>\n",
       "      <td>-8.498</td>\n",
       "      <td>-22.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.817</td>\n",
       "      <td>10.107</td>\n",
       "      <td>7.483</td>\n",
       "      <td>3.636</td>\n",
       "      <td>-11.922</td>\n",
       "      <td>4.975</td>\n",
       "      <td>11.538</td>\n",
       "      <td>10.601</td>\n",
       "      <td>7.714</td>\n",
       "      <td>10.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>5.636</td>\n",
       "      <td>6.074</td>\n",
       "      <td>6.092</td>\n",
       "      <td>4.787</td>\n",
       "      <td>-2.191</td>\n",
       "      <td>1.284</td>\n",
       "      <td>6.706</td>\n",
       "      <td>4.480</td>\n",
       "      <td>-3.478</td>\n",
       "      <td>-3.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.100</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-5.400</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.960</td>\n",
       "      <td>8.653</td>\n",
       "      <td>7.091</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-10.202</td>\n",
       "      <td>-6.897</td>\n",
       "      <td>10.758</td>\n",
       "      <td>3.634</td>\n",
       "      <td>-14.543</td>\n",
       "      <td>-9.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2.900</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>2.400</td>\n",
       "      <td>6.700</td>\n",
       "      <td>7.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>9.572</td>\n",
       "      <td>9.871</td>\n",
       "      <td>8.799</td>\n",
       "      <td>16.301</td>\n",
       "      <td>15.747</td>\n",
       "      <td>18.858</td>\n",
       "      <td>10.466</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>4.942</td>\n",
       "      <td>-1.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-8.000</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>8.100</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-3.400</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>3.100</td>\n",
       "      <td>5.700</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.849</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-6.451</td>\n",
       "      <td>-11.873</td>\n",
       "      <td>-39.551</td>\n",
       "      <td>-3.007</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>-16.648</td>\n",
       "      <td>-2.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-2.500</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-9.600</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>6.200</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>5.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955</td>\n",
       "      <td>7.698</td>\n",
       "      <td>4.759</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-3.965</td>\n",
       "      <td>-5.827</td>\n",
       "      <td>7.623</td>\n",
       "      <td>26.231</td>\n",
       "      <td>5.241</td>\n",
       "      <td>-7.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>3.400</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-9.000</td>\n",
       "      <td>-8.100</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>5.500</td>\n",
       "      <td>...</td>\n",
       "      <td>4.409</td>\n",
       "      <td>-1.708</td>\n",
       "      <td>2.505</td>\n",
       "      <td>5.998</td>\n",
       "      <td>9.426</td>\n",
       "      <td>4.537</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-15.471</td>\n",
       "      <td>-11.840</td>\n",
       "      <td>-15.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  \\\n",
       "0    6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400   \n",
       "1   -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500   \n",
       "2   -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900   \n",
       "3    4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000   \n",
       "4    0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "495  1.100 -7.700 -1.400 -0.200  6.700 -5.400  0.800 -6.200 -2.800 -2.100   \n",
       "496  2.900 -0.200  7.500  1.600 -9.800  2.400  6.700  7.100  0.200  0.800   \n",
       "497 -8.000 -6.200 -9.900 -1.400  8.100  5.100 -3.400 -5.000  3.100  5.700   \n",
       "498 -2.500 -0.900  0.600 -9.600  1.400 -1.700 -2.400  6.200 -9.100  5.200   \n",
       "499  3.400  4.500  1.700 -9.000 -8.100 -1.700 -0.500 -9.100 -6.300  5.500   \n",
       "\n",
       "     ...  [ 0.54 -0.36 -0.65 -0.15]  [ 0.19  0.09  0.49 -0.57]  \\\n",
       "0    ...                      3.692                     -2.203   \n",
       "1    ...                    -12.617                    -20.127   \n",
       "2    ...                    -10.894                    -21.447   \n",
       "3    ...                     -1.817                     10.107   \n",
       "4    ...                      5.636                      6.074   \n",
       "..   ...                        ...                        ...   \n",
       "495  ...                     -2.960                      8.653   \n",
       "496  ...                      9.572                      9.871   \n",
       "497  ...                    -25.849                     -0.551   \n",
       "498  ...                      0.955                      7.698   \n",
       "499  ...                      4.409                     -1.708   \n",
       "\n",
       "     [ 0.31 -0.14 -0.14 -0.53]  [ 0.63  0.3  -0.16  0.07]  \\\n",
       "0                        2.486                      5.450   \n",
       "1                      -20.626                     -8.356   \n",
       "2                      -19.140                     -3.476   \n",
       "3                        7.483                      3.636   \n",
       "4                        6.092                      4.787   \n",
       "..                         ...                        ...   \n",
       "495                      7.091                      0.946   \n",
       "496                      8.799                     16.301   \n",
       "497                     -6.451                    -11.873   \n",
       "498                      4.759                     -0.187   \n",
       "499                      2.505                      5.998   \n",
       "\n",
       "     [ 0.53  0.2  -0.94  0.75]  [-0.42  0.68 -0.47 -0.49]  \\\n",
       "0                        9.484                      8.773   \n",
       "1                       -4.783                    -29.874   \n",
       "2                        8.628                    -13.647   \n",
       "3                      -11.922                      4.975   \n",
       "4                       -2.191                      1.284   \n",
       "..                         ...                        ...   \n",
       "495                    -10.202                     -6.897   \n",
       "496                     15.747                     18.858   \n",
       "497                    -39.551                     -3.007   \n",
       "498                     -3.965                     -5.827   \n",
       "499                      9.426                      4.537   \n",
       "\n",
       "     [ 0.09  0.01  0.24 -0.88]  [ 0.68 -0.84  0.97 -0.29]  \\\n",
       "0                        1.008                    -18.103   \n",
       "1                      -27.733                    -10.627   \n",
       "2                      -24.337                    -24.695   \n",
       "3                       11.538                     10.601   \n",
       "4                        6.706                      4.480   \n",
       "..                         ...                        ...   \n",
       "495                     10.758                      3.634   \n",
       "496                     10.466                     -0.107   \n",
       "497                     -2.432                     -2.656   \n",
       "498                      7.623                     26.231   \n",
       "499                      0.073                    -15.471   \n",
       "\n",
       "     [ 0.44 -0.38  0.65  0.78]  [-0.86 -0.01  0.96  0.45]  \n",
       "0                      -19.749                     -5.314  \n",
       "1                       -0.121                     -8.043  \n",
       "2                       -8.498                    -22.892  \n",
       "3                        7.714                     10.169  \n",
       "4                       -3.478                     -3.858  \n",
       "..                         ...                        ...  \n",
       "495                    -14.543                     -9.020  \n",
       "496                      4.942                     -1.550  \n",
       "497                    -16.648                     -2.982  \n",
       "498                      5.241                     -7.269  \n",
       "499                    -11.840                    -15.200  \n",
       "\n",
       "[500 rows x 223 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T17:14:54.650111Z",
     "start_time": "2020-11-17T17:14:54.609586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.54 -0.36 -0.65 -0.15]</th>\n",
       "      <th>[ 0.19  0.09  0.49 -0.57]</th>\n",
       "      <th>[ 0.31 -0.14 -0.14 -0.53]</th>\n",
       "      <th>[ 0.63  0.3  -0.16  0.07]</th>\n",
       "      <th>[ 0.53  0.2  -0.94  0.75]</th>\n",
       "      <th>[-0.42  0.68 -0.47 -0.49]</th>\n",
       "      <th>[ 0.09  0.01  0.24 -0.88]</th>\n",
       "      <th>[ 0.68 -0.84  0.97 -0.29]</th>\n",
       "      <th>[ 0.44 -0.38  0.65  0.78]</th>\n",
       "      <th>[-0.86 -0.01  0.96  0.45]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>2.918</td>\n",
       "      <td>-1.778</td>\n",
       "      <td>2.272</td>\n",
       "      <td>4.022</td>\n",
       "      <td>10.802</td>\n",
       "      <td>9.615</td>\n",
       "      <td>1.283</td>\n",
       "      <td>-18.217</td>\n",
       "      <td>-18.587</td>\n",
       "      <td>-5.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.450</td>\n",
       "      <td>-19.843</td>\n",
       "      <td>-20.068</td>\n",
       "      <td>-9.845</td>\n",
       "      <td>-4.659</td>\n",
       "      <td>-29.727</td>\n",
       "      <td>-27.834</td>\n",
       "      <td>-12.208</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-7.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.698</td>\n",
       "      <td>-21.591</td>\n",
       "      <td>-18.135</td>\n",
       "      <td>-3.749</td>\n",
       "      <td>9.527</td>\n",
       "      <td>-13.344</td>\n",
       "      <td>-24.774</td>\n",
       "      <td>-25.314</td>\n",
       "      <td>-8.372</td>\n",
       "      <td>-23.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>11.199</td>\n",
       "      <td>6.850</td>\n",
       "      <td>3.173</td>\n",
       "      <td>-11.667</td>\n",
       "      <td>4.660</td>\n",
       "      <td>12.429</td>\n",
       "      <td>9.535</td>\n",
       "      <td>7.732</td>\n",
       "      <td>9.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>5.468</td>\n",
       "      <td>6.361</td>\n",
       "      <td>6.492</td>\n",
       "      <td>3.930</td>\n",
       "      <td>-1.780</td>\n",
       "      <td>1.570</td>\n",
       "      <td>7.254</td>\n",
       "      <td>4.270</td>\n",
       "      <td>-3.447</td>\n",
       "      <td>-3.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.100</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-5.400</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.241</td>\n",
       "      <td>8.431</td>\n",
       "      <td>5.086</td>\n",
       "      <td>0.787</td>\n",
       "      <td>-9.952</td>\n",
       "      <td>-6.001</td>\n",
       "      <td>10.502</td>\n",
       "      <td>3.813</td>\n",
       "      <td>-13.464</td>\n",
       "      <td>-9.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2.900</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>2.400</td>\n",
       "      <td>6.700</td>\n",
       "      <td>7.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>9.985</td>\n",
       "      <td>9.110</td>\n",
       "      <td>8.864</td>\n",
       "      <td>16.272</td>\n",
       "      <td>16.398</td>\n",
       "      <td>18.361</td>\n",
       "      <td>9.363</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>4.851</td>\n",
       "      <td>-2.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-8.000</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>8.100</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-3.400</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>3.100</td>\n",
       "      <td>5.700</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.779</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-7.938</td>\n",
       "      <td>-11.803</td>\n",
       "      <td>-40.495</td>\n",
       "      <td>-3.706</td>\n",
       "      <td>-2.168</td>\n",
       "      <td>-3.430</td>\n",
       "      <td>-16.462</td>\n",
       "      <td>-1.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-2.500</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-9.600</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>6.200</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>5.200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223</td>\n",
       "      <td>6.687</td>\n",
       "      <td>5.124</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-5.173</td>\n",
       "      <td>-6.176</td>\n",
       "      <td>6.835</td>\n",
       "      <td>28.986</td>\n",
       "      <td>4.895</td>\n",
       "      <td>-7.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>3.400</td>\n",
       "      <td>4.500</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-9.000</td>\n",
       "      <td>-8.100</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-9.100</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>5.500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.322</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>1.708</td>\n",
       "      <td>6.454</td>\n",
       "      <td>8.394</td>\n",
       "      <td>4.865</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-16.058</td>\n",
       "      <td>-11.767</td>\n",
       "      <td>-14.419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  \\\n",
       "0    6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400   \n",
       "1   -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500   \n",
       "2   -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900   \n",
       "3    4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000   \n",
       "4    0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "495  1.100 -7.700 -1.400 -0.200  6.700 -5.400  0.800 -6.200 -2.800 -2.100   \n",
       "496  2.900 -0.200  7.500  1.600 -9.800  2.400  6.700  7.100  0.200  0.800   \n",
       "497 -8.000 -6.200 -9.900 -1.400  8.100  5.100 -3.400 -5.000  3.100  5.700   \n",
       "498 -2.500 -0.900  0.600 -9.600  1.400 -1.700 -2.400  6.200 -9.100  5.200   \n",
       "499  3.400  4.500  1.700 -9.000 -8.100 -1.700 -0.500 -9.100 -6.300  5.500   \n",
       "\n",
       "     ...  [ 0.54 -0.36 -0.65 -0.15]  [ 0.19  0.09  0.49 -0.57]  \\\n",
       "0    ...                      2.918                     -1.778   \n",
       "1    ...                    -12.450                    -19.843   \n",
       "2    ...                    -10.698                    -21.591   \n",
       "3    ...                     -1.568                     11.199   \n",
       "4    ...                      5.468                      6.361   \n",
       "..   ...                        ...                        ...   \n",
       "495  ...                     -3.241                      8.431   \n",
       "496  ...                      9.985                      9.110   \n",
       "497  ...                    -24.779                     -0.498   \n",
       "498  ...                      2.223                      6.687   \n",
       "499  ...                      2.322                     -0.971   \n",
       "\n",
       "     [ 0.31 -0.14 -0.14 -0.53]  [ 0.63  0.3  -0.16  0.07]  \\\n",
       "0                        2.272                      4.022   \n",
       "1                      -20.068                     -9.845   \n",
       "2                      -18.135                     -3.749   \n",
       "3                        6.850                      3.173   \n",
       "4                        6.492                      3.930   \n",
       "..                         ...                        ...   \n",
       "495                      5.086                      0.787   \n",
       "496                      8.864                     16.272   \n",
       "497                     -7.938                    -11.803   \n",
       "498                      5.124                      0.509   \n",
       "499                      1.708                      6.454   \n",
       "\n",
       "     [ 0.53  0.2  -0.94  0.75]  [-0.42  0.68 -0.47 -0.49]  \\\n",
       "0                       10.802                      9.615   \n",
       "1                       -4.659                    -29.727   \n",
       "2                        9.527                    -13.344   \n",
       "3                      -11.667                      4.660   \n",
       "4                       -1.780                      1.570   \n",
       "..                         ...                        ...   \n",
       "495                     -9.952                     -6.001   \n",
       "496                     16.398                     18.361   \n",
       "497                    -40.495                     -3.706   \n",
       "498                     -5.173                     -6.176   \n",
       "499                      8.394                      4.865   \n",
       "\n",
       "     [ 0.09  0.01  0.24 -0.88]  [ 0.68 -0.84  0.97 -0.29]  \\\n",
       "0                        1.283                    -18.217   \n",
       "1                      -27.834                    -12.208   \n",
       "2                      -24.774                    -25.314   \n",
       "3                       12.429                      9.535   \n",
       "4                        7.254                      4.270   \n",
       "..                         ...                        ...   \n",
       "495                     10.502                      3.813   \n",
       "496                      9.363                     -0.025   \n",
       "497                     -2.168                     -3.430   \n",
       "498                      6.835                     28.986   \n",
       "499                      0.751                    -16.058   \n",
       "\n",
       "     [ 0.44 -0.38  0.65  0.78]  [-0.86 -0.01  0.96  0.45]  \n",
       "0                      -18.587                     -5.749  \n",
       "1                        0.281                     -7.276  \n",
       "2                       -8.372                    -23.930  \n",
       "3                        7.732                      9.759  \n",
       "4                       -3.447                     -3.301  \n",
       "..                         ...                        ...  \n",
       "495                    -13.464                     -9.456  \n",
       "496                      4.851                     -2.471  \n",
       "497                    -16.462                     -1.369  \n",
       "498                      4.895                     -7.648  \n",
       "499                    -11.767                    -14.419  \n",
       "\n",
       "[500 rows x 223 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_valid_polynomial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:21.652482Z",
     "start_time": "2020-11-10T19:59:21.650349Z"
    }
   },
   "outputs": [],
   "source": [
    "#variable_values = []\n",
    "#for column in pred_evaluation_dataset_df.columns:\n",
    "#    variable_values.append(np.array(column[1:-1].split()).astype('float'))\n",
    "#variable_values = np.array(variable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:21.660200Z",
     "start_time": "2020-11-10T19:59:21.654575Z"
    }
   },
   "outputs": [],
   "source": [
    "#fv_with_vv = []\n",
    "#for function_values in tqdm(pred_evaluation_dataset_df.values):\n",
    "#    fv_with_vv.append(np.array([np.append(vv, fv) for fv, vv in zip(function_values, variable_values)]))\n",
    "#fv_with_vv = np.array(fv_with_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:21.974242Z",
     "start_time": "2020-11-10T19:59:21.662604Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-b23760cfb23d>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _, entry in tqdm(enumerate(clf_list)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402d52a086544cf39d3fccc92d85f1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:22.919436Z",
     "start_time": "2020-11-10T19:59:21.976660Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:23.460855Z",
     "start_time": "2020-11-10T19:59:22.922012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.080</td>\n",
       "      <td>10.029</td>\n",
       "      <td>9.978</td>\n",
       "      <td>9.926</td>\n",
       "      <td>9.874</td>\n",
       "      <td>9.822</td>\n",
       "      <td>9.769</td>\n",
       "      <td>9.714</td>\n",
       "      <td>9.658</td>\n",
       "      <td>9.600</td>\n",
       "      <td>...</td>\n",
       "      <td>4.179</td>\n",
       "      <td>4.170</td>\n",
       "      <td>4.161</td>\n",
       "      <td>4.152</td>\n",
       "      <td>4.143</td>\n",
       "      <td>4.134</td>\n",
       "      <td>4.125</td>\n",
       "      <td>4.116</td>\n",
       "      <td>4.107</td>\n",
       "      <td>4.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.357</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.310</td>\n",
       "      <td>2.287</td>\n",
       "      <td>2.264</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.190</td>\n",
       "      <td>2.165</td>\n",
       "      <td>2.138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.093</td>\n",
       "      <td>5.088</td>\n",
       "      <td>5.084</td>\n",
       "      <td>5.080</td>\n",
       "      <td>5.077</td>\n",
       "      <td>5.073</td>\n",
       "      <td>5.069</td>\n",
       "      <td>5.065</td>\n",
       "      <td>5.061</td>\n",
       "      <td>5.058</td>\n",
       "      <td>...</td>\n",
       "      <td>2.836</td>\n",
       "      <td>2.831</td>\n",
       "      <td>2.826</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.816</td>\n",
       "      <td>2.810</td>\n",
       "      <td>2.806</td>\n",
       "      <td>2.801</td>\n",
       "      <td>2.797</td>\n",
       "      <td>2.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.313</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.254</td>\n",
       "      <td>8.223</td>\n",
       "      <td>8.201</td>\n",
       "      <td>8.147</td>\n",
       "      <td>8.089</td>\n",
       "      <td>8.025</td>\n",
       "      <td>7.992</td>\n",
       "      <td>7.911</td>\n",
       "      <td>...</td>\n",
       "      <td>3.809</td>\n",
       "      <td>3.799</td>\n",
       "      <td>3.791</td>\n",
       "      <td>3.783</td>\n",
       "      <td>3.775</td>\n",
       "      <td>3.768</td>\n",
       "      <td>3.761</td>\n",
       "      <td>3.755</td>\n",
       "      <td>3.746</td>\n",
       "      <td>3.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.752</td>\n",
       "      <td>9.686</td>\n",
       "      <td>9.633</td>\n",
       "      <td>9.573</td>\n",
       "      <td>9.516</td>\n",
       "      <td>9.478</td>\n",
       "      <td>9.429</td>\n",
       "      <td>9.358</td>\n",
       "      <td>9.322</td>\n",
       "      <td>9.291</td>\n",
       "      <td>...</td>\n",
       "      <td>4.165</td>\n",
       "      <td>4.155</td>\n",
       "      <td>4.144</td>\n",
       "      <td>4.132</td>\n",
       "      <td>4.123</td>\n",
       "      <td>4.111</td>\n",
       "      <td>4.098</td>\n",
       "      <td>4.088</td>\n",
       "      <td>4.082</td>\n",
       "      <td>4.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.698</td>\n",
       "      <td>11.636</td>\n",
       "      <td>11.557</td>\n",
       "      <td>11.472</td>\n",
       "      <td>11.390</td>\n",
       "      <td>11.318</td>\n",
       "      <td>11.219</td>\n",
       "      <td>11.147</td>\n",
       "      <td>11.070</td>\n",
       "      <td>10.990</td>\n",
       "      <td>...</td>\n",
       "      <td>4.487</td>\n",
       "      <td>4.476</td>\n",
       "      <td>4.467</td>\n",
       "      <td>4.457</td>\n",
       "      <td>4.448</td>\n",
       "      <td>4.441</td>\n",
       "      <td>4.434</td>\n",
       "      <td>4.422</td>\n",
       "      <td>4.416</td>\n",
       "      <td>4.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.917</td>\n",
       "      <td>18.807</td>\n",
       "      <td>18.697</td>\n",
       "      <td>18.587</td>\n",
       "      <td>18.476</td>\n",
       "      <td>18.363</td>\n",
       "      <td>18.249</td>\n",
       "      <td>18.131</td>\n",
       "      <td>18.008</td>\n",
       "      <td>17.880</td>\n",
       "      <td>...</td>\n",
       "      <td>5.635</td>\n",
       "      <td>5.624</td>\n",
       "      <td>5.612</td>\n",
       "      <td>5.599</td>\n",
       "      <td>5.587</td>\n",
       "      <td>5.574</td>\n",
       "      <td>5.562</td>\n",
       "      <td>5.550</td>\n",
       "      <td>5.537</td>\n",
       "      <td>5.524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count       500.000       500.000       500.000       500.000       500.000   \n",
       "mean         10.080        10.029         9.978         9.926         9.874   \n",
       "std           2.357         2.333         2.310         2.287         2.264   \n",
       "min           5.093         5.088         5.084         5.080         5.077   \n",
       "25%           8.313         8.290         8.254         8.223         8.201   \n",
       "50%           9.752         9.686         9.633         9.573         9.516   \n",
       "75%          11.698        11.636        11.557        11.472        11.390   \n",
       "max          18.917        18.807        18.697        18.587        18.476   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count       500.000       500.000       500.000       500.000        500.000   \n",
       "mean          9.822         9.769         9.714         9.658          9.600   \n",
       "std           2.240         2.216         2.190         2.165          2.138   \n",
       "min           5.073         5.069         5.065         5.061          5.058   \n",
       "25%           8.147         8.089         8.025         7.992          7.911   \n",
       "50%           9.478         9.429         9.358         9.322          9.291   \n",
       "75%          11.318        11.219        11.147        11.070         10.990   \n",
       "max          18.363        18.249        18.131        18.008         17.880   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...         500.000         500.000         500.000         500.000   \n",
       "mean   ...           4.179           4.170           4.161           4.152   \n",
       "std    ...           0.494           0.493           0.492           0.492   \n",
       "min    ...           2.836           2.831           2.826           2.822   \n",
       "25%    ...           3.809           3.799           3.791           3.783   \n",
       "50%    ...           4.165           4.155           4.144           4.132   \n",
       "75%    ...           4.487           4.476           4.467           4.457   \n",
       "max    ...           5.635           5.624           5.612           5.599   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count         500.000         500.000         500.000         500.000   \n",
       "mean            4.143           4.134           4.125           4.116   \n",
       "std             0.491           0.491           0.490           0.489   \n",
       "min             2.816           2.810           2.806           2.801   \n",
       "25%             3.775           3.768           3.761           3.755   \n",
       "50%             4.123           4.111           4.098           4.088   \n",
       "75%             4.448           4.441           4.434           4.422   \n",
       "max             5.587           5.574           5.562           5.550   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count         500.000         500.000  \n",
       "mean            4.107           4.098  \n",
       "std             0.489           0.488  \n",
       "min             2.797           2.792  \n",
       "25%             3.746           3.738  \n",
       "50%             4.082           4.075  \n",
       "75%             4.416           4.406  \n",
       "max             5.537           5.524  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:23.995640Z",
     "start_time": "2020-11-10T19:59:23.462869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.190</td>\n",
       "      <td>10.140</td>\n",
       "      <td>10.089</td>\n",
       "      <td>10.038</td>\n",
       "      <td>9.986</td>\n",
       "      <td>9.934</td>\n",
       "      <td>9.880</td>\n",
       "      <td>9.825</td>\n",
       "      <td>9.769</td>\n",
       "      <td>9.710</td>\n",
       "      <td>...</td>\n",
       "      <td>4.408</td>\n",
       "      <td>4.399</td>\n",
       "      <td>4.390</td>\n",
       "      <td>4.382</td>\n",
       "      <td>4.373</td>\n",
       "      <td>4.364</td>\n",
       "      <td>4.356</td>\n",
       "      <td>4.347</td>\n",
       "      <td>4.338</td>\n",
       "      <td>4.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.451</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.405</td>\n",
       "      <td>2.382</td>\n",
       "      <td>2.358</td>\n",
       "      <td>2.335</td>\n",
       "      <td>2.310</td>\n",
       "      <td>2.285</td>\n",
       "      <td>2.259</td>\n",
       "      <td>2.232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.188</td>\n",
       "      <td>5.175</td>\n",
       "      <td>5.161</td>\n",
       "      <td>5.148</td>\n",
       "      <td>5.134</td>\n",
       "      <td>5.121</td>\n",
       "      <td>5.107</td>\n",
       "      <td>5.093</td>\n",
       "      <td>5.080</td>\n",
       "      <td>5.066</td>\n",
       "      <td>...</td>\n",
       "      <td>3.063</td>\n",
       "      <td>3.051</td>\n",
       "      <td>3.039</td>\n",
       "      <td>3.028</td>\n",
       "      <td>3.017</td>\n",
       "      <td>3.007</td>\n",
       "      <td>2.996</td>\n",
       "      <td>2.984</td>\n",
       "      <td>2.973</td>\n",
       "      <td>2.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.376</td>\n",
       "      <td>8.334</td>\n",
       "      <td>8.300</td>\n",
       "      <td>8.272</td>\n",
       "      <td>8.247</td>\n",
       "      <td>8.210</td>\n",
       "      <td>8.178</td>\n",
       "      <td>8.146</td>\n",
       "      <td>8.105</td>\n",
       "      <td>8.025</td>\n",
       "      <td>...</td>\n",
       "      <td>3.967</td>\n",
       "      <td>3.955</td>\n",
       "      <td>3.948</td>\n",
       "      <td>3.942</td>\n",
       "      <td>3.932</td>\n",
       "      <td>3.926</td>\n",
       "      <td>3.920</td>\n",
       "      <td>3.912</td>\n",
       "      <td>3.905</td>\n",
       "      <td>3.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.837</td>\n",
       "      <td>9.809</td>\n",
       "      <td>9.783</td>\n",
       "      <td>9.746</td>\n",
       "      <td>9.685</td>\n",
       "      <td>9.640</td>\n",
       "      <td>9.594</td>\n",
       "      <td>9.554</td>\n",
       "      <td>9.495</td>\n",
       "      <td>9.441</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385</td>\n",
       "      <td>4.373</td>\n",
       "      <td>4.365</td>\n",
       "      <td>4.359</td>\n",
       "      <td>4.351</td>\n",
       "      <td>4.339</td>\n",
       "      <td>4.330</td>\n",
       "      <td>4.322</td>\n",
       "      <td>4.311</td>\n",
       "      <td>4.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.743</td>\n",
       "      <td>11.657</td>\n",
       "      <td>11.559</td>\n",
       "      <td>11.490</td>\n",
       "      <td>11.447</td>\n",
       "      <td>11.328</td>\n",
       "      <td>11.246</td>\n",
       "      <td>11.196</td>\n",
       "      <td>11.110</td>\n",
       "      <td>11.050</td>\n",
       "      <td>...</td>\n",
       "      <td>4.801</td>\n",
       "      <td>4.791</td>\n",
       "      <td>4.780</td>\n",
       "      <td>4.773</td>\n",
       "      <td>4.767</td>\n",
       "      <td>4.756</td>\n",
       "      <td>4.747</td>\n",
       "      <td>4.736</td>\n",
       "      <td>4.728</td>\n",
       "      <td>4.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.023</td>\n",
       "      <td>18.912</td>\n",
       "      <td>18.802</td>\n",
       "      <td>18.690</td>\n",
       "      <td>18.578</td>\n",
       "      <td>18.464</td>\n",
       "      <td>18.346</td>\n",
       "      <td>18.226</td>\n",
       "      <td>18.101</td>\n",
       "      <td>17.968</td>\n",
       "      <td>...</td>\n",
       "      <td>6.411</td>\n",
       "      <td>6.405</td>\n",
       "      <td>6.398</td>\n",
       "      <td>6.392</td>\n",
       "      <td>6.387</td>\n",
       "      <td>6.384</td>\n",
       "      <td>6.378</td>\n",
       "      <td>6.371</td>\n",
       "      <td>6.366</td>\n",
       "      <td>6.361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count           500.000           500.000           500.000           500.000   \n",
       "mean             10.190            10.140            10.089            10.038   \n",
       "std               2.451             2.428             2.405             2.382   \n",
       "min               5.188             5.175             5.161             5.148   \n",
       "25%               8.376             8.334             8.300             8.272   \n",
       "50%               9.837             9.809             9.783             9.746   \n",
       "75%              11.743            11.657            11.559            11.490   \n",
       "max              19.023            18.912            18.802            18.690   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count           500.000           500.000           500.000           500.000   \n",
       "mean              9.986             9.934             9.880             9.825   \n",
       "std               2.358             2.335             2.310             2.285   \n",
       "min               5.134             5.121             5.107             5.093   \n",
       "25%               8.247             8.210             8.178             8.146   \n",
       "50%               9.685             9.640             9.594             9.554   \n",
       "75%              11.447            11.328            11.246            11.196   \n",
       "max              18.578            18.464            18.346            18.226   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count           500.000            500.000  ...             500.000   \n",
       "mean              9.769              9.710  ...               4.408   \n",
       "std               2.259              2.232  ...               0.584   \n",
       "min               5.080              5.066  ...               3.063   \n",
       "25%               8.105              8.025  ...               3.967   \n",
       "50%               9.495              9.441  ...               4.385   \n",
       "75%              11.110             11.050  ...               4.801   \n",
       "max              18.101             17.968  ...               6.411   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count             500.000             500.000             500.000   \n",
       "mean                4.399               4.390               4.382   \n",
       "std                 0.583               0.583               0.582   \n",
       "min                 3.051               3.039               3.028   \n",
       "25%                 3.955               3.948               3.942   \n",
       "50%                 4.373               4.365               4.359   \n",
       "75%                 4.791               4.780               4.773   \n",
       "max                 6.405               6.398               6.392   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count             500.000             500.000             500.000   \n",
       "mean                4.373               4.364               4.356   \n",
       "std                 0.581               0.580               0.579   \n",
       "min                 3.017               3.007               2.996   \n",
       "25%                 3.932               3.926               3.920   \n",
       "50%                 4.351               4.339               4.330   \n",
       "75%                 4.767               4.756               4.747   \n",
       "max                 6.387               6.384               6.378   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count             500.000             500.000             500.000  \n",
       "mean                4.347               4.338               4.330  \n",
       "std                 0.578               0.578               0.577  \n",
       "min                 2.984               2.973               2.962  \n",
       "25%                 3.912               3.905               3.899  \n",
       "50%                 4.322               4.311               4.297  \n",
       "75%                 4.736               4.728               4.719  \n",
       "max                 6.371               6.366               6.361  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:24.529228Z",
     "start_time": "2020-11-10T19:59:23.997799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.074</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.081</td>\n",
       "      <td>1.093</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.392</td>\n",
       "      <td>2.386</td>\n",
       "      <td>2.394</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.371</td>\n",
       "      <td>2.352</td>\n",
       "      <td>2.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.675</td>\n",
       "      <td>...</td>\n",
       "      <td>3.954</td>\n",
       "      <td>3.890</td>\n",
       "      <td>3.960</td>\n",
       "      <td>3.902</td>\n",
       "      <td>3.905</td>\n",
       "      <td>3.928</td>\n",
       "      <td>3.858</td>\n",
       "      <td>3.925</td>\n",
       "      <td>3.860</td>\n",
       "      <td>3.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.013</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.171</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.181</td>\n",
       "      <td>1.165</td>\n",
       "      <td>1.155</td>\n",
       "      <td>1.163</td>\n",
       "      <td>1.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.032</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.021</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.068</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614</td>\n",
       "      <td>1.611</td>\n",
       "      <td>1.615</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.606</td>\n",
       "      <td>1.602</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.069</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.072</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.153</td>\n",
       "      <td>1.169</td>\n",
       "      <td>1.194</td>\n",
       "      <td>...</td>\n",
       "      <td>2.396</td>\n",
       "      <td>2.347</td>\n",
       "      <td>2.367</td>\n",
       "      <td>2.360</td>\n",
       "      <td>2.386</td>\n",
       "      <td>2.372</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.352</td>\n",
       "      <td>2.350</td>\n",
       "      <td>2.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.598</td>\n",
       "      <td>3.568</td>\n",
       "      <td>5.232</td>\n",
       "      <td>5.123</td>\n",
       "      <td>5.827</td>\n",
       "      <td>6.608</td>\n",
       "      <td>7.832</td>\n",
       "      <td>10.251</td>\n",
       "      <td>9.367</td>\n",
       "      <td>10.054</td>\n",
       "      <td>...</td>\n",
       "      <td>57.032</td>\n",
       "      <td>55.876</td>\n",
       "      <td>56.527</td>\n",
       "      <td>55.907</td>\n",
       "      <td>55.908</td>\n",
       "      <td>56.149</td>\n",
       "      <td>54.964</td>\n",
       "      <td>55.598</td>\n",
       "      <td>55.066</td>\n",
       "      <td>55.107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count         500.000         500.000         500.000         500.000   \n",
       "mean            1.074           1.073           1.081           1.093   \n",
       "std             0.161           0.196           0.259           0.279   \n",
       "min             0.966           0.953           0.940           0.933   \n",
       "25%             1.013           1.005           1.001           0.999   \n",
       "50%             1.032           1.023           1.020           1.021   \n",
       "75%             1.069           1.067           1.072           1.080   \n",
       "max             2.598           3.568           5.232           5.123   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count         500.000         500.000         500.000         500.000   \n",
       "mean            1.113           1.132           1.156           1.180   \n",
       "std             0.337           0.411           0.491           0.590   \n",
       "min             0.923           0.914           0.901           0.884   \n",
       "25%             0.999           1.000           1.000           1.002   \n",
       "50%             1.027           1.033           1.041           1.051   \n",
       "75%             1.100           1.120           1.133           1.153   \n",
       "max             5.827           6.608           7.832          10.251   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count         500.000          500.000  ...           500.000   \n",
       "mean            1.199            1.229  ...             2.392   \n",
       "std             0.602            0.675  ...             3.954   \n",
       "min             0.869            0.860  ...             0.333   \n",
       "25%             1.004            1.006  ...             1.203   \n",
       "50%             1.059            1.068  ...             1.614   \n",
       "75%             1.169            1.194  ...             2.396   \n",
       "max             9.367           10.054  ...            57.032   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count           500.000           500.000           500.000           500.000   \n",
       "mean              2.386             2.394             2.377             2.377   \n",
       "std               3.890             3.960             3.902             3.905   \n",
       "min               0.328             0.326             0.329             0.327   \n",
       "25%               1.173             1.171             1.168             1.173   \n",
       "50%               1.611             1.615             1.607             1.606   \n",
       "75%               2.347             2.367             2.360             2.386   \n",
       "max              55.876            56.527            55.907            55.908   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count           500.000           500.000           500.000           500.000   \n",
       "mean              2.370             2.363             2.371             2.352   \n",
       "std               3.928             3.858             3.925             3.860   \n",
       "min               0.327             0.321             0.320             0.322   \n",
       "25%               1.181             1.165             1.155             1.163   \n",
       "50%               1.602             1.600             1.592             1.591   \n",
       "75%               2.372             2.337             2.352             2.350   \n",
       "max              56.149            54.964            55.598            55.066   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count           500.000  \n",
       "mean              2.352  \n",
       "std               3.864  \n",
       "min               0.321  \n",
       "25%               1.166  \n",
       "50%               1.598  \n",
       "75%               2.359  \n",
       "max              55.107  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:25.061778Z",
     "start_time": "2020-11-10T19:59:24.531230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.121</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.151</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.226</td>\n",
       "      <td>1.256</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.319</td>\n",
       "      <td>1.353</td>\n",
       "      <td>...</td>\n",
       "      <td>2.758</td>\n",
       "      <td>2.751</td>\n",
       "      <td>2.743</td>\n",
       "      <td>2.737</td>\n",
       "      <td>2.726</td>\n",
       "      <td>2.717</td>\n",
       "      <td>2.712</td>\n",
       "      <td>2.703</td>\n",
       "      <td>2.697</td>\n",
       "      <td>2.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.225</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.892</td>\n",
       "      <td>2.120</td>\n",
       "      <td>2.348</td>\n",
       "      <td>2.585</td>\n",
       "      <td>2.821</td>\n",
       "      <td>3.052</td>\n",
       "      <td>3.291</td>\n",
       "      <td>...</td>\n",
       "      <td>8.265</td>\n",
       "      <td>8.202</td>\n",
       "      <td>8.131</td>\n",
       "      <td>8.063</td>\n",
       "      <td>7.937</td>\n",
       "      <td>7.854</td>\n",
       "      <td>7.815</td>\n",
       "      <td>7.727</td>\n",
       "      <td>7.656</td>\n",
       "      <td>7.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.001</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.983</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.015</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.034</td>\n",
       "      <td>...</td>\n",
       "      <td>1.497</td>\n",
       "      <td>1.491</td>\n",
       "      <td>1.489</td>\n",
       "      <td>1.485</td>\n",
       "      <td>1.482</td>\n",
       "      <td>1.481</td>\n",
       "      <td>1.481</td>\n",
       "      <td>1.479</td>\n",
       "      <td>1.475</td>\n",
       "      <td>1.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.055</td>\n",
       "      <td>1.052</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.066</td>\n",
       "      <td>1.076</td>\n",
       "      <td>1.086</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.149</td>\n",
       "      <td>...</td>\n",
       "      <td>2.217</td>\n",
       "      <td>2.222</td>\n",
       "      <td>2.219</td>\n",
       "      <td>2.204</td>\n",
       "      <td>2.201</td>\n",
       "      <td>2.199</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.191</td>\n",
       "      <td>2.181</td>\n",
       "      <td>2.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27.996</td>\n",
       "      <td>32.880</td>\n",
       "      <td>37.674</td>\n",
       "      <td>42.567</td>\n",
       "      <td>47.509</td>\n",
       "      <td>52.439</td>\n",
       "      <td>57.549</td>\n",
       "      <td>62.647</td>\n",
       "      <td>67.588</td>\n",
       "      <td>72.704</td>\n",
       "      <td>...</td>\n",
       "      <td>153.137</td>\n",
       "      <td>151.461</td>\n",
       "      <td>149.656</td>\n",
       "      <td>147.743</td>\n",
       "      <td>144.341</td>\n",
       "      <td>142.045</td>\n",
       "      <td>141.022</td>\n",
       "      <td>138.661</td>\n",
       "      <td>136.596</td>\n",
       "      <td>133.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count             500.000             500.000             500.000   \n",
       "mean                1.121               1.132               1.151   \n",
       "std                 1.225               1.447               1.667   \n",
       "min                 0.950               0.940               0.927   \n",
       "25%                 1.001               0.994               0.990   \n",
       "50%                 1.015               1.008               1.007   \n",
       "75%                 1.055               1.052               1.056   \n",
       "max                27.996              32.880              37.674   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count             500.000             500.000             500.000   \n",
       "mean                1.173               1.199               1.226   \n",
       "std                 1.892               2.120               2.348   \n",
       "min                 0.915               0.906               0.898   \n",
       "25%                 0.988               0.985               0.984   \n",
       "50%                 1.007               1.011               1.014   \n",
       "75%                 1.066               1.076               1.086   \n",
       "max                42.567              47.509              52.439   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count             500.000             500.000             500.000   \n",
       "mean                1.256               1.287               1.319   \n",
       "std                 2.585               2.821               3.052   \n",
       "min                 0.885               0.874               0.864   \n",
       "25%                 0.983               0.983               0.985   \n",
       "50%                 1.017               1.022               1.028   \n",
       "75%                 1.100               1.114               1.133   \n",
       "max                57.549              62.647              67.588   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count              500.000  ...               500.000               500.000   \n",
       "mean                 1.353  ...                 2.758                 2.751   \n",
       "std                  3.291  ...                 8.265                 8.202   \n",
       "min                  0.855  ...                 0.383                 0.382   \n",
       "25%                  0.983  ...                 1.011                 1.008   \n",
       "50%                  1.034  ...                 1.497                 1.491   \n",
       "75%                  1.149  ...                 2.217                 2.222   \n",
       "max                 72.704  ...               153.137               151.461   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count               500.000               500.000               500.000   \n",
       "mean                  2.743                 2.737                 2.726   \n",
       "std                   8.131                 8.063                 7.937   \n",
       "min                   0.381                 0.381                 0.381   \n",
       "25%                   1.009                 1.006                 1.001   \n",
       "50%                   1.489                 1.485                 1.482   \n",
       "75%                   2.219                 2.204                 2.201   \n",
       "max                 149.656               147.743               144.341   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count               500.000               500.000               500.000   \n",
       "mean                  2.717                 2.712                 2.703   \n",
       "std                   7.854                 7.815                 7.727   \n",
       "min                   0.381                 0.380                 0.380   \n",
       "25%                   0.995                 0.994                 0.993   \n",
       "50%                   1.481                 1.481                 1.479   \n",
       "75%                   2.199                 2.192                 2.191   \n",
       "max                 142.045               141.022               138.661   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count               500.000               500.000  \n",
       "mean                  2.697                 2.686  \n",
       "std                   7.656                 7.527  \n",
       "min                   0.381                 0.381  \n",
       "25%                   0.992                 0.993  \n",
       "50%                   1.475                 1.467  \n",
       "75%                   2.181                 2.182  \n",
       "max                 136.596               133.044  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:26.623891Z",
     "start_time": "2020-11-10T19:59:25.063589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:59:28.177089Z",
     "start_time": "2020-11-10T19:59:26.626009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mail Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:03:43.798275Z",
     "start_time": "2020-11-10T21:03:43.772398Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mail_notifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3f06a372a9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../_baselib/pymail'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmail_notifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m mail_notifier.notification(sender='lahoffma.ines@gmail.com', \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mail_notifier'"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0,'../_baselib/pymail')\n",
    "#import mail_notifier\n",
    "\n",
    "#mail_notifier.notification(sender='lahoffma.ines@gmail.com', \n",
    "#                           to='hoffmann@es.uni-mannheim.de', \n",
    "#                           subject='PyMail - 02_lambda_net Finished', \n",
    "#                           notification='The entire notebook was executed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%email lahoffma.ines@gmail.com --to hoffmann@es.uni-mannheim.de --s '[PyMail] Lambda-Net Notebook Finished' --keep-password\n",
    "print('Success')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
