{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import sympy as sym\n",
    "from sympy import Symbol, sympify, lambdify, abc, SympifyError\n",
    "\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sympy import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import types\n",
    "import random\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26381042, 5)\n"
     ]
    }
   ],
   "source": [
    "#path = \"./data/replica_pump_data.csv\"\n",
    "path = \"./data/replica_pump_data.csv\"\n",
    "pump_data_replica = pd.read_csv(path)\n",
    "print(pump_data_replica.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_norm_log</th>\n",
       "      <th>temperature_diff</th>\n",
       "      <th>rms_norm_log</th>\n",
       "      <th>details_ratedhead</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.642337</td>\n",
       "      <td>-0.585072</td>\n",
       "      <td>-2.831278</td>\n",
       "      <td>47.369469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.384410</td>\n",
       "      <td>-2.051363</td>\n",
       "      <td>-2.900545</td>\n",
       "      <td>120.240341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.047895</td>\n",
       "      <td>2.104730</td>\n",
       "      <td>-2.742720</td>\n",
       "      <td>92.577971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.962318</td>\n",
       "      <td>0.375291</td>\n",
       "      <td>-2.975236</td>\n",
       "      <td>75.714544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.082340</td>\n",
       "      <td>-1.878716</td>\n",
       "      <td>-2.900094</td>\n",
       "      <td>19.732252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   energy_norm_log  temperature_diff  rms_norm_log  details_ratedhead  state\n",
       "0        -4.642337         -0.585072     -2.831278          47.369469      1\n",
       "1        -4.384410         -2.051363     -2.900545         120.240341      1\n",
       "2        -5.047895          2.104730     -2.742720          92.577971      1\n",
       "3        -4.962318          0.375291     -2.975236          75.714544      1\n",
       "4        -5.082340         -1.878716     -2.900094          19.732252      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pump_data_replica.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/randForestBest_20201002.pkl\", 'rb') as f:\n",
    "    random_forest_model = joblib.load(f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pump_data_replica.sample(n=100_000)\n",
    "\n",
    "X_data = data.drop(['state'], axis=1).values\n",
    "y_data = data[['state']].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=42)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_pred = random_forest_model.predict(X_train)\n",
    "y_test_rf_pred = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9265588386580558\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9486\n",
      "Performance (F1 Score) Logistic Regression: 0.8877902455282746\n",
      "Performance (Accuracy) Logistic Regression: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.4549914146182 e^{- 1.657 x_{0} - 0.096 x_{1} - 0.319 x_{2} + 0.003 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.4549914146182*exp(-1.657*x_0 - 0.096*x_1 - 0.319*x_2 + 0.003*x_3) + 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9267666908360755\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.94748\n",
      "Performance (F1 Score) Logistic Regression: 0.8922642942389063\n",
      "Performance (Accuracy) Logistic Regression: 0.92056\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.94082323977601 e^{- 0.943 x_{0} - 0.06 x_{1} - 0.785 x_{2} + 0.002 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.94082323977601*exp(-0.943*x_0 - 0.06*x_1 - 0.785*x_2 + 0.002*x_3) + 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.939836448598131\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9588\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8650212765957447\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.90484\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.9659024892452506 e^{- 0.012386654575196278 x_{0}^{2} - 0.15407804247971868 x_{0} x_{1} + 0.0035209866203692675 x_{0} x_{2} - 0.011509519787982037 x_{0} x_{3} - 0.24148277446681493 x_{0} + 0.003513467481004637 x_{1}^{2} + 0.03969601071605454 x_{1} x_{2} - 0.00017725754260900685 x_{1} x_{3} - 0.30377841117305615 x_{1} + 0.0015257266999313462 x_{2}^{2} - 0.00996244339743366 x_{2} x_{3} - 0.09771645122257876 x_{2} - 1.8927771694089615 \\cdot 10^{-6} x_{3}^{2} + 0.02352325108234961 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.9659024892452506*exp(-0.012386654575196278*x0**2 - 0.15407804247971868*x0*x1 + 0.0035209866203692675*x0*x2 - 0.011509519787982037*x0*x3 - 0.24148277446681493*x0 + 0.003513467481004637*x1**2 + 0.03969601071605454*x1*x2 - 0.00017725754260900685*x1*x3 - 0.30377841117305615*x1 + 0.0015257266999313462*x2**2 - 0.00996244339743366*x2*x3 - 0.09771645122257876*x2 - 1.8927771694089615e-6*x3**2 + 0.02352325108234961*x3) + 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0002263698898328147, 0.9992708041271765, 0.9998493483075224, 0.9944529589541462, 0.8465446861688402]\n",
      "[[2.26369890e-04]\n",
      " [9.99270804e-01]\n",
      " [9.99849348e-01]\n",
      " [9.94452959e-01]\n",
      " [8.46544686e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = X_test_poly[:5]\n",
    "function_vars = [sym.symbols(variable_identifier) for variable_identifier in variable_identifier_list]\n",
    "function_values = []\n",
    "for data_point in X:\n",
    "    function_value = logistic_regression_function_sympy.evalf(subs={var: data_point[index] for index, var in enumerate(list(function_vars))})\n",
    "    try:\n",
    "        function_value = float(function_value)\n",
    "    except TypeError as te:\n",
    "        function_value = np.inf\n",
    "    function_values.append(function_value)\n",
    "Y_est = function_values#np.nan_to_num(function_values).ravel()\n",
    "print(Y_est)\n",
    "print(clf.predict_proba(X)[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9329530839344629\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.95204\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8778551229062432\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.91016\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.99756791936098441 e^{0.026657839314309335 x_{0}^{2} - 0.03854969575786857 x_{0} x_{1} + 0.055926191845977774 x_{0} x_{2} - 0.004375097256243473 x_{0} x_{3} - 0.26155658418757943 x_{0} + 0.0014470980602936023 x_{1}^{2} - 0.009920175805347292 x_{1} x_{2} + 3.5558591566098 \\cdot 10^{-5} x_{1} x_{3} - 0.16893588642723195 x_{1} + 0.03268949251072916 x_{2}^{2} - 0.005557611808121668 x_{2} x_{3} - 0.12288380769309638 x_{2} + 4.999007259803651 \\cdot 10^{-6} x_{3}^{2} + 0.004436618616761277 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.99756791936098441*exp(0.026657839314309335*x0**2 - 0.03854969575786857*x0*x1 + 0.055926191845977774*x0*x2 - 0.004375097256243473*x0*x3 - 0.26155658418757943*x0 + 0.0014470980602936023*x1**2 - 0.009920175805347292*x1*x2 + 3.5558591566098e-5*x1*x3 - 0.16893588642723195*x1 + 0.03268949251072916*x2**2 - 0.005557611808121668*x2*x3 - 0.12288380769309638*x2 + 4.999007259803651e-6*x3**2 + 0.004436618616761277*x3) + 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) 2*X0 - Random Forest Model: 0.9130008648025367\n",
      "Fidelity (Accuracy) 2*X0 - Random Forest Model: 0.93964\n",
      "Performance (F1 Score) 2*X0: 0.8815686274509804\n",
      "Performance (Accuracy) 2*X0: 0.91544\n"
     ]
    }
   ],
   "source": [
    "preds = np.clip(np.round(10*X_test[:,0]), 0, 1)\n",
    "\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) 2*X0 - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) 2*X0 - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) 2*X0:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) 2*X0:', acc_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest: 0.90676\n"
     ]
    }
   ],
   "source": [
    "y_test_random_forest = random_forest_model.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_test_random_forest)\n",
    "print('Accuracy Random Forest: '+ str(accuracy_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pump_data_replica\n",
    "\n",
    "X_data = data.drop(['state'], axis=1).values\n",
    "y_data = data[['state']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=42)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_pred = random_forest_model.predict(X_train)\n",
    "y_test_rf_pred = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9257077296584201\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9488276506418776\n",
      "Performance (F1 Score) Logistic Regression: 0.887081469837777\n",
      "Performance (Accuracy) Logistic Regression: 0.9199218954337061\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.43619894196035 e^{- 1.684 x_{0} - 0.093 x_{1} - 0.31 x_{2} + 0.003 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.43619894196035*exp(-1.684*x_0 - 0.093*x_1 - 0.31*x_2 + 0.003*x_3) + 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9271903410829246\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9485808067338047\n",
      "Performance (F1 Score) Logistic Regression: 0.8923695003720781\n",
      "Performance (Accuracy) Logistic Regression: 0.9217974845877972\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.946485147953484 e^{- 0.961 x_{0} - 0.059 x_{1} - 0.744 x_{2} + 0.002 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.946485147953484*exp(-0.961*x_0 - 0.059*x_1 - 0.744*x_2 + 0.002*x_3) + 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model: 0.9390044399747365\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9588381718327751\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8700849414247459\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.9096827252173947\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.9302691313554123 e^{0.05725573523869259 x_{0}^{2} - 0.24762088466722346 x_{0} x_{1} + 0.03473811553535881 x_{0} x_{2} - 0.005397006942481776 x_{0} x_{3} - 0.7124400215706337 x_{0} + 0.0014154508585449606 x_{1}^{2} + 0.0976229795617102 x_{1} x_{2} + 0.0001033927449932151 x_{1} x_{3} - 0.29327694836096946 x_{1} + 0.021300181038441127 x_{2}^{2} - 0.011031777120725459 x_{2} x_{3} - 0.29874458127519593 x_{2} - 3.7756524358565736 \\cdot 10^{-6} x_{3}^{2} + 0.019400874359348078 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.9302691313554123*exp(0.05725573523869259*x0**2 - 0.24762088466722346*x0*x1 + 0.03473811553535881*x0*x2 - 0.005397006942481776*x0*x3 - 0.7124400215706337*x0 + 0.0014154508585449606*x1**2 + 0.0976229795617102*x1*x2 + 0.0001033927449932151*x1*x3 - 0.29327694836096946*x1 + 0.021300181038441127*x2**2 - 0.011031777120725459*x2*x3 - 0.29874458127519593*x2 - 3.7756524358565736e-6*x3**2 + 0.019400874359348078*x3) + 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9309141365696707\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9514580241782699\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8766664443692268\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.9108296093209958\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.99776471015717396 e^{0.0313915099696629 x_{0}^{2} - 0.04371246113089017 x_{0} x_{1} + 0.05865400782941809 x_{0} x_{2} - 0.004098624641049362 x_{0} x_{3} - 0.26540609224316764 x_{0} + 0.0013634931461093595 x_{1}^{2} - 0.009314100528232337 x_{1} x_{2} + 3.237221315525861 \\cdot 10^{-5} x_{1} x_{3} - 0.1701177507397923 x_{1} + 0.03401984692395202 x_{2}^{2} - 0.005191755446501386 x_{2} x_{3} - 0.12348228993584681 x_{2} + 3.994845900842643 \\cdot 10^{-6} x_{3}^{2} + 0.004342889365317119 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.99776471015717396*exp(0.0313915099696629*x0**2 - 0.04371246113089017*x0*x1 + 0.05865400782941809*x0*x2 - 0.004098624641049362*x0*x3 - 0.26540609224316764*x0 + 0.0013634931461093595*x1**2 - 0.009314100528232337*x1*x2 + 3.237221315525861e-5*x1*x3 - 0.1701177507397923*x1 + 0.03401984692395202*x2**2 - 0.005191755446501386*x2*x3 - 0.12348228993584681*x2 + 3.994845900842643e-6*x3**2 + 0.004342889365317119*x3) + 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model: 0.8714735808231988\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9142658645351563\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8089378746906527\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.8686593297823998\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.000000492225927 e^{- 9.23633471934513 \\cdot 10^{-5} x_{0}^{3} + 1.5807543945835542 \\cdot 10^{-6} x_{0}^{2} x_{1} - 4.1509423347920933 \\cdot 10^{-5} x_{0}^{2} x_{2} + 0.00031164575813618083 x_{0}^{2} x_{3} + 8.17100393905537 \\cdot 10^{-6} x_{0}^{2} - 0.0007574033601676178 x_{0} x_{1}^{2} + 8.527169147573101 \\cdot 10^{-6} x_{0} x_{1} x_{2} - 0.0017782365078487657 x_{0} x_{1} x_{3} - 4.480970919177078 \\cdot 10^{-5} x_{0} x_{1} - 2.0759828869959416 \\cdot 10^{-5} x_{0} x_{2}^{2} + 0.00016750043799227017 x_{0} x_{2} x_{3} + 4.696894371656872 \\cdot 10^{-6} x_{0} x_{2} - 6.834432525005995 \\cdot 10^{-5} x_{0} x_{3}^{2} - 0.00023275120647287887 x_{0} x_{3} - 7.437724024261284 \\cdot 10^{-6} x_{0} - 0.00010074156231792606 x_{1}^{3} - 0.0003168964679618513 x_{1}^{2} x_{2} + 5.901933145298026 \\cdot 10^{-5} x_{1}^{2} x_{3} - 7.458389836440907 \\cdot 10^{-5} x_{1}^{2} + 4.613587027222967 \\cdot 10^{-6} x_{1} x_{2}^{2} - 0.0007624054813891951 x_{1} x_{2} x_{3} - 1.929782612391575 \\cdot 10^{-5} x_{1} x_{2} - 5.482782994495632 \\cdot 10^{-6} x_{1} x_{3}^{2} - 0.0004194554851889597 x_{1} x_{3} - 5.649875320461432 \\cdot 10^{-6} x_{1} - 1.1002938836571546 \\cdot 10^{-5} x_{2}^{3} + 9.383210063035078 \\cdot 10^{-5} x_{2}^{2} x_{3} + 2.5697129875376507 \\cdot 10^{-6} x_{2}^{2} - 4.491557266570163 \\cdot 10^{-5} x_{2} x_{3}^{2} - 0.00010315460640368964 x_{2} x_{3} - 3.3709719295767436 \\cdot 10^{-6} x_{2} + 6.721416031182079 \\cdot 10^{-8} x_{3}^{3} + 8.936577374355665 \\cdot 10^{-5} x_{3}^{2} + 1.2919117397863465 \\cdot 10^{-5} x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.000000492225927*exp(-9.23633471934513e-5*x0**3 + 1.5807543945835542e-6*x0**2*x1 - 4.1509423347920933e-5*x0**2*x2 + 0.00031164575813618083*x0**2*x3 + 8.17100393905537e-6*x0**2 - 0.0007574033601676178*x0*x1**2 + 8.527169147573101e-6*x0*x1*x2 - 0.0017782365078487657*x0*x1*x3 - 4.480970919177078e-5*x0*x1 - 2.0759828869959416e-5*x0*x2**2 + 0.00016750043799227017*x0*x2*x3 + 4.696894371656872e-6*x0*x2 - 6.834432525005995e-5*x0*x3**2 - 0.00023275120647287887*x0*x3 - 7.437724024261284e-6*x0 - 0.00010074156231792606*x1**3 - 0.0003168964679618513*x1**2*x2 + 5.901933145298026e-5*x1**2*x3 - 7.458389836440907e-5*x1**2 + 4.613587027222967e-6*x1*x2**2 - 0.0007624054813891951*x1*x2*x3 - 1.929782612391575e-5*x1*x2 - 5.482782994495632e-6*x1*x3**2 - 0.0004194554851889597*x1*x3 - 5.649875320461432e-6*x1 - 1.1002938836571546e-5*x2**3 + 9.383210063035078e-5*x2**2*x3 + 2.5697129875376507e-6*x2**2 - 4.491557266570163e-5*x2*x3**2 - 0.00010315460640368964*x2*x3 - 3.3709719295767436e-6*x2 + 6.721416031182079e-8*x3**3 + 8.936577374355665e-5*x3**2 + 1.2919117397863465e-5*x3) + 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 3, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.8583692683995165\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9033190650074349\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.7998877587840507\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.8593218676258605\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.000000239899468 e^{- 3.987240290103183 \\cdot 10^{-5} x_{0}^{3} + 9.038463986333095 \\cdot 10^{-7} x_{0}^{2} x_{1} - 1.8171759889124487 \\cdot 10^{-5} x_{0}^{2} x_{2} + 0.00016588439333354777 x_{0}^{2} x_{3} + 3.4479068946158208 \\cdot 10^{-6} x_{0}^{2} - 0.0002999692246696982 x_{0} x_{1}^{2} + 4.182136565111579 \\cdot 10^{-6} x_{0} x_{1} x_{2} - 0.0006591348357414004 x_{0} x_{1} x_{3} - 1.7371131296959287 \\cdot 10^{-5} x_{0} x_{1} - 9.08175037821819 \\cdot 10^{-6} x_{0} x_{2}^{2} + 9.441691202991266 \\cdot 10^{-5} x_{0} x_{2} x_{3} + 2.068431533372318 \\cdot 10^{-6} x_{0} x_{2} - 2.0795985919481787 \\cdot 10^{-5} x_{0} x_{3}^{2} - 0.00012005588600760279 x_{0} x_{3} - 3.0674408451190046 \\cdot 10^{-6} x_{0} - 1.2823992603799582 \\cdot 10^{-5} x_{1}^{3} - 0.0001363484155795731 x_{1}^{2} x_{2} - 2.8992809356304144 \\cdot 10^{-5} x_{1}^{2} x_{3} - 1.9106064788095217 \\cdot 10^{-5} x_{1}^{2} + 2.7257227168321305 \\cdot 10^{-6} x_{1} x_{2}^{2} - 0.0002944331200464126 x_{1} x_{2} x_{3} - 7.890795379721237 \\cdot 10^{-6} x_{1} x_{2} + 3.5603366204827266 \\cdot 10^{-7} x_{1} x_{3}^{2} - 5.0188410445643023 \\cdot 10^{-5} x_{1} x_{3} - 9.432858392023209 \\cdot 10^{-7} x_{1} - 4.837304319938414 \\cdot 10^{-6} x_{2}^{3} + 4.976549666060826 \\cdot 10^{-5} x_{2}^{2} x_{3} + 1.125358348925249 \\cdot 10^{-6} x_{2}^{2} - 1.4858812265037941 \\cdot 10^{-5} x_{2} x_{3}^{2} - 5.654044164350087 \\cdot 10^{-5} x_{2} x_{3} - 1.455192818426714 \\cdot 10^{-6} x_{2} + 1.3306205289206738 \\cdot 10^{-8} x_{3}^{3} + 3.2941692023968235 \\cdot 10^{-5} x_{3}^{2} + 6.367894693187306 \\cdot 10^{-6} x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.000000239899468*exp(-3.987240290103183e-5*x0**3 + 9.038463986333095e-7*x0**2*x1 - 1.8171759889124487e-5*x0**2*x2 + 0.00016588439333354777*x0**2*x3 + 3.4479068946158208e-6*x0**2 - 0.0002999692246696982*x0*x1**2 + 4.182136565111579e-6*x0*x1*x2 - 0.0006591348357414004*x0*x1*x3 - 1.7371131296959287e-5*x0*x1 - 9.08175037821819e-6*x0*x2**2 + 9.441691202991266e-5*x0*x2*x3 + 2.068431533372318e-6*x0*x2 - 2.0795985919481787e-5*x0*x3**2 - 0.00012005588600760279*x0*x3 - 3.0674408451190046e-6*x0 - 1.2823992603799582e-5*x1**3 - 0.0001363484155795731*x1**2*x2 - 2.8992809356304144e-5*x1**2*x3 - 1.9106064788095217e-5*x1**2 + 2.7257227168321305e-6*x1*x2**2 - 0.0002944331200464126*x1*x2*x3 - 7.890795379721237e-6*x1*x2 + 3.5603366204827266e-7*x1*x3**2 - 5.0188410445643023e-5*x1*x3 - 9.432858392023209e-7*x1 - 4.837304319938414e-6*x2**3 + 4.976549666060826e-5*x2**2*x3 + 1.125358348925249e-6*x2**2 - 1.4858812265037941e-5*x2*x3**2 - 5.654044164350087e-5*x2*x3 - 1.455192818426714e-6*x2 + 1.3306205289206738e-8*x3**3 + 3.2941692023968235e-5*x3**2 + 6.367894693187306e-6*x3) + 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 3, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) 2*X0 - Random Forest Model: 0.9126073764149031\n",
      "Fidelity (Accuracy) 2*X0 - Random Forest Model: 0.9402887922100429\n",
      "Performance (F1 Score) 2*X0: 0.8796552490988172\n",
      "Performance (Accuracy) 2*X0: 0.9153228355936179\n"
     ]
    }
   ],
   "source": [
    "preds = np.clip(np.round(10*X_test[:,0]), 0, 1)\n",
    "\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) 2*X0 - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) 2*X0 - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) 2*X0:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) 2*X0:', acc_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest: 0.9078285150504278\n"
     ]
    }
   ],
   "source": [
    "y_test_random_forest = random_forest_model.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_test_random_forest)\n",
    "print('Accuracy Random Forest: '+ str(accuracy_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
