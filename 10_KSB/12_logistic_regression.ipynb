{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "import sympy as sym\n",
    "from sympy import Symbol, sympify, lambdify, abc, SympifyError\n",
    "\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sympy import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import types\n",
    "import random\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26381042, 5)\n"
     ]
    }
   ],
   "source": [
    "#path = \"./data/replica_pump_data.csv\"\n",
    "path = \"./data/replica_pump_data.csv\"\n",
    "pump_data_replica = pd.read_csv(path)\n",
    "print(pump_data_replica.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy_norm_log</th>\n",
       "      <th>temperature_diff</th>\n",
       "      <th>rms_norm_log</th>\n",
       "      <th>details_ratedhead</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.642337</td>\n",
       "      <td>-0.585072</td>\n",
       "      <td>-2.831278</td>\n",
       "      <td>47.369469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.384410</td>\n",
       "      <td>-2.051363</td>\n",
       "      <td>-2.900545</td>\n",
       "      <td>120.240341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.047895</td>\n",
       "      <td>2.104730</td>\n",
       "      <td>-2.742720</td>\n",
       "      <td>92.577971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.962318</td>\n",
       "      <td>0.375291</td>\n",
       "      <td>-2.975236</td>\n",
       "      <td>75.714544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.082340</td>\n",
       "      <td>-1.878716</td>\n",
       "      <td>-2.900094</td>\n",
       "      <td>19.732252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   energy_norm_log  temperature_diff  rms_norm_log  details_ratedhead  state\n",
       "0        -4.642337         -0.585072     -2.831278          47.369469      1\n",
       "1        -4.384410         -2.051363     -2.900545         120.240341      1\n",
       "2        -5.047895          2.104730     -2.742720          92.577971      1\n",
       "3        -4.962318          0.375291     -2.975236          75.714544      1\n",
       "4        -5.082340         -1.878716     -2.900094          19.732252      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pump_data_replica.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/randForestBest_20201002.pkl\", 'rb') as f:\n",
    "    random_forest_model = joblib.load(f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pump_data_replica.sample(n=100_000)\n",
    "\n",
    "X_data = data.drop(['state'], axis=1).values\n",
    "y_data = data[['state']].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=42)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_pred = random_forest_model.predict(X_train)\n",
    "y_test_rf_pred = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9265588386580558\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9486\n",
      "Performance (F1 Score) Logistic Regression: 0.8877902455282746\n",
      "Performance (Accuracy) Logistic Regression: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.4549914146182 e^{- 1.657 x_{0} - 0.096 x_{1} - 0.319 x_{2} + 0.003 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.4549914146182*exp(-1.657*x_0 - 0.096*x_1 - 0.319*x_2 + 0.003*x_3) + 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9267666908360755\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.94748\n",
      "Performance (F1 Score) Logistic Regression: 0.8922642942389063\n",
      "Performance (Accuracy) Logistic Regression: 0.92056\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.94082323977601 e^{- 0.943 x_{0} - 0.06 x_{1} - 0.785 x_{2} + 0.002 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.94082323977601*exp(-0.943*x_0 - 0.06*x_1 - 0.785*x_2 + 0.002*x_3) + 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.943707839032333\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.96128\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.87362295915485\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.91052\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.9152464017738899 e^{0.026922765081768933 x_{0}^{2} - 0.2130094423483082 x_{0} x_{1} + 0.024876353235428837 x_{0} x_{2} - 0.0036654543302487596 x_{0} x_{3} - 0.7396430183121271 x_{0} + 0.0031687769147276785 x_{1}^{2} + 0.193984304485567 x_{1} x_{2} + 2.3089260003405987 \\cdot 10^{-5} x_{1} x_{3} - 0.29329118059497566 x_{1} + 0.017025754142234215 x_{2}^{2} - 0.011020832020520174 x_{2} x_{3} - 0.29940115591448 x_{2} - 8.079021373281422 \\cdot 10^{-6} x_{3}^{2} + 0.020310332824683364 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.9152464017738899*exp(0.026922765081768933*x0**2 - 0.2130094423483082*x0*x1 + 0.024876353235428837*x0*x2 - 0.0036654543302487596*x0*x3 - 0.7396430183121271*x0 + 0.0031687769147276785*x1**2 + 0.193984304485567*x1*x2 + 2.3089260003405987e-5*x1*x3 - 0.29329118059497566*x1 + 0.017025754142234215*x2**2 - 0.011020832020520174*x2*x3 - 0.29940115591448*x2 - 8.079021373281422e-6*x3**2 + 0.020310332824683364*x3) + 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0002932160455848126, 0.9995043799254669, 0.9996959248743833, 0.963067511052141, 0.01301469864938835]\n",
      "[[2.93216046e-04]\n",
      " [9.99504380e-01]\n",
      " [9.99695925e-01]\n",
      " [9.63067511e-01]\n",
      " [1.30146986e-02]]\n"
     ]
    }
   ],
   "source": [
    "X = X_test_poly[:5]\n",
    "function_vars = [sym.symbols(variable_identifier) for variable_identifier in variable_identifier_list]\n",
    "function_values = []\n",
    "for data_point in X:\n",
    "    function_value = logistic_regression_function_sympy.evalf(subs={var: data_point[index] for index, var in enumerate(list(function_vars))})\n",
    "    try:\n",
    "        function_value = float(function_value)\n",
    "    except TypeError as te:\n",
    "        function_value = np.inf\n",
    "    function_values.append(function_value)\n",
    "Y_est = function_values#np.nan_to_num(function_values).ravel()\n",
    "print(Y_est)\n",
    "print(clf.predict_proba(X)[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9279344153326317\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.94796\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8892121995904732\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.91776\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.000068759320953 e^{0.0419301935483896 x_{0}^{2} - 0.02684067101921403 x_{0} x_{1} + 0.06604972870756479 x_{0} x_{2} - 0.0032924400027167064 x_{0} x_{3} - 0.3094754918818906 x_{0} - 0.0006565481410156127 x_{1}^{2} - 0.0018161321770334706 x_{1} x_{2} + 2.8469081546370982 \\cdot 10^{-5} x_{1} x_{3} - 0.07560405537628562 x_{1} + 0.037784249444640414 x_{2}^{2} - 0.005633470955109121 x_{2} x_{3} - 0.1462251071588629 x_{2} + 6.4768525473217545 \\cdot 10^{-6} x_{3}^{2} + 0.0002009339712090635 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.000068759320953*exp(0.0419301935483896*x0**2 - 0.02684067101921403*x0*x1 + 0.06604972870756479*x0*x2 - 0.0032924400027167064*x0*x3 - 0.3094754918818906*x0 - 0.0006565481410156127*x1**2 - 0.0018161321770334706*x1*x2 + 2.8469081546370982e-5*x1*x3 - 0.07560405537628562*x1 + 0.037784249444640414*x2**2 - 0.005633470955109121*x2*x3 - 0.1462251071588629*x2 + 6.4768525473217545e-6*x3**2 + 0.0002009339712090635*x3) + 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) 2*X0 - Random Forest Model: 0.9130008648025367\n",
      "Fidelity (Accuracy) 2*X0 - Random Forest Model: 0.93964\n",
      "Performance (F1 Score) 2*X0: 0.8815686274509804\n",
      "Performance (Accuracy) 2*X0: 0.91544\n"
     ]
    }
   ],
   "source": [
    "preds = np.clip(np.round(10*X_test[:,0]), 0, 1)\n",
    "\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) 2*X0 - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) 2*X0 - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) 2*X0:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) 2*X0:', acc_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest: 0.90676\n"
     ]
    }
   ],
   "source": [
    "y_test_random_forest = random_forest_model.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_test_random_forest)\n",
    "print('Accuracy Random Forest: '+ str(accuracy_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pump_data_replica\n",
    "\n",
    "X_data = data.drop(['state'], axis=1).values\n",
    "y_data = data[['state']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=42)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_pred = random_forest_model.predict(X_train)\n",
    "y_test_rf_pred = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9257077296584201\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9488276506418776\n",
      "Performance (F1 Score) Logistic Regression: 0.887081469837777\n",
      "Performance (Accuracy) Logistic Regression: 0.9199218954337061\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.43619894196035 e^{- 1.684 x_{0} - 0.093 x_{1} - 0.31 x_{2} + 0.003 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.43619894196035*exp(-1.684*x_0 - 0.093*x_1 - 0.31*x_2 + 0.003*x_3) + 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression - Random Forest Model: 0.9271903410829246\n",
      "Fidelity (Accuracy) Logistic Regression - Random Forest Model: 0.9485808067338047\n",
      "Performance (F1 Score) Logistic Regression: 0.8923695003720781\n",
      "Performance (Accuracy) Logistic Regression: 0.9217974845877972\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.946485147953484 e^{- 0.961 x_{0} - 0.059 x_{1} - 0.744 x_{2} + 0.002 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.946485147953484*exp(-0.961*x_0 - 0.059*x_1 - 0.744*x_2 + 0.002*x_3) + 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regression - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regression:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regression:', acc_performance)\n",
    "\n",
    "coef = ' + '.join([str(np.round(coefficient, 3)) + '*x_' + str(i) for i, coefficient in enumerate(clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 3))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model: 0.937180946521965\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9573915270373682\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8652301383175963\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.9058440598484275\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.9676585585857316 e^{0.04920466398198076 x_{0}^{2} - 0.14650829689050376 x_{0} x_{1} + 0.03075354993556312 x_{0} x_{2} - 0.012250895657378622 x_{0} x_{3} - 0.2798706230230561 x_{0} + 0.004129523845031069 x_{1}^{2} + 0.03391475955993701 x_{1} x_{2} - 0.00010103508249413183 x_{1} x_{3} - 0.3440832982965765 x_{1} + 0.014146010468240476 x_{2}^{2} - 0.00932836059388275 x_{2} x_{3} - 0.11713812652494542 x_{2} + 4.1143954734075075 \\cdot 10^{-6} x_{3}^{2} + 0.02053969476926292 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.9676585585857316*exp(0.04920466398198076*x0**2 - 0.14650829689050376*x0*x1 + 0.03075354993556312*x0*x2 - 0.012250895657378622*x0*x3 - 0.2798706230230561*x0 + 0.004129523845031069*x1**2 + 0.03391475955993701*x1*x2 - 0.00010103508249413183*x1*x3 - 0.3440832982965765*x1 + 0.014146010468240476*x2**2 - 0.00932836059388275*x2*x3 - 0.11713812652494542*x2 + 4.1143954734075075e-6*x3**2 + 0.02053969476926292*x3) + 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9308662331580162\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9514299737341706\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8765836012912316\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.9107797250177059\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{0.9978786654516176 e^{0.031126070961043265 x_{0}^{2} - 0.042134708405514636 x_{0} x_{1} + 0.05754156690915437 x_{0} x_{2} - 0.004286244036417964 x_{0} x_{3} - 0.2580978683904601 x_{0} + 0.001354823866066021 x_{1}^{2} - 0.00973931318884627 x_{1} x_{2} + 2.9129363259691755 \\cdot 10^{-5} x_{1} x_{3} - 0.169384462874112 x_{1} + 0.03340024202343735 x_{2}^{2} - 0.005025505749609331 x_{2} x_{3} - 0.12015782467664578 x_{2} + 4.103052597698609 \\cdot 10^{-6} x_{3}^{2} + 0.004444205741733472 x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(0.9978786654516176*exp(0.031126070961043265*x0**2 - 0.042134708405514636*x0*x1 + 0.05754156690915437*x0*x2 - 0.004286244036417964*x0*x3 - 0.2580978683904601*x0 + 0.001354823866066021*x1**2 - 0.00973931318884627*x1*x2 + 2.9129363259691755e-5*x1*x3 - 0.169384462874112*x1 + 0.03340024202343735*x2**2 - 0.005025505749609331*x2*x3 - 0.12015782467664578*x2 + 4.103052597698609e-6*x3**2 + 0.004444205741733472*x3) + 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model: 0.8709441310526813\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9138614832680617\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8084033068825933\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.8682158295175885\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.0000004897659 e^{- 9.205540033175983 \\cdot 10^{-5} x_{0}^{3} + 1.420859910694102 \\cdot 10^{-6} x_{0}^{2} x_{1} - 4.137046067683137 \\cdot 10^{-5} x_{0}^{2} x_{2} + 0.00031076902749998647 x_{0}^{2} x_{3} + 8.12800944527168 \\cdot 10^{-6} x_{0}^{2} - 0.0007543997074441295 x_{0} x_{1}^{2} + 8.413160680350168 \\cdot 10^{-6} x_{0} x_{1} x_{2} - 0.001768136631892626 x_{0} x_{1} x_{3} - 4.445456941335716 \\cdot 10^{-5} x_{0} x_{1} - 2.0689112186820324 \\cdot 10^{-5} x_{0} x_{2}^{2} + 0.00016718870452897094 x_{0} x_{2} x_{3} + 4.672875786443072 \\cdot 10^{-6} x_{0} x_{2} - 6.525368821809038 \\cdot 10^{-5} x_{0} x_{3}^{2} - 0.00023271613086312638 x_{0} x_{3} - 7.408224784279621 \\cdot 10^{-6} x_{0} - 9.64884806887795 \\cdot 10^{-5} x_{1}^{3} - 0.0003155982421590985 x_{1}^{2} x_{2} + 5.84274507411005 \\cdot 10^{-5} x_{1}^{2} x_{3} - 7.388339143758112 \\cdot 10^{-5} x_{1}^{2} + 4.560371895704769 \\cdot 10^{-6} x_{1} x_{2}^{2} - 0.0007578482496253008 x_{1} x_{2} x_{3} - 1.913642693477143 \\cdot 10^{-5} x_{1} x_{2} - 5.427017633237873 \\cdot 10^{-6} x_{1} x_{3}^{2} - 0.0004180076560564835 x_{1} x_{3} - 5.629077887521126 \\cdot 10^{-6} x_{1} - 1.0964997115305976 \\cdot 10^{-5} x_{2}^{3} + 9.360818542735503 \\cdot 10^{-5} x_{2}^{2} x_{3} + 2.5563398685612205 \\cdot 10^{-6} x_{2}^{2} - 4.8125019787920865 \\cdot 10^{-5} x_{2} x_{3}^{2} - 0.00010318951513777226 x_{2} x_{3} - 3.3577264564221742 \\cdot 10^{-6} x_{2} + 6.555932521392413 \\cdot 10^{-8} x_{3}^{3} + 8.698303970399563 \\cdot 10^{-5} x_{3}^{2} + 1.2882218472650215 \\cdot 10^{-5} x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.0000004897659*exp(-9.205540033175983e-5*x0**3 + 1.420859910694102e-6*x0**2*x1 - 4.137046067683137e-5*x0**2*x2 + 0.00031076902749998647*x0**2*x3 + 8.12800944527168e-6*x0**2 - 0.0007543997074441295*x0*x1**2 + 8.413160680350168e-6*x0*x1*x2 - 0.001768136631892626*x0*x1*x3 - 4.445456941335716e-5*x0*x1 - 2.0689112186820324e-5*x0*x2**2 + 0.00016718870452897094*x0*x2*x3 + 4.672875786443072e-6*x0*x2 - 6.525368821809038e-5*x0*x3**2 - 0.00023271613086312638*x0*x3 - 7.408224784279621e-6*x0 - 9.64884806887795e-5*x1**3 - 0.0003155982421590985*x1**2*x2 + 5.84274507411005e-5*x1**2*x3 - 7.388339143758112e-5*x1**2 + 4.560371895704769e-6*x1*x2**2 - 0.0007578482496253008*x1*x2*x3 - 1.913642693477143e-5*x1*x2 - 5.427017633237873e-6*x1*x3**2 - 0.0004180076560564835*x1*x3 - 5.629077887521126e-6*x1 - 1.0964997115305976e-5*x2**3 + 9.360818542735503e-5*x2**2*x3 + 2.5563398685612205e-6*x2**2 - 4.8125019787920865e-5*x2*x3**2 - 0.00010318951513777226*x2*x3 - 3.3577264564221742e-6*x2 + 6.555932521392413e-8*x3**3 + 8.698303970399563e-5*x3**2 + 1.2882218472650215e-5*x3) + 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 3, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train_rf_pred)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regression Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model: 0.874729618099276\n",
      "Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model: 0.9194773338007397\n",
      "Performance (F1 Score) Logistic Regressionn Polynomial Features: 0.8053099024513487\n",
      "Performance (Accuracy) Logistic Regressionn Polynomial Features: 0.8708892642762736\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1.0000016106431354 e^{- 0.0002274839421384301 x_{0}^{3} + 1.017661883581187 \\cdot 10^{-5} x_{0}^{2} x_{1} - 0.00010432914171845161 x_{0}^{2} x_{2} + 0.0009355966093813967 x_{0}^{2} x_{3} + 2.0825992943575626 \\cdot 10^{-5} x_{0}^{2} - 0.0008699754312646064 x_{0} x_{1}^{2} + 1.8744244188375736 \\cdot 10^{-5} x_{0} x_{1} x_{2} - 0.0005229717180125711 x_{0} x_{1} x_{3} - 5.1416383711300855 \\cdot 10^{-5} x_{0} x_{1} - 5.2400464705139407 \\cdot 10^{-5} x_{0} x_{2}^{2} + 0.0005250368543701037 x_{0} x_{2} x_{3} + 1.2080469821845088 \\cdot 10^{-5} x_{0} x_{2} - 1.641199567315536 \\cdot 10^{-5} x_{0} x_{3}^{2} - 0.0007047405800217966 x_{0} x_{3} - 1.7463009358849987 \\cdot 10^{-5} x_{0} - 2.4292854561875446 \\cdot 10^{-5} x_{1}^{3} - 0.00041963359923172965 x_{1}^{2} x_{2} - 2.2358829334282765 \\cdot 10^{-5} x_{1}^{2} x_{3} - 6.604770082801392 \\cdot 10^{-5} x_{1}^{2} + 1.1633686989751384 \\cdot 10^{-5} x_{1} x_{2}^{2} - 0.00027678053043936503 x_{1} x_{2} x_{3} - 2.398127764985474 \\cdot 10^{-5} x_{1} x_{2} + 9.0172298571421 \\cdot 10^{-7} x_{1} x_{3}^{2} - 0.0003436282390803636 x_{1} x_{3} - 5.417503235470609 \\cdot 10^{-6} x_{1} - 2.8066131040881334 \\cdot 10^{-5} x_{2}^{3} + 0.00027558765781202495 x_{2}^{2} x_{3} + 6.490263843822449 \\cdot 10^{-6} x_{2}^{2} - 1.3972110406408409 \\cdot 10^{-5} x_{2} x_{3}^{2} - 0.00033603349921119307 x_{2} x_{3} - 8.352676775437502 \\cdot 10^{-6} x_{2} + 1.8779616701487164 \\cdot 10^{-8} x_{3}^{3} + 7.729556049061049 \\cdot 10^{-6} x_{3}^{2} + 4.817522085386133 \\cdot 10^{-5} x_{3}} + 1}$"
      ],
      "text/plain": [
       "1/(1.0000016106431354*exp(-0.0002274839421384301*x0**3 + 1.017661883581187e-5*x0**2*x1 - 0.00010432914171845161*x0**2*x2 + 0.0009355966093813967*x0**2*x3 + 2.0825992943575626e-5*x0**2 - 0.0008699754312646064*x0*x1**2 + 1.8744244188375736e-5*x0*x1*x2 - 0.0005229717180125711*x0*x1*x3 - 5.1416383711300855e-5*x0*x1 - 5.2400464705139407e-5*x0*x2**2 + 0.0005250368543701037*x0*x2*x3 + 1.2080469821845088e-5*x0*x2 - 1.641199567315536e-5*x0*x3**2 - 0.0007047405800217966*x0*x3 - 1.7463009358849987e-5*x0 - 2.4292854561875446e-5*x1**3 - 0.00041963359923172965*x1**2*x2 - 2.2358829334282765e-5*x1**2*x3 - 6.604770082801392e-5*x1**2 + 1.1633686989751384e-5*x1*x2**2 - 0.00027678053043936503*x1*x2*x3 - 2.398127764985474e-5*x1*x2 + 9.0172298571421e-7*x1*x3**2 - 0.0003436282390803636*x1*x3 - 5.417503235470609e-6*x1 - 2.8066131040881334e-5*x2**3 + 0.00027558765781202495*x2**2*x3 + 6.490263843822449e-6*x2**2 - 1.3972110406408409e-5*x2*x3**2 - 0.00033603349921119307*x2*x3 - 8.352676775437502e-6*x2 + 1.8779616701487164e-8*x3**3 + 7.729556049061049e-6*x3**2 + 4.817522085386133e-5*x3) + 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree = 3, interaction_only=False, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000,\n",
    "                         penalty='l2', \n",
    "                         tol=0.0001, \n",
    "                         C=1.0, \n",
    "                         random_state=0,\n",
    "                         n_jobs=-1).fit(X_train_poly, y_train)\n",
    "\n",
    "preds = clf.predict(X_test_poly)\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) Logistic Regressionn Polynomial Features - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) Logistic Regressionn Polynomial Features - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) Logistic Regressionn Polynomial Features:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) Logistic Regressionn Polynomial Features:', acc_performance)\n",
    "\n",
    "variable_identifier_list = [variable_identifier.replace(' ', '*') for variable_identifier in poly.get_feature_names()]\n",
    "coef = ' + '.join([str(np.round(coefficient, 300)) + '*' + variable_identifier for variable_identifier, coefficient in zip(variable_identifier_list, clf.coef_[0])])\n",
    "intercept = str(np.round(clf.intercept_[0], 300))\n",
    "coef_intercept = coef + ' + ' + intercept\n",
    "\n",
    "logistic_regression_function = '1/(1+exp(-(' + coef_intercept + ')))'\n",
    "logistic_regression_function_sympy = sympify(logistic_regression_function)\n",
    "\n",
    "logistic_regression_function_sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity (F1 Score) 2*X0 - Random Forest Model: 0.9126073764149031\n",
      "Fidelity (Accuracy) 2*X0 - Random Forest Model: 0.9402887922100429\n",
      "Performance (F1 Score) 2*X0: 0.8796552490988172\n",
      "Performance (Accuracy) 2*X0: 0.9153228355936179\n"
     ]
    }
   ],
   "source": [
    "preds = np.clip(np.round(10*X_test[:,0]), 0, 1)\n",
    "\n",
    "f1_fidelity = f1_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (F1 Score) 2*X0 - Random Forest Model:', f1_fidelity)\n",
    "acc_fidelity = accuracy_score(y_test_rf_pred, preds)\n",
    "print('Fidelity (Accuracy) 2*X0 - Random Forest Model:', acc_fidelity)\n",
    "\n",
    "f1_performance = f1_score(y_test, preds)\n",
    "print('Performance (F1 Score) 2*X0:', f1_performance)\n",
    "acc_performance = accuracy_score(y_test, preds)\n",
    "print('Performance (Accuracy) 2*X0:', acc_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest: 0.9078285150504278\n"
     ]
    }
   ],
   "source": [
    "y_test_random_forest = random_forest_model.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_test_random_forest)\n",
    "print('Accuracy Random Forest: '+ str(accuracy_random_forest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
