{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:23.563624Z",
     "iopub.status.busy": "2022-06-04T12:45:23.563446Z",
     "iopub.status.idle": "2022-06-04T12:45:28.269495Z",
     "shell.execute_reply": "2022-06-04T12:45:28.268480Z",
     "shell.execute_reply.started": "2022-06-04T12:45:23.563583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def sigmoid_squeeze(x, factor=3):\n",
    "    x = 1/(1+K.exp(-factor*x))\n",
    "    return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa08eca3-ed45-4eca-bd81-d8d679cb13bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.270530Z",
     "iopub.status.busy": "2022-06-04T12:45:28.270403Z",
     "iopub.status.idle": "2022-06-04T12:45:28.273703Z",
     "shell.execute_reply": "2022-06-04T12:45:28.273302Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.270512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0c3ac7-7321-4224-8ffd-b103b1b669fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.275043Z",
     "iopub.status.busy": "2022-06-04T12:45:28.274859Z",
     "iopub.status.idle": "2022-06-04T12:45:28.330365Z",
     "shell.execute_reply": "2022-06-04T12:45:28.329694Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.275028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_real_world_data(X_data):\n",
    "    normalizer_list = []\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        for column_name in X_data:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_data[column_name].values.reshape(-1, 1))\n",
    "            X_data[column_name] = scaler.transform(X_data[column_name].values.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "    else:\n",
    "        for i, column in enumerate(X_data.T):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(column.reshape(-1, 1))\n",
    "            X_data[:,i] = scaler.transform(column.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "        \n",
    "    return X_data, normalizer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58193a0-9fea-4be3-8aca-d8f14b3480e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.331657Z",
     "iopub.status.busy": "2022-06-04T12:45:28.331423Z",
     "iopub.status.idle": "2022-06-04T12:45:28.473708Z",
     "shell.execute_reply": "2022-06-04T12:45:28.473274Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.331626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.activations.sparsemax([1.,3.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1576279-fe14-4861-964f-1f8ebb0a1608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.474515Z",
     "iopub.status.busy": "2022-06-04T12:45:28.474384Z",
     "iopub.status.idle": "2022-06-04T12:45:28.547517Z",
     "shell.execute_reply": "2022-06-04T12:45:28.547099Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.474498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.seq2seq.hardmax([1.,3.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86901d32-132c-4737-83fa-a789d46e3fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.548236Z",
     "iopub.status.busy": "2022-06-04T12:45:28.548106Z",
     "iopub.status.idle": "2022-06-04T12:45:28.595772Z",
     "shell.execute_reply": "2022-06-04T12:45:28.595470Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.548220Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.hard_sigmoid(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7d2dfc-8d2e-4c37-b257-e33fa853aaeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.596400Z",
     "iopub.status.busy": "2022-06-04T12:45:28.596284Z",
     "iopub.status.idle": "2022-06-04T12:45:28.626991Z",
     "shell.execute_reply": "2022-06-04T12:45:28.626700Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.596386Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sigmoid(1000*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf86db3b-ecdc-4135-8887-7308e9eff00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.627604Z",
     "iopub.status.busy": "2022-06-04T12:45:28.627492Z",
     "iopub.status.idle": "2022-06-04T12:45:28.718310Z",
     "shell.execute_reply": "2022-06-04T12:45:28.718012Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.627589Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.activations.sparsemax([1., 9, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f2cbe97-681d-4d9f-ac92-89c5ec9e441d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.718940Z",
     "iopub.status.busy": "2022-06-04T12:45:28.718826Z",
     "iopub.status.idle": "2022-06-04T12:45:28.771624Z",
     "shell.execute_reply": "2022-06-04T12:45:28.771324Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.718925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,))\n",
    "function_values_dhdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca547920-c17d-4199-a935-8c6b7878ca97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.772237Z",
     "iopub.status.busy": "2022-06-04T12:45:28.772121Z",
     "iopub.status.idle": "2022-06-04T12:45:28.816576Z",
     "shell.execute_reply": "2022-06-04T12:45:28.816247Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.772221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([function_values_dhdt, tf.constant([1.])], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b00c43a-52bc-48a0-b38a-a10a9d48133b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.817220Z",
     "iopub.status.busy": "2022-06-04T12:45:28.817101Z",
     "iopub.status.idle": "2022-06-04T12:45:28.874706Z",
     "shell.execute_reply": "2022-06-04T12:45:28.874245Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.817204Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 2, 3, 1], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([tf.constant([0,2,3]), tf.constant([1])], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2e6c81c-f508-40c5-a440-c88c1b833d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:45:28.876654Z",
     "iopub.status.busy": "2022-06-04T12:45:28.876532Z",
     "iopub.status.idle": "2022-06-04T12:45:28.890006Z",
     "shell.execute_reply": "2022-06-04T12:45:28.889518Z",
     "shell.execute_reply.started": "2022-06-04T12:45:28.876639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros(shape=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da5c63-bd40-4bd2-b72e-7d69aca5f842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f2e596-9bb2-4026-890e-7b42365e81f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:11:54.232383Z",
     "iopub.status.busy": "2022-06-04T13:11:54.232113Z",
     "iopub.status.idle": "2022-06-04T13:11:54.269268Z",
     "shell.execute_reply": "2022-06-04T13:11:54.268428Z",
     "shell.execute_reply.started": "2022-06-04T13:11:54.232355Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHDT(tf.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            squeeze_factor = 5,\n",
    "            learning_rate=1e-3,\n",
    "            loss='binary_crossentropy',#'mae',\n",
    "            optimizer = 'adam',\n",
    "            random_seed=42,\n",
    "            verbosity=1):    \n",
    "        \n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = tf.keras.losses.get(loss)\n",
    "        self.seed = random_seed\n",
    "        self.verbosity = verbosity\n",
    "        self.function_representation_type = function_representation_type\n",
    "        self.number_of_variables = number_of_variables\n",
    "        self.squeeze_factor = squeeze_factor\n",
    "        \n",
    "        self.internal_node_num_ = 2 ** self.depth - 1 \n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "        \n",
    "        tf.random.set_seed(self.seed)\n",
    "        \n",
    "        function_representation_length = ( \n",
    "          (2 ** self.depth - 1) * 2 + (2 ** self.depth)  if self.function_representation_type == 1 \n",
    "          else (2 ** self.depth - 1) + ((2 ** self.depth - 1) * self.number_of_variables) + (2 ** self.depth) if self.function_representation_type == 2 \n",
    "          else ((2 ** self.depth - 1) * self.number_of_variables * 2) + (2 ** self.depth)  if self.function_representation_type >= 3 \n",
    "          else None\n",
    "                                      )        \n",
    "        \n",
    "        self.dt_params =  tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(function_representation_length,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        \n",
    "        tf.print(self.dt_params)\n",
    "        \n",
    "        maximum_depth = self.depth\n",
    "        leaf_node_num_ = 2 ** maximum_depth\n",
    "        internal_node_num_ = 2 ** maximum_depth - 1\n",
    "        \n",
    "        #dt_params_activation = self.dt_params#self.apply_activation(self.dt_params)\n",
    "        \n",
    "        #internal_nodes, leaf_nodes = self.get_shaped_parameters_for_decision_tree(dt_params_activation)\n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_         \n",
    "        \n",
    "        self.split_values = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_values_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_values')\n",
    "        #tf.sigmoid(self.dt_params[:split_values_num_params])\n",
    "        self.split_index_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_index_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_index_array')\n",
    "        #self.dt_params[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "        self.leaf_classes_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(leaf_classes_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='leaf_classes_array')\n",
    "        #tf.sigmoid(self.dt_params[split_values_num_params+split_index_num_params:])        \n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        self.optimizer.learning_rate = self.learning_rate\n",
    "                \n",
    "    def fit(self, X, y, batch_size=32, epochs=100, early_stopping_epochs=5):\n",
    "        \n",
    "        minimum_loss_epoch = np.inf\n",
    "        epochs_without_improvement = 0        \n",
    "        \n",
    "        for current_epoch in tqdm(range(epochs)):\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            X = tf.random.shuffle(X, seed=self.seed + current_epoch)\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            y = tf.random.shuffle(y, seed=self.seed + current_epoch)\n",
    "            \n",
    "            loss_list = []\n",
    "            for index, (X_batch, y_batch) in enumerate(zip(make_batch(X, batch_size), make_batch(y, batch_size))):\n",
    "                current_loss = self.backward(X_batch, y_batch)\n",
    "                loss_list.append(float(current_loss))\n",
    "                \n",
    "                if self.verbosity >= 2:\n",
    "                    batch_idx = (index+1)*batch_size\n",
    "                    msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    print(msg.format(current_epoch, batch_idx, current_loss))                   \n",
    "                  \n",
    "            if self.verbosity > 0:    \n",
    "                msg = \"Epoch: {:02d} | Loss: {:.5f} |\"\n",
    "                print(msg.format(current_epoch, np.mean(loss_list)))              \n",
    "            \n",
    "            current_loss_epoch = np.mean(loss_list)\n",
    "\n",
    "            if current_loss_epoch < minimum_loss_epoch:\n",
    "                minimum_loss_epoch = current_loss_epoch\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= early_stopping_epochs:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    #split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    split_index = tfa.activations.sparsemax(10 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "    def predict(self, X):\n",
    "        return tf.sigmoid(self.forward(X))\n",
    "        \n",
    "    def backward(self, x,y):\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)#tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            #tape.watch(self.dt_params)\n",
    "            predicted = self.forward(x)\n",
    "            current_loss = self.loss(y, predicted, from_logits=True)\n",
    "            \n",
    "        #tf.print('predicted', predicted)\n",
    "        #tf.print('current_loss', current_loss, summarize=-1)\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.leaf_classes_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.leaf_classes_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)        \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.split_values)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_values]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.split_index_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_index_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "\n",
    "        #optimizer.apply_gradients(zip(grads, self.dt_params),\n",
    "        #                          global_step=tf.compat.v1.train.get_or_create_global_step())     \n",
    "        \n",
    "        #self.optimizer.apply_gradients(zip([grads], [self.dt_params]))\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        return current_loss\n",
    "        \n",
    "    def plot(self, normalizer_list=None, path='./dt_plot.png'):\n",
    "        from anytree import Node, RenderTree\n",
    "        from anytree.exporter import DotExporter\n",
    "\n",
    "        internal_node_num_ = 2 ** self.depth - 1 \n",
    "        \n",
    "        #split_values = self.split_values\n",
    "        split_values = sigmoid_squeeze(self.split_values, self.squeeze_factor)\n",
    "        split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "        split_index_array = self.split_index_array \n",
    "        split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "        split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "        splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "        \n",
    "        splits = splits.numpy()\n",
    "        leaf_classes = tf.sigmoid(self.leaf_classes_array).numpy()\n",
    "\n",
    "\n",
    "        if normalizer_list is not None: \n",
    "            transpose = splits.transpose()\n",
    "            transpose_normalized = []\n",
    "            for i, column in enumerate(transpose):\n",
    "                column_new = column\n",
    "                if len(column_new[column_new != 0]) != 0:\n",
    "                    column_new[column_new != 0] = normalizer_list[i].inverse_transform(column[column != 0].reshape(-1, 1)).ravel()\n",
    "                #column_new = normalizer_list[i].inverse_transform(column.reshape(-1, 1)).ravel()\n",
    "                transpose_normalized.append(column_new)\n",
    "            splits = np.array(transpose_normalized).transpose()\n",
    "\n",
    "        splits_by_layer = []\n",
    "        for i in range(self.depth+1):\n",
    "            start = 2**i - 1\n",
    "            end = 2**(i+1) -1\n",
    "            splits_by_layer.append(splits[start:end])\n",
    "\n",
    "        nodes = {\n",
    "        }\n",
    "        #tree = Tree()\n",
    "        for i, splits in enumerate(splits_by_layer):\n",
    "            for j, split in enumerate(splits):\n",
    "                if i == 0:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, data=None)            \n",
    "                else:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "                    parent_name = 'n' + str(parent_node_id)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)\n",
    "\n",
    "        for j, leaf_class in enumerate(leaf_classes):\n",
    "            i = self.depth\n",
    "            current_node_id = int(2**i - 1 + j)\n",
    "            name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "            parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "            parent_name = 'n' + str(parent_node_id)\n",
    "            #split_variable = np.argmax(np.abs(split))\n",
    "            #split_value = np.round(split[split_variable], 3)\n",
    "            split_description = str(np.round((leaf_class), 3))#'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "            nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "            #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)        \n",
    "\n",
    "            DotExporter(nodes['n0'], nodeattrfunc=lambda node: 'label=\"{}\"'.format(node.display_name)).to_picture(path)\n",
    "\n",
    "\n",
    "        return Image(path)#, nodes#nodes#tree        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7298793a-3131-489d-88c2-8c9da85d6db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:11:55.149433Z",
     "iopub.status.busy": "2022-06-04T13:11:55.149167Z",
     "iopub.status.idle": "2022-06-04T13:11:55.163162Z",
     "shell.execute_reply": "2022-06-04T13:11:55.162535Z",
     "shell.execute_reply.started": "2022-06-04T13:11:55.149402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10_000, n_features=5, n_informative=2, n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "#todo: anpassen, dass nur basierend auf train data normalized\n",
    "X, normalizer_list = normalize_real_world_data(X)\n",
    "\n",
    "train_samples = 1_000#1000  # Samples used for training the models\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    shuffle=False,\n",
    "    test_size=10_000 - train_samples,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46fb31cb-037d-43de-9387-c1eb289a0f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:11:56.100627Z",
     "iopub.status.busy": "2022-06-04T13:11:56.100025Z",
     "iopub.status.idle": "2022-06-04T13:11:56.114477Z",
     "shell.execute_reply": "2022-06-04T13:11:56.113755Z",
     "shell.execute_reply.started": "2022-06-04T13:11:56.100593Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867777777777778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "model_sklearn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f931c0-e848-4924-b020-bcd5de1f31a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:11:57.090757Z",
     "iopub.status.busy": "2022-06-04T13:11:57.090560Z",
     "iopub.status.idle": "2022-06-04T13:12:12.774876Z",
     "shell.execute_reply": "2022-06-04T13:12:12.774240Z",
     "shell.execute_reply.started": "2022-06-04T13:11:57.090730Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0328287482 -0.0907697529 -0.00796891749 ... 0.0222832412 0.156567946 -0.0625172406]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183fcc0d29d84792b52d97c0a2d7f9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Got an unexpected keyword argument 'from_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2604804/2531506379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             verbosity=1)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_test_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2604804/2933999925.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, epochs, early_stopping_epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2604804/2933999925.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m#tape.watch(self.dt_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m#tf.print('predicted', predicted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miterable_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_iterable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Got an unexpected keyword argument 'from_logits'"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='mae',#'binary_crossentropy',\n",
    "            random_seed=42,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c109ca-984a-48b6-b3dc-d8439c279bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:12:19.172376Z",
     "iopub.status.busy": "2022-06-04T13:12:19.172163Z",
     "iopub.status.idle": "2022-06-04T13:13:51.461618Z",
     "shell.execute_reply": "2022-06-04T13:13:51.460803Z",
     "shell.execute_reply.started": "2022-06-04T13:12:19.172350Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.147681668 -0.171174631 0.0654113144 ... 0.0471678823 -0.117765374 -0.0936949179]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5b18e569b04366a925441e59d7beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.73191 |\n",
      "Epoch: 01 | Loss: 0.69488 |\n",
      "Epoch: 02 | Loss: 0.67161 |\n",
      "Epoch: 03 | Loss: 0.65227 |\n",
      "Epoch: 04 | Loss: 0.63575 |\n",
      "Epoch: 05 | Loss: 0.61581 |\n",
      "Epoch: 06 | Loss: 0.59687 |\n",
      "Epoch: 07 | Loss: 0.58361 |\n",
      "Epoch: 08 | Loss: 0.57027 |\n",
      "Epoch: 09 | Loss: 0.55967 |\n",
      "Epoch: 10 | Loss: 0.54740 |\n",
      "Epoch: 11 | Loss: 0.53671 |\n",
      "Epoch: 12 | Loss: 0.52692 |\n",
      "Epoch: 13 | Loss: 0.51966 |\n",
      "Epoch: 14 | Loss: 0.50947 |\n",
      "Epoch: 15 | Loss: 0.50266 |\n",
      "Epoch: 16 | Loss: 0.49619 |\n",
      "Epoch: 17 | Loss: 0.49069 |\n",
      "Epoch: 18 | Loss: 0.48211 |\n",
      "Epoch: 19 | Loss: 0.47625 |\n",
      "Epoch: 20 | Loss: 0.47214 |\n",
      "Epoch: 21 | Loss: 0.46519 |\n",
      "Epoch: 22 | Loss: 0.46093 |\n",
      "Epoch: 23 | Loss: 0.45588 |\n",
      "Epoch: 24 | Loss: 0.45285 |\n",
      "Epoch: 25 | Loss: 0.44977 |\n",
      "Epoch: 26 | Loss: 0.44390 |\n",
      "Epoch: 27 | Loss: 0.44005 |\n",
      "Epoch: 28 | Loss: 0.43740 |\n",
      "Epoch: 29 | Loss: 0.43389 |\n",
      "Epoch: 30 | Loss: 0.42890 |\n",
      "Epoch: 31 | Loss: 0.42482 |\n",
      "Epoch: 32 | Loss: 0.42316 |\n",
      "Epoch: 33 | Loss: 0.42077 |\n",
      "Epoch: 34 | Loss: 0.41970 |\n",
      "Epoch: 35 | Loss: 0.41405 |\n",
      "Epoch: 36 | Loss: 0.41329 |\n",
      "Epoch: 37 | Loss: 0.41176 |\n",
      "Epoch: 38 | Loss: 0.40907 |\n",
      "Epoch: 39 | Loss: 0.40967 |\n",
      "Epoch: 40 | Loss: 0.40568 |\n",
      "Epoch: 41 | Loss: 0.40405 |\n",
      "Epoch: 42 | Loss: 0.39900 |\n",
      "Epoch: 43 | Loss: 0.39992 |\n",
      "Epoch: 44 | Loss: 0.39913 |\n",
      "Epoch: 45 | Loss: 0.39468 |\n",
      "Epoch: 46 | Loss: 0.39407 |\n",
      "Epoch: 47 | Loss: 0.39393 |\n",
      "Epoch: 48 | Loss: 0.39325 |\n",
      "Epoch: 49 | Loss: 0.38965 |\n",
      "Epoch: 50 | Loss: 0.38737 |\n",
      "Epoch: 51 | Loss: 0.38768 |\n",
      "Epoch: 52 | Loss: 0.38513 |\n",
      "Epoch: 53 | Loss: 0.38201 |\n",
      "Epoch: 54 | Loss: 0.38384 |\n",
      "Epoch: 55 | Loss: 0.38252 |\n",
      "Epoch: 56 | Loss: 0.38184 |\n",
      "Epoch: 57 | Loss: 0.38024 |\n",
      "Epoch: 58 | Loss: 0.38049 |\n",
      "Epoch: 59 | Loss: 0.38094 |\n",
      "Epoch: 60 | Loss: 0.37602 |\n",
      "Epoch: 61 | Loss: 0.37732 |\n",
      "Epoch: 62 | Loss: 0.37623 |\n",
      "Epoch: 63 | Loss: 0.37681 |\n",
      "Epoch: 64 | Loss: 0.37218 |\n",
      "Epoch: 65 | Loss: 0.37415 |\n",
      "Epoch: 66 | Loss: 0.37414 |\n",
      "Epoch: 67 | Loss: 0.37168 |\n",
      "Epoch: 68 | Loss: 0.36947 |\n",
      "Epoch: 69 | Loss: 0.37018 |\n",
      "Epoch: 70 | Loss: 0.36963 |\n",
      "Epoch: 71 | Loss: 0.37017 |\n",
      "Epoch: 72 | Loss: 0.36627 |\n",
      "Epoch: 73 | Loss: 0.36581 |\n",
      "Epoch: 74 | Loss: 0.36551 |\n",
      "Epoch: 75 | Loss: 0.36709 |\n",
      "Epoch: 76 | Loss: 0.36400 |\n",
      "Epoch: 77 | Loss: 0.36169 |\n",
      "Epoch: 78 | Loss: 0.36423 |\n",
      "Epoch: 79 | Loss: 0.36051 |\n",
      "Epoch: 80 | Loss: 0.36284 |\n",
      "Epoch: 81 | Loss: 0.35920 |\n",
      "Epoch: 82 | Loss: 0.35989 |\n",
      "Epoch: 83 | Loss: 0.36099 |\n",
      "Epoch: 84 | Loss: 0.35931 |\n",
      "Epoch: 85 | Loss: 0.35967 |\n",
      "Epoch: 86 | Loss: 0.35733 |\n",
      "Epoch: 87 | Loss: 0.35750 |\n",
      "Epoch: 88 | Loss: 0.35813 |\n",
      "Epoch: 89 | Loss: 0.35907 |\n",
      "Epoch: 90 | Loss: 0.35911 |\n",
      "Epoch: 91 | Loss: 0.35732 |\n",
      "Epoch: 92 | Loss: 0.35388 |\n",
      "Epoch: 93 | Loss: 0.35268 |\n",
      "Epoch: 94 | Loss: 0.35722 |\n",
      "Epoch: 95 | Loss: 0.35709 |\n",
      "Epoch: 96 | Loss: 0.35817 |\n",
      "Epoch: 97 | Loss: 0.35549 |\n",
      "Epoch: 98 | Loss: 0.35515 |\n",
      "Epoch: 99 | Loss: 0.35256 |\n",
      "Epoch: 100 | Loss: 0.35194 |\n",
      "Epoch: 101 | Loss: 0.35575 |\n",
      "Epoch: 102 | Loss: 0.35201 |\n",
      "Epoch: 103 | Loss: 0.35265 |\n",
      "Epoch: 104 | Loss: 0.34968 |\n",
      "Epoch: 105 | Loss: 0.34946 |\n",
      "Epoch: 106 | Loss: 0.35178 |\n",
      "Epoch: 107 | Loss: 0.35140 |\n",
      "Epoch: 108 | Loss: 0.34919 |\n",
      "Epoch: 109 | Loss: 0.34960 |\n",
      "Epoch: 110 | Loss: 0.35093 |\n",
      "Epoch: 111 | Loss: 0.34887 |\n",
      "Epoch: 112 | Loss: 0.34827 |\n",
      "Epoch: 113 | Loss: 0.35148 |\n",
      "Epoch: 114 | Loss: 0.34647 |\n",
      "Epoch: 115 | Loss: 0.35006 |\n",
      "Epoch: 116 | Loss: 0.34937 |\n",
      "Epoch: 117 | Loss: 0.34548 |\n",
      "Epoch: 118 | Loss: 0.34976 |\n",
      "Epoch: 119 | Loss: 0.34420 |\n",
      "Epoch: 120 | Loss: 0.34810 |\n",
      "Epoch: 121 | Loss: 0.34609 |\n",
      "Epoch: 122 | Loss: 0.34530 |\n",
      "Epoch: 123 | Loss: 0.34739 |\n",
      "Epoch: 124 | Loss: 0.34748 |\n",
      "Epoch: 125 | Loss: 0.34309 |\n",
      "Epoch: 126 | Loss: 0.34777 |\n",
      "Epoch: 127 | Loss: 0.34515 |\n",
      "Epoch: 128 | Loss: 0.34180 |\n",
      "Epoch: 129 | Loss: 0.34257 |\n",
      "Epoch: 130 | Loss: 0.34250 |\n",
      "Epoch: 131 | Loss: 0.34522 |\n",
      "Epoch: 132 | Loss: 0.34079 |\n",
      "Epoch: 133 | Loss: 0.34198 |\n",
      "Epoch: 134 | Loss: 0.34116 |\n",
      "Epoch: 135 | Loss: 0.34008 |\n",
      "Epoch: 136 | Loss: 0.33942 |\n",
      "Epoch: 137 | Loss: 0.34337 |\n",
      "Epoch: 138 | Loss: 0.34254 |\n",
      "Epoch: 139 | Loss: 0.34071 |\n",
      "Epoch: 140 | Loss: 0.34551 |\n",
      "Epoch: 141 | Loss: 0.34032 |\n",
      "Epoch: 142 | Loss: 0.34030 |\n",
      "Epoch: 143 | Loss: 0.34016 |\n",
      "Epoch: 144 | Loss: 0.34078 |\n",
      "Epoch: 145 | Loss: 0.33923 |\n",
      "Epoch: 146 | Loss: 0.33843 |\n",
      "Epoch: 147 | Loss: 0.34121 |\n",
      "Epoch: 148 | Loss: 0.33857 |\n",
      "Epoch: 149 | Loss: 0.33868 |\n",
      "Epoch: 150 | Loss: 0.33913 |\n",
      "Epoch: 151 | Loss: 0.33557 |\n",
      "Epoch: 152 | Loss: 0.33738 |\n",
      "Epoch: 153 | Loss: 0.33717 |\n",
      "Epoch: 154 | Loss: 0.33758 |\n",
      "Epoch: 155 | Loss: 0.34126 |\n",
      "Epoch: 156 | Loss: 0.34229 |\n",
      "Epoch: 157 | Loss: 0.33461 |\n",
      "Epoch: 158 | Loss: 0.33745 |\n",
      "Epoch: 159 | Loss: 0.33877 |\n",
      "Epoch: 160 | Loss: 0.34447 |\n",
      "Epoch: 161 | Loss: 0.33840 |\n",
      "Epoch: 162 | Loss: 0.33596 |\n",
      "Epoch: 163 | Loss: 0.33650 |\n",
      "Epoch: 164 | Loss: 0.33719 |\n",
      "Epoch: 165 | Loss: 0.33897 |\n",
      "Epoch: 166 | Loss: 0.33485 |\n",
      "Epoch: 167 | Loss: 0.33735 |\n",
      "Epoch: 168 | Loss: 0.33489 |\n",
      "Epoch: 169 | Loss: 0.33414 |\n",
      "Epoch: 170 | Loss: 0.33440 |\n",
      "Epoch: 171 | Loss: 0.33288 |\n",
      "Epoch: 172 | Loss: 0.33633 |\n",
      "Epoch: 173 | Loss: 0.33243 |\n",
      "Epoch: 174 | Loss: 0.33682 |\n",
      "Epoch: 175 | Loss: 0.33381 |\n",
      "Epoch: 176 | Loss: 0.33427 |\n",
      "Epoch: 177 | Loss: 0.33472 |\n",
      "Epoch: 178 | Loss: 0.33121 |\n",
      "Epoch: 179 | Loss: 0.33343 |\n",
      "Epoch: 180 | Loss: 0.32980 |\n",
      "Epoch: 181 | Loss: 0.33253 |\n",
      "Epoch: 182 | Loss: 0.33288 |\n",
      "Epoch: 183 | Loss: 0.33117 |\n",
      "Epoch: 184 | Loss: 0.33198 |\n",
      "Epoch: 185 | Loss: 0.33630 |\n",
      "Epoch: 186 | Loss: 0.33106 |\n",
      "Epoch: 187 | Loss: 0.33140 |\n",
      "Epoch: 188 | Loss: 0.33465 |\n",
      "Epoch: 189 | Loss: 0.33204 |\n",
      "Epoch: 190 | Loss: 0.33093 |\n",
      "Epoch: 191 | Loss: 0.32874 |\n",
      "Epoch: 192 | Loss: 0.33247 |\n",
      "Epoch: 193 | Loss: 0.33063 |\n",
      "Epoch: 194 | Loss: 0.33178 |\n",
      "Epoch: 195 | Loss: 0.33165 |\n",
      "Epoch: 196 | Loss: 0.33081 |\n",
      "Epoch: 197 | Loss: 0.33187 |\n",
      "Epoch: 198 | Loss: 0.33675 |\n",
      "Epoch: 199 | Loss: 0.32677 |\n",
      "Epoch: 200 | Loss: 0.32956 |\n",
      "Epoch: 201 | Loss: 0.32749 |\n",
      "Epoch: 202 | Loss: 0.33157 |\n",
      "Epoch: 203 | Loss: 0.33429 |\n",
      "Epoch: 204 | Loss: 0.33113 |\n",
      "Epoch: 205 | Loss: 0.33242 |\n",
      "Epoch: 206 | Loss: 0.33187 |\n",
      "Epoch: 207 | Loss: 0.33179 |\n",
      "Epoch: 208 | Loss: 0.33198 |\n",
      "Epoch: 209 | Loss: 0.33168 |\n",
      "Epoch: 210 | Loss: 0.33058 |\n",
      "Epoch: 211 | Loss: 0.32992 |\n",
      "Epoch: 212 | Loss: 0.33130 |\n",
      "Epoch: 213 | Loss: 0.33136 |\n",
      "Epoch: 214 | Loss: 0.32799 |\n",
      "Epoch: 215 | Loss: 0.33024 |\n",
      "Epoch: 216 | Loss: 0.32892 |\n",
      "Epoch: 217 | Loss: 0.33078 |\n",
      "Epoch: 218 | Loss: 0.32699 |\n",
      "Epoch: 219 | Loss: 0.33071 |\n",
      "Test Accuracy 0.881\n"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa0dfba-21f2-4c6b-9c34-e836e7dadf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:32.461383Z",
     "iopub.status.busy": "2022-06-04T12:50:32.461260Z",
     "iopub.status.idle": "2022-06-04T12:50:34.040987Z",
     "shell.execute_reply": "2022-06-04T12:50:34.040002Z",
     "shell.execute_reply.started": "2022-06-04T12:50:32.461365Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "image = model_dhdt.plot()\n",
    "display(image)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(model_sklearn, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260bce89-e3d1-474d-9df3-21cbc4f70194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:34.042812Z",
     "iopub.status.busy": "2022-06-04T12:50:34.042250Z",
     "iopub.status.idle": "2022-06-04T12:50:34.048033Z",
     "shell.execute_reply": "2022-06-04T12:50:34.047425Z",
     "shell.execute_reply.started": "2022-06-04T12:50:34.042786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e83a9767-c75c-46e9-ac03-6dc2831e602f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:34.049153Z",
     "iopub.status.busy": "2022-06-04T12:50:34.048824Z",
     "iopub.status.idle": "2022-06-04T12:50:34.133000Z",
     "shell.execute_reply": "2022-06-04T12:50:34.132203Z",
     "shell.execute_reply.started": "2022-06-04T12:50:34.049133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt.dt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de46c63a-a485-4d91-9371-1e968aa5c0e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:34.134663Z",
     "iopub.status.busy": "2022-06-04T12:50:34.134209Z",
     "iopub.status.idle": "2022-06-04T12:50:34.176559Z",
     "shell.execute_reply": "2022-06-04T12:50:34.175772Z",
     "shell.execute_reply.started": "2022-06-04T12:50:34.134626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.sigmoid(model_dhdt.dt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6c2019e-609d-4f52-93d6-29fa5934f320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:34.178280Z",
     "iopub.status.busy": "2022-06-04T12:50:34.177734Z",
     "iopub.status.idle": "2022-06-04T12:50:36.591554Z",
     "shell.execute_reply": "2022-06-04T12:50:36.590822Z",
     "shell.execute_reply.started": "2022-06-04T12:50:34.178243Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba167628-4a58-47e8-92c6-d56274e650d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T12:50:36.592840Z",
     "iopub.status.busy": "2022-06-04T12:50:36.592710Z",
     "iopub.status.idle": "2022-06-04T12:50:38.689144Z",
     "shell.execute_reply": "2022-06-04T12:50:38.688500Z",
     "shell.execute_reply.started": "2022-06-04T12:50:36.592819Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "image = model_dhdt.plot()\n",
    "display(image)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(model_sklearn, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32042bc9-1008-4311-8d2d-7896db8a3a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
