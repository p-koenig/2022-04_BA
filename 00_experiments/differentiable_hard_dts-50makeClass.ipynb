{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3b444-064b-4639-983d-238b3da68ad9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6531aaa-2c26-40f5-8078-d754aceda153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:21:20.050821Z",
     "iopub.status.busy": "2022-06-09T07:21:20.050669Z",
     "iopub.status.idle": "2022-06-09T07:21:20.057975Z",
     "shell.execute_reply": "2022-06-09T07:21:20.057682Z",
     "shell.execute_reply.started": "2022-06-09T07:21:20.050779Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dhdt': {\n",
    "        'depth': 3,\n",
    "        'learning_rate': 0.01,#1e-3,\n",
    "        \n",
    "        'initializer': 'GlorotUniform', #GlorotUniform\n",
    "        \n",
    "        'loss': 'binary_crossentropy',#'mae',\n",
    "        'optimizer': 'adam',        \n",
    "        \n",
    "        'beta_1': 10,\n",
    "        'beta_2': 50,\n",
    "        \n",
    "        'squeeze_factor': 1,\n",
    "        \n",
    "        'batch_size': 512,\n",
    "        'epochs': 1_000,\n",
    "        'early_stopping_epochs': 50,\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "    'make_classification': {\n",
    "        'number_of_variables': 5,\n",
    "        'n_samples': 5_000,\n",
    "        'num_eval': 50,\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'trials': 5,\n",
    "        'n_jobs': 50,\n",
    "        'verbosity': 0,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:21:20.058591Z",
     "iopub.status.busy": "2022-06-09T07:21:20.058461Z",
     "iopub.status.idle": "2022-06-09T07:21:24.926982Z",
     "shell.execute_reply": "2022-06-09T07:21:24.926178Z",
     "shell.execute_reply.started": "2022-06-09T07:21:20.058576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities import *\n",
    "from utilities.DHDT import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397843-32d2-4579-b197-0686f4a99b54",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e7da9-12bb-467f-919f-4fd510ce6463",
   "metadata": {},
   "source": [
    "## make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8a1baf-79d0-4959-8376-fc6a5446a229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:21:24.928819Z",
     "iopub.status.busy": "2022-06-09T07:21:24.928530Z",
     "iopub.status.idle": "2022-06-09T07:21:24.935432Z",
     "shell.execute_reply": "2022-06-09T07:21:24.934904Z",
     "shell.execute_reply.started": "2022-06-09T07:21:24.928793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    config_test = deepcopy(config)\n",
    "    config_test['make_classification']['n_samples'] = 10_000\n",
    "    config_test['dhdt']['epochs'] = 3\n",
    "\n",
    "    dataset_dict = {}\n",
    "    model_dict = {}\n",
    "\n",
    "    scores_dict = {'sklearn': {},\n",
    "                   'DHDT': {}}\n",
    "\n",
    "    dataset_dict = get_preprocessed_dataset('make_classification',\n",
    "                                            random_seed=config_test['computation']['random_seed'],\n",
    "                                            config=config_test['make_classification'],\n",
    "                                            verbosity=1)\n",
    "\n",
    "    model_dict['sklearn'] = DecisionTreeClassifier(max_depth=3, \n",
    "                                                   random_state=config_test['computation']['random_seed'])\n",
    "\n",
    "    model_dict['sklearn'].fit(dataset_dict['X_train'], \n",
    "                              dataset_dict['y_train'])\n",
    "\n",
    "\n",
    "\n",
    "    model_dict['DHDT'] = DHDT(dataset_dict['X_train'].shape[1],\n",
    "\n",
    "                                depth = config_test['dhdt']['depth'],\n",
    "\n",
    "                                learning_rate = config_test['dhdt']['learning_rate'],\n",
    "                                optimizer = config_test['dhdt']['optimizer'],\n",
    "\n",
    "                                beta_1 = config_test['dhdt']['beta_1'],\n",
    "                                beta_2 = config_test['dhdt']['beta_2'],\n",
    "\n",
    "                                squeeze_factor = config_test['dhdt']['squeeze_factor'],\n",
    "\n",
    "                                loss = config_test['dhdt']['loss'],#'mae',\n",
    "\n",
    "                                random_seed = config_test['computation']['random_seed'],\n",
    "                                verbosity = 2)        \n",
    "\n",
    "\n",
    "    scores_dict['history'] = model_dict['DHDT'].fit(dataset_dict['X_train'], \n",
    "                                                  dataset_dict['y_train'], \n",
    "                                                  batch_size=config_test['dhdt']['batch_size'], \n",
    "                                                  epochs=config_test['dhdt']['epochs'], \n",
    "                                                  early_stopping_epochs=config_test['dhdt']['early_stopping_epochs'], \n",
    "                                                  valid_data=(dataset_dict['X_valid'], dataset_dict['y_valid']))\n",
    "\n",
    "\n",
    "\n",
    "    scores_dict['sklearn']['accuracy_test'] = model_dict['sklearn'].score(dataset_dict['X_test'], \n",
    "                                                                     dataset_dict['y_test'])\n",
    "\n",
    "\n",
    "    dataset_dict['y_test_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_test'])\n",
    "    scores_dict['DHDT']['accuracy_test'] = accuracy_score(dataset_dict['y_test'], np.round(dataset_dict['y_test_dhdt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a13b2a-ca9d-44de-9a52-b8ca61808093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:21:24.936452Z",
     "iopub.status.busy": "2022-06-09T07:21:24.936111Z",
     "iopub.status.idle": "2022-06-09T07:29:46.126202Z",
     "shell.execute_reply": "2022-06-09T07:29:46.125747Z",
     "shell.execute_reply.started": "2022-06-09T07:21:24.936433Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of  50 | elapsed:  5.0min remaining: 120.9min\n",
      "[Parallel(n_jobs=50)]: Done  19 out of  50 | elapsed:  5.9min remaining:  9.7min\n",
      "[Parallel(n_jobs=50)]: Done  36 out of  50 | elapsed:  7.0min remaining:  2.7min\n",
      "[Parallel(n_jobs=50)]: Done  50 out of  50 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_mean</th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>DHDT accuracy_test_std</th>\n",
       "      <th>sklearn accuracy_test_mean</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7142</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.046348</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.862</td>\n",
       "      <td>6.120654e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8720</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.074512</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.933</td>\n",
       "      <td>1.880000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.023025</td>\n",
       "      <td>0.8906</td>\n",
       "      <td>0.897</td>\n",
       "      <td>1.280000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.032327</td>\n",
       "      <td>0.8162</td>\n",
       "      <td>0.921</td>\n",
       "      <td>5.240000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6868</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.049648</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>0.790</td>\n",
       "      <td>2.520000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.825</td>\n",
       "      <td>8.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.045504</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.280000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6708</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.042696</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>0.900</td>\n",
       "      <td>4.920000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.052911</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>0.872</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.050787</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.711</td>\n",
       "      <td>8.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6520</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.110349</td>\n",
       "      <td>0.8580</td>\n",
       "      <td>0.866</td>\n",
       "      <td>4.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.065730</td>\n",
       "      <td>0.8474</td>\n",
       "      <td>0.869</td>\n",
       "      <td>1.080000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.7786</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.039782</td>\n",
       "      <td>0.8566</td>\n",
       "      <td>0.899</td>\n",
       "      <td>2.120000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.8398</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.875</td>\n",
       "      <td>2.480000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.025160</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.844</td>\n",
       "      <td>4.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.897</td>\n",
       "      <td>8.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.7272</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.083497</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>0.860</td>\n",
       "      <td>1.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.063635</td>\n",
       "      <td>0.8042</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.240000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6742</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.071121</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.910</td>\n",
       "      <td>4.040000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.7286</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.866</td>\n",
       "      <td>1.720000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.7408</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.091283</td>\n",
       "      <td>0.8570</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.8170</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.047011</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.860</td>\n",
       "      <td>1.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.8764</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.9506</td>\n",
       "      <td>0.965</td>\n",
       "      <td>2.880000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.7590</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.074522</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>0.930</td>\n",
       "      <td>9.200000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.8616</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.107036</td>\n",
       "      <td>0.9038</td>\n",
       "      <td>0.944</td>\n",
       "      <td>8.040000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.026776</td>\n",
       "      <td>0.7918</td>\n",
       "      <td>0.819</td>\n",
       "      <td>1.360000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.7108</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.122117</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.962</td>\n",
       "      <td>5.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.8468</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.018595</td>\n",
       "      <td>0.8928</td>\n",
       "      <td>0.893</td>\n",
       "      <td>4.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.052147</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.855</td>\n",
       "      <td>3.680000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.6784</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.057767</td>\n",
       "      <td>0.7986</td>\n",
       "      <td>0.849</td>\n",
       "      <td>2.520000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.8206</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.106447</td>\n",
       "      <td>0.9166</td>\n",
       "      <td>0.951</td>\n",
       "      <td>6.880000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.7222</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.106685</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>0.922</td>\n",
       "      <td>1.800000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.064167</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>0.862</td>\n",
       "      <td>1.160000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.8428</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>0.929</td>\n",
       "      <td>7.200000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.840000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.888</td>\n",
       "      <td>4.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.021541</td>\n",
       "      <td>0.8738</td>\n",
       "      <td>0.913</td>\n",
       "      <td>1.960000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.920</td>\n",
       "      <td>9.200000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.8278</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.8404</td>\n",
       "      <td>0.946</td>\n",
       "      <td>5.280000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.7628</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.059650</td>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.078413</td>\n",
       "      <td>0.7706</td>\n",
       "      <td>0.861</td>\n",
       "      <td>4.520000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.8452</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.840000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.826</td>\n",
       "      <td>4.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.028310</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.923</td>\n",
       "      <td>3.600000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.112987</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>0.943</td>\n",
       "      <td>5.520000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.7510</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>0.893</td>\n",
       "      <td>2.720000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.8812</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.909</td>\n",
       "      <td>2.280000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.7702</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.080824</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.848</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.7008</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.058184</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.817</td>\n",
       "      <td>4.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.200000e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_test_mean  DHDT accuracy_test_max  DHDT accuracy_test_std  \\\n",
       "0                    0.7142                   0.806                0.046348   \n",
       "1                    0.8720                   0.926                0.074512   \n",
       "2                    0.8632                   0.887                0.023025   \n",
       "3                    0.7514                   0.799                0.032327   \n",
       "4                    0.6868                   0.761                0.049648   \n",
       "5                    0.7818                   0.791                0.007730   \n",
       "6                    0.8704                   0.903                0.045504   \n",
       "7                    0.6708                   0.730                0.042696   \n",
       "8                    0.6950                   0.734                0.052911   \n",
       "9                    0.6582                   0.720                0.050787   \n",
       "10                   0.6520                   0.857                0.110349   \n",
       "11                   0.6720                   0.736                0.065730   \n",
       "12                   0.7786                   0.858                0.039782   \n",
       "13                   0.8398                   0.868                0.016461   \n",
       "14                   0.6934                   0.733                0.025160   \n",
       "15                   0.8602                   0.875                0.017475   \n",
       "16                   0.7272                   0.866                0.083497   \n",
       "17                   0.7736                   0.813                0.063635   \n",
       "18                   0.6742                   0.791                0.071121   \n",
       "19                   0.7286                   0.779                0.080584   \n",
       "20                   0.7408                   0.818                0.091283   \n",
       "21                   0.8170                   0.842                0.047011   \n",
       "22                   0.8764                   0.957                0.061963   \n",
       "23                   0.7590                   0.828                0.074522   \n",
       "24                   0.8616                   0.925                0.107036   \n",
       "25                   0.6422                   0.664                0.026776   \n",
       "26                   0.7108                   0.909                0.122117   \n",
       "27                   0.8468                   0.871                0.018595   \n",
       "28                   0.7368                   0.803                0.052147   \n",
       "29                   0.6784                   0.773                0.057767   \n",
       "30                   0.8206                   0.950                0.106447   \n",
       "31                   0.7222                   0.905                0.106685   \n",
       "32                   0.7068                   0.825                0.064167   \n",
       "33                   0.8428                   0.888                0.067686   \n",
       "34                   0.8120                   0.824                0.014533   \n",
       "35                   0.7456                   0.811                0.056102   \n",
       "36                   0.8080                   0.846                0.021541   \n",
       "37                   0.7624                   0.877                0.109755   \n",
       "38                   0.8278                   0.930                0.055214   \n",
       "39                   0.7628                   0.803                0.059650   \n",
       "40                   0.7152                   0.830                0.078413   \n",
       "41                   0.8452                   0.918                0.070641   \n",
       "42                   0.7124                   0.741                0.016596   \n",
       "43                   0.8696                   0.885                0.028310   \n",
       "44                   0.8132                   0.907                0.112987   \n",
       "45                   0.7510                   0.799                0.026161   \n",
       "46                   0.8812                   0.908                0.031739   \n",
       "47                   0.7702                   0.853                0.080824   \n",
       "48                   0.7008                   0.788                0.058184   \n",
       "49                   0.7776                   0.792                0.007499   \n",
       "\n",
       "    sklearn accuracy_test_mean  sklearn accuracy_test_max  \\\n",
       "0                       0.7396                      0.862   \n",
       "1                       0.9236                      0.933   \n",
       "2                       0.8906                      0.897   \n",
       "3                       0.8162                      0.921   \n",
       "4                       0.7774                      0.790   \n",
       "5                       0.8210                      0.825   \n",
       "6                       0.9294                      0.955   \n",
       "7                       0.8016                      0.900   \n",
       "8                       0.8710                      0.872   \n",
       "9                       0.6950                      0.711   \n",
       "10                      0.8580                      0.866   \n",
       "11                      0.8474                      0.869   \n",
       "12                      0.8566                      0.899   \n",
       "13                      0.8626                      0.875   \n",
       "14                      0.8360                      0.844   \n",
       "15                      0.8810                      0.897   \n",
       "16                      0.8320                      0.860   \n",
       "17                      0.8042                      0.829   \n",
       "18                      0.8898                      0.910   \n",
       "19                      0.8316                      0.866   \n",
       "20                      0.8570                      0.857   \n",
       "21                      0.8530                      0.860   \n",
       "22                      0.9506                      0.965   \n",
       "23                      0.9116                      0.930   \n",
       "24                      0.9038                      0.944   \n",
       "25                      0.7918                      0.819   \n",
       "26                      0.8540                      0.962   \n",
       "27                      0.8928                      0.893   \n",
       "28                      0.7814                      0.855   \n",
       "29                      0.7986                      0.849   \n",
       "30                      0.9166                      0.951   \n",
       "31                      0.8860                      0.922   \n",
       "32                      0.8388                      0.862   \n",
       "33                      0.9254                      0.929   \n",
       "34                      0.8758                      0.885   \n",
       "35                      0.8800                      0.888   \n",
       "36                      0.8738                      0.913   \n",
       "37                      0.9154                      0.920   \n",
       "38                      0.8404                      0.946   \n",
       "39                      0.8330                      0.833   \n",
       "40                      0.7706                      0.861   \n",
       "41                      0.9082                      0.945   \n",
       "42                      0.8252                      0.826   \n",
       "43                      0.9050                      0.923   \n",
       "44                      0.9154                      0.943   \n",
       "45                      0.8794                      0.893   \n",
       "46                      0.8976                      0.909   \n",
       "47                      0.8480                      0.848   \n",
       "48                      0.7290                      0.817   \n",
       "49                      0.8356                      0.838   \n",
       "\n",
       "    sklearn accuracy_test_std  \n",
       "0                6.120654e-02  \n",
       "1                1.880000e-02  \n",
       "2                1.280000e-02  \n",
       "3                5.240000e-02  \n",
       "4                2.520000e-02  \n",
       "5                8.000000e-03  \n",
       "6                1.280000e-02  \n",
       "7                4.920000e-02  \n",
       "8                2.000000e-03  \n",
       "9                8.000000e-03  \n",
       "10               4.000000e-03  \n",
       "11               1.080000e-02  \n",
       "12               2.120000e-02  \n",
       "13               2.480000e-02  \n",
       "14               4.000000e-03  \n",
       "15               8.000000e-03  \n",
       "16               1.400000e-02  \n",
       "17               1.240000e-02  \n",
       "18               4.040000e-02  \n",
       "19               1.720000e-02  \n",
       "20               0.000000e+00  \n",
       "21               1.400000e-02  \n",
       "22               2.880000e-02  \n",
       "23               9.200000e-03  \n",
       "24               8.040000e-02  \n",
       "25               1.360000e-02  \n",
       "26               5.400000e-02  \n",
       "27               4.000000e-04  \n",
       "28               3.680000e-02  \n",
       "29               2.520000e-02  \n",
       "30               6.880000e-02  \n",
       "31               1.800000e-02  \n",
       "32               1.160000e-02  \n",
       "33               7.200000e-03  \n",
       "34               1.840000e-02  \n",
       "35               4.000000e-03  \n",
       "36               1.960000e-02  \n",
       "37               9.200000e-03  \n",
       "38               5.280000e-02  \n",
       "39               0.000000e+00  \n",
       "40               4.520000e-02  \n",
       "41               1.840000e-02  \n",
       "42               4.000000e-04  \n",
       "43               3.600000e-02  \n",
       "44               5.520000e-02  \n",
       "45               2.720000e-02  \n",
       "46               2.280000e-02  \n",
       "47               1.110223e-16  \n",
       "48               4.400000e-02  \n",
       "49               1.200000e-03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.887</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.791</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.903</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.734</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.736</td>\n",
       "      <td>0.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.858</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.868</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.733</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.813</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.791</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.818</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.909</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.905</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.888</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.811</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.877</td>\n",
       "      <td>0.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.930</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.918</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.741</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.885</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.907</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.853</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.788</td>\n",
       "      <td>0.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.792</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_test_max  sklearn accuracy_test_max\n",
       "0                    0.806                      0.862\n",
       "1                    0.926                      0.933\n",
       "2                    0.887                      0.897\n",
       "3                    0.799                      0.921\n",
       "4                    0.761                      0.790\n",
       "5                    0.791                      0.825\n",
       "6                    0.903                      0.955\n",
       "7                    0.730                      0.900\n",
       "8                    0.734                      0.872\n",
       "9                    0.720                      0.711\n",
       "10                   0.857                      0.866\n",
       "11                   0.736                      0.869\n",
       "12                   0.858                      0.899\n",
       "13                   0.868                      0.875\n",
       "14                   0.733                      0.844\n",
       "15                   0.875                      0.897\n",
       "16                   0.866                      0.860\n",
       "17                   0.813                      0.829\n",
       "18                   0.791                      0.910\n",
       "19                   0.779                      0.866\n",
       "20                   0.818                      0.857\n",
       "21                   0.842                      0.860\n",
       "22                   0.957                      0.965\n",
       "23                   0.828                      0.930\n",
       "24                   0.925                      0.944\n",
       "25                   0.664                      0.819\n",
       "26                   0.909                      0.962\n",
       "27                   0.871                      0.893\n",
       "28                   0.803                      0.855\n",
       "29                   0.773                      0.849\n",
       "30                   0.950                      0.951\n",
       "31                   0.905                      0.922\n",
       "32                   0.825                      0.862\n",
       "33                   0.888                      0.929\n",
       "34                   0.824                      0.885\n",
       "35                   0.811                      0.888\n",
       "36                   0.846                      0.913\n",
       "37                   0.877                      0.920\n",
       "38                   0.930                      0.946\n",
       "39                   0.803                      0.833\n",
       "40                   0.830                      0.861\n",
       "41                   0.918                      0.945\n",
       "42                   0.741                      0.826\n",
       "43                   0.885                      0.923\n",
       "44                   0.907                      0.943\n",
       "45                   0.799                      0.893\n",
       "46                   0.908                      0.909\n",
       "47                   0.853                      0.848\n",
       "48                   0.788                      0.817\n",
       "49                   0.792                      0.838"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_mean</th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>DHDT accuracy_test_std</th>\n",
       "      <th>sklearn accuracy_test_mean</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.765612</td>\n",
       "      <td>0.834060</td>\n",
       "      <td>0.056633</td>\n",
       "      <td>0.853168</td>\n",
       "      <td>0.883340</td>\n",
       "      <td>0.022592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070386</td>\n",
       "      <td>0.067362</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>0.055008</td>\n",
       "      <td>0.050227</td>\n",
       "      <td>0.020121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.711200</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.760700</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.055658</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.074520</td>\n",
       "      <td>0.892250</td>\n",
       "      <td>0.921750</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.122117</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DHDT accuracy_test_mean  DHDT accuracy_test_max  \\\n",
       "count                50.000000               50.000000   \n",
       "mean                  0.765612                0.834060   \n",
       "std                   0.070386                0.067362   \n",
       "min                   0.642200                0.664000   \n",
       "25%                   0.711200                0.791250   \n",
       "50%                   0.760700                0.829000   \n",
       "75%                   0.826000                0.886500   \n",
       "max                   0.881200                0.957000   \n",
       "\n",
       "       DHDT accuracy_test_std  sklearn accuracy_test_mean  \\\n",
       "count               50.000000                   50.000000   \n",
       "mean                 0.056633                    0.853168   \n",
       "std                  0.030816                    0.055008   \n",
       "min                  0.007499                    0.695000   \n",
       "25%                  0.029167                    0.826800   \n",
       "50%                  0.055658                    0.856800   \n",
       "75%                  0.074520                    0.892250   \n",
       "max                  0.122117                    0.950600   \n",
       "\n",
       "       sklearn accuracy_test_max  sklearn accuracy_test_std  \n",
       "count                  50.000000                  50.000000  \n",
       "mean                    0.883340                   0.022592  \n",
       "std                     0.050227                   0.020121  \n",
       "min                     0.711000                   0.000000  \n",
       "25%                     0.855500                   0.008000  \n",
       "50%                     0.886500                   0.017600  \n",
       "75%                     0.921750                   0.034200  \n",
       "max                     0.965000                   0.080400  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    parallel_eval_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_synthetic = parallel_eval_synthetic(delayed(evaluate_synthetic_parallel)(index = index,\n",
    "                                                                                                random_seed_data = config['computation']['random_seed']+index,\n",
    "                                                                                                random_seed_model = config['computation']['random_seed'],#+random_seed_model,\n",
    "                                                                                                config = config,\n",
    "                                                                                                verbosity = -1) for index in range(config['make_classification']['num_eval']))\n",
    "\n",
    "    for i, synthetic_result in enumerate(evaluation_results_synthetic):\n",
    "        if i == 0:\n",
    "            model_dict_synthetic = synthetic_result[0]\n",
    "            scores_dict_synthetic = synthetic_result[1]\n",
    "            dataset_dict_synthetic = synthetic_result[2]\n",
    "        else: \n",
    "            model_dict_synthetic = mergeDict(model_dict_synthetic, synthetic_result[0])\n",
    "            scores_dict_synthetic = mergeDict(scores_dict_synthetic, synthetic_result[1])\n",
    "            dataset_dict_synthetic = mergeDict(dataset_dict_synthetic, synthetic_result[2])\n",
    "\n",
    "\n",
    "    metrics = ['accuracy_test']\n",
    "    index = [i for i in range(config['make_classification']['num_eval'])]\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "    scores_DHDT = [scores_dict_synthetic[i]['DHDT'][metrics[0]] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "    scores_sklearn = [scores_dict_synthetic[i]['sklearn'][metrics[0]] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "\n",
    "    scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['make_classification']['num_eval'])\n",
    "    scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['make_classification']['num_eval'])\n",
    "\n",
    "    results_DHDT = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "    results_sklearn = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "\n",
    "    scores_dataframe_synthetic = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_synthetic)\n",
    "    display(scores_dataframe_synthetic[scores_dataframe_synthetic.columns[1::3]])\n",
    "    display(scores_dataframe_synthetic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2b804-f2e3-4ec9-9c06-1334d7449519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f856c6c9-368b-4b75-b0d8-6e7a28e4267c",
   "metadata": {},
   "source": [
    "## Real-World Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb9eca0-b6be-4389-960f-857e68e6b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:29:46.127256Z",
     "iopub.status.busy": "2022-06-09T07:29:46.127086Z",
     "iopub.status.idle": "2022-06-09T07:29:46.135806Z",
     "shell.execute_reply": "2022-06-09T07:29:46.135431Z",
     "shell.execute_reply.started": "2022-06-09T07:29:46.127233Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    #identifier_list = ['Habermans Survival']\n",
    "\n",
    "    parallel_eval_real_world = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_real_world = parallel_eval_real_world(delayed(evaluate_real_world_parallel)(identifier_list=identifier_list, \n",
    "                                                                                                   random_seed_model=config['computation']['random_seed']+i,\n",
    "                                                                                                   config = config,\n",
    "                                                                                                   verbosity = -1) for i in range(config['computation']['trials']))\n",
    "\n",
    "\n",
    "    for i, real_world_result in enumerate(evaluation_results_real_world):\n",
    "        if i == 0:\n",
    "            model_dict_real_world = real_world_result[0]\n",
    "            scores_dict_real_world = real_world_result[1]\n",
    "            dataset_dict_real_world = real_world_result[2]\n",
    "        else: \n",
    "            model_dict_real_world = mergeDict(model_dict_real_world, real_world_result[0])\n",
    "            scores_dict_real_world = mergeDict(scores_dict_real_world, real_world_result[1])\n",
    "            dataset_dict_real_world = mergeDict(dataset_dict_real_world, real_world_result[2])\n",
    "\n",
    "    metrics = ['accuracy_test']\n",
    "    index = identifier_list\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "    scores_DHDT = [scores_dict_real_world[identifier]['DHDT'][metrics[0]] for identifier in identifier_list]\n",
    "\n",
    "    scores_sklearn = [scores_dict_real_world[identifier]['sklearn'][metrics[0]] for identifier in identifier_list]\n",
    "\n",
    "    scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "    scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "    results_DHDT = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "    results_sklearn = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "\n",
    "    scores_dataframe_real_world = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_real_world)\n",
    "    display(scores_dataframe_real_world[scores_dataframe_real_world.columns[1::3]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c55370-5fa9-4f3c-a1a8-be597467f808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:29:46.136661Z",
     "iopub.status.busy": "2022-06-09T07:29:46.136390Z",
     "iopub.status.idle": "2022-06-09T07:29:46.183169Z",
     "shell.execute_reply": "2022-06-09T07:29:46.182805Z",
     "shell.execute_reply.started": "2022-06-09T07:29:46.136640Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier = identifier_list[0]#\"Absenteeism\"\n",
    "    plt.figure(figsize=(15,8))\n",
    "    image = model_dict_real_world[identifier]['DHDT'].plot(normalizer_list=dataset_dict_real_world[identifier]['normalizer_list'])\n",
    "    display(image)\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plot_tree(model_dict_real_world[identifier]['sklearn'], fontsize=10) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fbd88-44c9-4d7f-b58d-def1eeb6d434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e73582-0f9e-4522-9a94-dc59f4e566e9",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09315b63-50a1-4b2c-bfa9-d7b6d961de34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:29:46.183873Z",
     "iopub.status.busy": "2022-06-09T07:29:46.183719Z",
     "iopub.status.idle": "2022-06-09T07:29:46.233111Z",
     "shell.execute_reply": "2022-06-09T07:29:46.232409Z",
     "shell.execute_reply.started": "2022-06-09T07:29:46.183858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "        'depth': [3],\n",
    "        'learning_rate': [0.05, 0.01, 0.005, 0.001, 0.0005],#[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        \n",
    "        'loss': ['binary_crossentropy'],#['binary_crossentropy', 'rmse'],#'mae',\n",
    "        'optimizer': ['adam', 'sgd'],#['adam', 'sgd'],        \n",
    "        \n",
    "        'beta_1': [10, 50],#[10, 50, 100],\n",
    "        'beta_2': [10, 50],#[10, 50, 100],\n",
    "        \n",
    "        'squeeze_factor': [0.2, 0.5, 1, 2, 5],#[0.2, 0.5, 1, 2, 5],    \n",
    "}\n",
    "\n",
    "parameter_dict = {\n",
    "        #'depth': [3],\n",
    "        #'learning_rate': [0.05, 0.01, 0.005, 0.001, 0.0005],#[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        \n",
    "        'loss': ['binary_crossentropy'],#['binary_crossentropy', 'rmse'],#'mae',\n",
    "        'optimizer': ['adam', 'sgd'],#['adam', 'sgd'],        \n",
    "        \n",
    "        #'beta_1': [10, 50],#[10, 50, 100],\n",
    "        #'beta_2': [10, 50],#[10, 50, 100],\n",
    "        \n",
    "        #'squeeze_factor': [0.2, 0.5, 1, 2, 5],#[0.2, 0.5, 1, 2, 5],    \n",
    "}\n",
    "\n",
    "parameter_grid = ParameterGrid(parameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952be342-9751-474b-8780-f185b3f9c29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:29:46.235234Z",
     "iopub.status.busy": "2022-06-09T07:29:46.234658Z",
     "iopub.status.idle": "2022-06-09T07:29:46.311592Z",
     "shell.execute_reply": "2022-06-09T07:29:46.310670Z",
     "shell.execute_reply.started": "2022-06-09T07:29:46.235200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    parallel_hpo_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_hpo_synthetic = parallel_hpo_synthetic(delayed(evaluate_parameter_setting_synthetic)(parameter_setting) for parameter_setting in parameter_grid)\n",
    "\n",
    "    sorted_evaluation_results_hpo_synthetic = sorted(evaluation_results_hpo_synthetic, key=lambda x: x[0], reverse=True)\n",
    "    print(sorted_evaluation_results_hpo_synthetic[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af8512c-cb73-43ac-811a-d7d334db97ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:29:46.313120Z",
     "iopub.status.busy": "2022-06-09T07:29:46.312727Z",
     "iopub.status.idle": "2022-06-09T07:29:46.393469Z",
     "shell.execute_reply": "2022-06-09T07:29:46.392792Z",
     "shell.execute_reply.started": "2022-06-09T07:29:46.313095Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    sorted_evaluation_results_hpo_real_dict = {}\n",
    "\n",
    "    for identifier in identifier_list:\n",
    "        parallel_hpo_real = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "        evaluation_results_hpo_real = parallel_hpo_real(delayed(evaluate_parameter_setting_real_world)(parameter_setting, identifier) for parameter_setting in parameter_grid)\n",
    "\n",
    "        sorted_evaluation_results_hpo_real = sorted(evaluation_results_hpo_real, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        sorted_evaluation_results_hpo_real_dict[identifier] = sorted_evaluation_results_hpo_real\n",
    "\n",
    "        print(sorted_evaluation_results_hpo_real[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40147db1-760f-432d-ac2c-0267a8256f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358c9e7-58be-4006-b120-6b14f8364122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
