{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3b444-064b-4639-983d-238b3da68ad9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6531aaa-2c26-40f5-8078-d754aceda153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:40:46.470205Z",
     "iopub.status.busy": "2022-06-09T07:40:46.469607Z",
     "iopub.status.idle": "2022-06-09T07:40:46.482589Z",
     "shell.execute_reply": "2022-06-09T07:40:46.482000Z",
     "shell.execute_reply.started": "2022-06-09T07:40:46.470104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dhdt': {\n",
    "        'depth': 3,\n",
    "        'learning_rate': 0.01,#1e-3,\n",
    "        \n",
    "        'initializer': 'GlorotUniform', #GlorotUniform\n",
    "        \n",
    "        'loss': 'binary_crossentropy',#'mae',\n",
    "        'optimizer': 'adam',        \n",
    "        \n",
    "        'beta_1': 10,\n",
    "        'beta_2': 50,\n",
    "        \n",
    "        'squeeze_factor': 1,\n",
    "        \n",
    "        'batch_size': 512,\n",
    "        'epochs': 1_000,\n",
    "        'early_stopping_epochs': 50,\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "    'make_classification': {\n",
    "        'number_of_variables': 20,\n",
    "        'n_samples': 5_000,\n",
    "        'num_eval': 50,\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'trials': 5,\n",
    "        'n_jobs': 50,\n",
    "        'verbosity': 0,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:40:46.484230Z",
     "iopub.status.busy": "2022-06-09T07:40:46.483603Z",
     "iopub.status.idle": "2022-06-09T07:40:52.333636Z",
     "shell.execute_reply": "2022-06-09T07:40:52.332599Z",
     "shell.execute_reply.started": "2022-06-09T07:40:46.484194Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities import *\n",
    "from utilities.DHDT import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397843-32d2-4579-b197-0686f4a99b54",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e7da9-12bb-467f-919f-4fd510ce6463",
   "metadata": {},
   "source": [
    "## make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8a1baf-79d0-4959-8376-fc6a5446a229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:40:52.336351Z",
     "iopub.status.busy": "2022-06-09T07:40:52.336083Z",
     "iopub.status.idle": "2022-06-09T07:40:52.348815Z",
     "shell.execute_reply": "2022-06-09T07:40:52.348080Z",
     "shell.execute_reply.started": "2022-06-09T07:40:52.336316Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    config_test = deepcopy(config)\n",
    "    config_test['make_classification']['n_samples'] = 10_000\n",
    "    config_test['dhdt']['epochs'] = 3\n",
    "\n",
    "    dataset_dict = {}\n",
    "    model_dict = {}\n",
    "\n",
    "    scores_dict = {'sklearn': {},\n",
    "                   'DHDT': {}}\n",
    "\n",
    "    dataset_dict = get_preprocessed_dataset('make_classification',\n",
    "                                            random_seed=config_test['computation']['random_seed'],\n",
    "                                            config=config_test['make_classification'],\n",
    "                                            verbosity=1)\n",
    "\n",
    "    model_dict['sklearn'] = DecisionTreeClassifier(max_depth=3, \n",
    "                                                   random_state=config_test['computation']['random_seed'])\n",
    "\n",
    "    model_dict['sklearn'].fit(dataset_dict['X_train'], \n",
    "                              dataset_dict['y_train'])\n",
    "\n",
    "\n",
    "\n",
    "    model_dict['DHDT'] = DHDT(dataset_dict['X_train'].shape[1],\n",
    "\n",
    "                                depth = config_test['dhdt']['depth'],\n",
    "\n",
    "                                learning_rate = config_test['dhdt']['learning_rate'],\n",
    "                                optimizer = config_test['dhdt']['optimizer'],\n",
    "\n",
    "                                beta_1 = config_test['dhdt']['beta_1'],\n",
    "                                beta_2 = config_test['dhdt']['beta_2'],\n",
    "\n",
    "                                squeeze_factor = config_test['dhdt']['squeeze_factor'],\n",
    "\n",
    "                                loss = config_test['dhdt']['loss'],#'mae',\n",
    "\n",
    "                                random_seed = config_test['computation']['random_seed'],\n",
    "                                verbosity = 2)        \n",
    "\n",
    "\n",
    "    scores_dict['history'] = model_dict['DHDT'].fit(dataset_dict['X_train'], \n",
    "                                                  dataset_dict['y_train'], \n",
    "                                                  batch_size=config_test['dhdt']['batch_size'], \n",
    "                                                  epochs=config_test['dhdt']['epochs'], \n",
    "                                                  early_stopping_epochs=config_test['dhdt']['early_stopping_epochs'], \n",
    "                                                  valid_data=(dataset_dict['X_valid'], dataset_dict['y_valid']))\n",
    "\n",
    "\n",
    "\n",
    "    scores_dict['sklearn']['accuracy_test'] = model_dict['sklearn'].score(dataset_dict['X_test'], \n",
    "                                                                     dataset_dict['y_test'])\n",
    "\n",
    "\n",
    "    dataset_dict['y_test_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_test'])\n",
    "    scores_dict['DHDT']['accuracy_test'] = accuracy_score(dataset_dict['y_test'], np.round(dataset_dict['y_test_dhdt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a13b2a-ca9d-44de-9a52-b8ca61808093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:40:52.350443Z",
     "iopub.status.busy": "2022-06-09T07:40:52.350086Z",
     "iopub.status.idle": "2022-06-09T07:48:06.925047Z",
     "shell.execute_reply": "2022-06-09T07:48:06.924401Z",
     "shell.execute_reply.started": "2022-06-09T07:40:52.350407Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of  50 | elapsed:  5.3min remaining: 127.0min\n",
      "[Parallel(n_jobs=50)]: Done  19 out of  50 | elapsed:  5.9min remaining:  9.7min\n",
      "[Parallel(n_jobs=50)]: Done  36 out of  50 | elapsed:  6.2min remaining:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  50 out of  50 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_mean</th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>DHDT accuracy_test_std</th>\n",
       "      <th>sklearn accuracy_test_mean</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.6432</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.022186</td>\n",
       "      <td>0.6402</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6088</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>0.7746</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.062484</td>\n",
       "      <td>0.7758</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.0464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6332</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.038520</td>\n",
       "      <td>0.6818</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.0436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5626</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6518</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.053349</td>\n",
       "      <td>0.7238</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5830</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.050220</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.0380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.7014</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.0432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6386</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.7066</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.025526</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5400</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.6172</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.0104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5506</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.0280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.0760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.046006</td>\n",
       "      <td>0.8038</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.0180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6302</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>0.7284</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6536</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.030781</td>\n",
       "      <td>0.7312</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.7014</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.026066</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.0124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.0524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.045839</td>\n",
       "      <td>0.6214</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.018595</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5794</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.037366</td>\n",
       "      <td>0.6758</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.0056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.6568</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.049390</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5934</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.6596</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.0332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.024260</td>\n",
       "      <td>0.6142</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5860</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>0.6416</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.0732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.6138</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.020331</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.6544</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.050090</td>\n",
       "      <td>0.7722</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.0064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.032206</td>\n",
       "      <td>0.6614</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.6488</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.6828</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.6238</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.066716</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.0520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.5590</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.0116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.6596</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.7614</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.6326</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.059308</td>\n",
       "      <td>0.7488</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.6146</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.0472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.067504</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.0756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.6506</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.036751</td>\n",
       "      <td>0.7290</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.6336</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.060889</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.0380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.6316</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.5732</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.054730</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.7096</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.078066</td>\n",
       "      <td>0.8060</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.041726</td>\n",
       "      <td>0.7510</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.6766</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.050178</td>\n",
       "      <td>0.7462</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.0416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.5902</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.019833</td>\n",
       "      <td>0.6890</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_test_mean  DHDT accuracy_test_max  DHDT accuracy_test_std  \\\n",
       "0                    0.5724                   0.669                0.058332   \n",
       "1                    0.5776                   0.610                0.022186   \n",
       "2                    0.6088                   0.647                0.040241   \n",
       "3                    0.6656                   0.717                0.062484   \n",
       "4                    0.6332                   0.704                0.038520   \n",
       "5                    0.5626                   0.615                0.038370   \n",
       "6                    0.6518                   0.699                0.053349   \n",
       "7                    0.5830                   0.675                0.050220   \n",
       "8                    0.6894                   0.754                0.066159   \n",
       "9                    0.6264                   0.647                0.021978   \n",
       "10                   0.6386                   0.683                0.039072   \n",
       "11                   0.5810                   0.617                0.025526   \n",
       "12                   0.5400                   0.600                0.031793   \n",
       "13                   0.5506                   0.568                0.018885   \n",
       "14                   0.5318                   0.563                0.025270   \n",
       "15                   0.6212                   0.675                0.046006   \n",
       "16                   0.6502                   0.704                0.032609   \n",
       "17                   0.6302                   0.652                0.019009   \n",
       "18                   0.6536                   0.696                0.030781   \n",
       "19                   0.7014                   0.746                0.026066   \n",
       "20                   0.6680                   0.749                0.049449   \n",
       "21                   0.5760                   0.640                0.045839   \n",
       "22                   0.5952                   0.617                0.018595   \n",
       "23                   0.5794                   0.629                0.037366   \n",
       "24                   0.5680                   0.602                0.028865   \n",
       "25                   0.6568                   0.715                0.049390   \n",
       "26                   0.5668                   0.594                0.016005   \n",
       "27                   0.5934                   0.618                0.019469   \n",
       "28                   0.5428                   0.590                0.024260   \n",
       "29                   0.5860                   0.628                0.026683   \n",
       "30                   0.6138                   0.652                0.020331   \n",
       "31                   0.6544                   0.705                0.050090   \n",
       "32                   0.5950                   0.634                0.032206   \n",
       "33                   0.6488                   0.688                0.025055   \n",
       "34                   0.5936                   0.639                0.027746   \n",
       "35                   0.5742                   0.664                0.055058   \n",
       "36                   0.6238                   0.711                0.066716   \n",
       "37                   0.5590                   0.651                0.062498   \n",
       "38                   0.6596                   0.715                0.044383   \n",
       "39                   0.6326                   0.696                0.059308   \n",
       "40                   0.6146                   0.693                0.041884   \n",
       "41                   0.6940                   0.757                0.067504   \n",
       "42                   0.6506                   0.708                0.036751   \n",
       "43                   0.6336                   0.722                0.060889   \n",
       "44                   0.5818                   0.603                0.013556   \n",
       "45                   0.5732                   0.682                0.054730   \n",
       "46                   0.7096                   0.817                0.078066   \n",
       "47                   0.6354                   0.681                0.041726   \n",
       "48                   0.6766                   0.724                0.050178   \n",
       "49                   0.5902                   0.625                0.019833   \n",
       "\n",
       "    sklearn accuracy_test_mean  sklearn accuracy_test_max  \\\n",
       "0                       0.6432                      0.752   \n",
       "1                       0.6402                      0.641   \n",
       "2                       0.7746                      0.775   \n",
       "3                       0.7758                      0.799   \n",
       "4                       0.6818                      0.769   \n",
       "5                       0.6560                      0.660   \n",
       "6                       0.7238                      0.731   \n",
       "7                       0.6060                      0.738   \n",
       "8                       0.7710                      0.790   \n",
       "9                       0.7014                      0.723   \n",
       "10                      0.7066                      0.721   \n",
       "11                      0.6754                      0.678   \n",
       "12                      0.6172                      0.638   \n",
       "13                      0.6160                      0.672   \n",
       "14                      0.5840                      0.736   \n",
       "15                      0.8038                      0.811   \n",
       "16                      0.7500                      0.786   \n",
       "17                      0.7284                      0.730   \n",
       "18                      0.7312                      0.743   \n",
       "19                      0.7718                      0.778   \n",
       "20                      0.7538                      0.780   \n",
       "21                      0.6214                      0.623   \n",
       "22                      0.6252                      0.630   \n",
       "23                      0.6758                      0.687   \n",
       "24                      0.6304                      0.635   \n",
       "25                      0.7900                      0.793   \n",
       "26                      0.6192                      0.624   \n",
       "27                      0.6596                      0.726   \n",
       "28                      0.6142                      0.659   \n",
       "29                      0.6416                      0.788   \n",
       "30                      0.6844                      0.694   \n",
       "31                      0.7722                      0.785   \n",
       "32                      0.6614                      0.671   \n",
       "33                      0.7130                      0.717   \n",
       "34                      0.6634                      0.679   \n",
       "35                      0.6828                      0.687   \n",
       "36                      0.7350                      0.761   \n",
       "37                      0.7422                      0.748   \n",
       "38                      0.7614                      0.779   \n",
       "39                      0.7488                      0.776   \n",
       "40                      0.7156                      0.810   \n",
       "41                      0.7892                      0.827   \n",
       "42                      0.7290                      0.769   \n",
       "43                      0.7380                      0.814   \n",
       "44                      0.6316                      0.634   \n",
       "45                      0.6534                      0.743   \n",
       "46                      0.8060                      0.826   \n",
       "47                      0.7510                      0.795   \n",
       "48                      0.7462                      0.767   \n",
       "49                      0.6890                      0.690   \n",
       "\n",
       "    sklearn accuracy_test_std  \n",
       "0                      0.0544  \n",
       "1                      0.0004  \n",
       "2                      0.0008  \n",
       "3                      0.0464  \n",
       "4                      0.0436  \n",
       "5                      0.0080  \n",
       "6                      0.0144  \n",
       "7                      0.0660  \n",
       "8                      0.0380  \n",
       "9                      0.0432  \n",
       "10                     0.0072  \n",
       "11                     0.0052  \n",
       "12                     0.0104  \n",
       "13                     0.0280  \n",
       "14                     0.0760  \n",
       "15                     0.0144  \n",
       "16                     0.0180  \n",
       "17                     0.0008  \n",
       "18                     0.0236  \n",
       "19                     0.0124  \n",
       "20                     0.0524  \n",
       "21                     0.0032  \n",
       "22                     0.0024  \n",
       "23                     0.0056  \n",
       "24                     0.0092  \n",
       "25                     0.0060  \n",
       "26                     0.0096  \n",
       "27                     0.0332  \n",
       "28                     0.0224  \n",
       "29                     0.0732  \n",
       "30                     0.0192  \n",
       "31                     0.0064  \n",
       "32                     0.0192  \n",
       "33                     0.0020  \n",
       "34                     0.0312  \n",
       "35                     0.0084  \n",
       "36                     0.0520  \n",
       "37                     0.0116  \n",
       "38                     0.0352  \n",
       "39                     0.0544  \n",
       "40                     0.0472  \n",
       "41                     0.0756  \n",
       "42                     0.0200  \n",
       "43                     0.0380  \n",
       "44                     0.0012  \n",
       "45                     0.0448  \n",
       "46                     0.0100  \n",
       "47                     0.0220  \n",
       "48                     0.0416  \n",
       "49                     0.0020  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.669</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.704</td>\n",
       "      <td>0.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.615</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.683</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.568</td>\n",
       "      <td>0.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.563</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.704</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.652</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.629</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.602</td>\n",
       "      <td>0.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.594</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.628</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.652</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.688</td>\n",
       "      <td>0.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.711</td>\n",
       "      <td>0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.722</td>\n",
       "      <td>0.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.603</td>\n",
       "      <td>0.634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.682</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.817</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.681</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.724</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_test_max  sklearn accuracy_test_max\n",
       "0                    0.669                      0.752\n",
       "1                    0.610                      0.641\n",
       "2                    0.647                      0.775\n",
       "3                    0.717                      0.799\n",
       "4                    0.704                      0.769\n",
       "5                    0.615                      0.660\n",
       "6                    0.699                      0.731\n",
       "7                    0.675                      0.738\n",
       "8                    0.754                      0.790\n",
       "9                    0.647                      0.723\n",
       "10                   0.683                      0.721\n",
       "11                   0.617                      0.678\n",
       "12                   0.600                      0.638\n",
       "13                   0.568                      0.672\n",
       "14                   0.563                      0.736\n",
       "15                   0.675                      0.811\n",
       "16                   0.704                      0.786\n",
       "17                   0.652                      0.730\n",
       "18                   0.696                      0.743\n",
       "19                   0.746                      0.778\n",
       "20                   0.749                      0.780\n",
       "21                   0.640                      0.623\n",
       "22                   0.617                      0.630\n",
       "23                   0.629                      0.687\n",
       "24                   0.602                      0.635\n",
       "25                   0.715                      0.793\n",
       "26                   0.594                      0.624\n",
       "27                   0.618                      0.726\n",
       "28                   0.590                      0.659\n",
       "29                   0.628                      0.788\n",
       "30                   0.652                      0.694\n",
       "31                   0.705                      0.785\n",
       "32                   0.634                      0.671\n",
       "33                   0.688                      0.717\n",
       "34                   0.639                      0.679\n",
       "35                   0.664                      0.687\n",
       "36                   0.711                      0.761\n",
       "37                   0.651                      0.748\n",
       "38                   0.715                      0.779\n",
       "39                   0.696                      0.776\n",
       "40                   0.693                      0.810\n",
       "41                   0.757                      0.827\n",
       "42                   0.708                      0.769\n",
       "43                   0.722                      0.814\n",
       "44                   0.603                      0.634\n",
       "45                   0.682                      0.743\n",
       "46                   0.817                      0.826\n",
       "47                   0.681                      0.795\n",
       "48                   0.724                      0.767\n",
       "49                   0.625                      0.690"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_test_mean</th>\n",
       "      <th>DHDT accuracy_test_max</th>\n",
       "      <th>DHDT accuracy_test_std</th>\n",
       "      <th>sklearn accuracy_test_mean</th>\n",
       "      <th>sklearn accuracy_test_max</th>\n",
       "      <th>sklearn accuracy_test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.614324</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.700060</td>\n",
       "      <td>0.731760</td>\n",
       "      <td>0.025408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.060864</td>\n",
       "      <td>0.060414</td>\n",
       "      <td>0.021950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.578050</td>\n",
       "      <td>0.625750</td>\n",
       "      <td>0.025334</td>\n",
       "      <td>0.645750</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.614200</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.704750</td>\n",
       "      <td>0.050209</td>\n",
       "      <td>0.749700</td>\n",
       "      <td>0.779750</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.709600</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.078066</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DHDT accuracy_test_mean  DHDT accuracy_test_max  \\\n",
       "count                50.000000               50.000000   \n",
       "mean                  0.614324                0.667800   \n",
       "std                   0.045126                0.054173   \n",
       "min                   0.531800                0.563000   \n",
       "25%                   0.578050                0.625750   \n",
       "50%                   0.614200                0.672000   \n",
       "75%                   0.650500                0.704750   \n",
       "max                   0.709600                0.817000   \n",
       "\n",
       "       DHDT accuracy_test_std  sklearn accuracy_test_mean  \\\n",
       "count               50.000000                   50.000000   \n",
       "mean                 0.039426                    0.700060   \n",
       "std                  0.016450                    0.060864   \n",
       "min                  0.013556                    0.584000   \n",
       "25%                  0.025334                    0.645750   \n",
       "50%                  0.038445                    0.704000   \n",
       "75%                  0.050209                    0.749700   \n",
       "max                  0.078066                    0.806000   \n",
       "\n",
       "       sklearn accuracy_test_max  sklearn accuracy_test_std  \n",
       "count                  50.000000                  50.000000  \n",
       "mean                    0.731760                   0.025408  \n",
       "std                     0.060414                   0.021950  \n",
       "min                     0.623000                   0.000400  \n",
       "25%                     0.681000                   0.007400  \n",
       "50%                     0.740500                   0.019200  \n",
       "75%                     0.779750                   0.042800  \n",
       "max                     0.827000                   0.076000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    parallel_eval_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_synthetic = parallel_eval_synthetic(delayed(evaluate_synthetic_parallel)(index = index,\n",
    "                                                                                                random_seed_data = config['computation']['random_seed']+index,\n",
    "                                                                                                random_seed_model = config['computation']['random_seed'],#+random_seed_model,\n",
    "                                                                                                config = config,\n",
    "                                                                                                verbosity = -1) for index in range(config['make_classification']['num_eval']))\n",
    "\n",
    "    for i, synthetic_result in enumerate(evaluation_results_synthetic):\n",
    "        if i == 0:\n",
    "            model_dict_synthetic = synthetic_result[0]\n",
    "            scores_dict_synthetic = synthetic_result[1]\n",
    "            dataset_dict_synthetic = synthetic_result[2]\n",
    "        else: \n",
    "            model_dict_synthetic = mergeDict(model_dict_synthetic, synthetic_result[0])\n",
    "            scores_dict_synthetic = mergeDict(scores_dict_synthetic, synthetic_result[1])\n",
    "            dataset_dict_synthetic = mergeDict(dataset_dict_synthetic, synthetic_result[2])\n",
    "\n",
    "\n",
    "    metrics = ['accuracy_test']\n",
    "    index = [i for i in range(config['make_classification']['num_eval'])]\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "    scores_DHDT = [scores_dict_synthetic[i]['DHDT'][metrics[0]] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "    scores_sklearn = [scores_dict_synthetic[i]['sklearn'][metrics[0]] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "\n",
    "    scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['make_classification']['num_eval'])\n",
    "    scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['make_classification']['num_eval'])\n",
    "\n",
    "    results_DHDT = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "    results_sklearn = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "\n",
    "    scores_dataframe_synthetic = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_synthetic)\n",
    "    display(scores_dataframe_synthetic[scores_dataframe_synthetic.columns[1::3]])\n",
    "    display(scores_dataframe_synthetic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2b804-f2e3-4ec9-9c06-1334d7449519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f856c6c9-368b-4b75-b0d8-6e7a28e4267c",
   "metadata": {},
   "source": [
    "## Real-World Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb9eca0-b6be-4389-960f-857e68e6b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:48:06.926222Z",
     "iopub.status.busy": "2022-06-09T07:48:06.926075Z",
     "iopub.status.idle": "2022-06-09T07:48:06.936961Z",
     "shell.execute_reply": "2022-06-09T07:48:06.936420Z",
     "shell.execute_reply.started": "2022-06-09T07:48:06.926203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    #identifier_list = ['Habermans Survival']\n",
    "\n",
    "    parallel_eval_real_world = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_real_world = parallel_eval_real_world(delayed(evaluate_real_world_parallel)(identifier_list=identifier_list, \n",
    "                                                                                                   random_seed_model=config['computation']['random_seed']+i,\n",
    "                                                                                                   config = config,\n",
    "                                                                                                   verbosity = -1) for i in range(config['computation']['trials']))\n",
    "\n",
    "\n",
    "    for i, real_world_result in enumerate(evaluation_results_real_world):\n",
    "        if i == 0:\n",
    "            model_dict_real_world = real_world_result[0]\n",
    "            scores_dict_real_world = real_world_result[1]\n",
    "            dataset_dict_real_world = real_world_result[2]\n",
    "        else: \n",
    "            model_dict_real_world = mergeDict(model_dict_real_world, real_world_result[0])\n",
    "            scores_dict_real_world = mergeDict(scores_dict_real_world, real_world_result[1])\n",
    "            dataset_dict_real_world = mergeDict(dataset_dict_real_world, real_world_result[2])\n",
    "\n",
    "    metrics = ['accuracy_test']\n",
    "    index = identifier_list\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "    scores_DHDT = [scores_dict_real_world[identifier]['DHDT'][metrics[0]] for identifier in identifier_list]\n",
    "\n",
    "    scores_sklearn = [scores_dict_real_world[identifier]['sklearn'][metrics[0]] for identifier in identifier_list]\n",
    "\n",
    "    scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "    scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "    scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "    scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "    results_DHDT = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "    results_sklearn = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "\n",
    "    scores_dataframe_real_world = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_real_world)\n",
    "    display(scores_dataframe_real_world[scores_dataframe_real_world.columns[1::3]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c55370-5fa9-4f3c-a1a8-be597467f808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:48:06.937836Z",
     "iopub.status.busy": "2022-06-09T07:48:06.937710Z",
     "iopub.status.idle": "2022-06-09T07:48:07.001993Z",
     "shell.execute_reply": "2022-06-09T07:48:07.001398Z",
     "shell.execute_reply.started": "2022-06-09T07:48:06.937821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier = identifier_list[0]#\"Absenteeism\"\n",
    "    plt.figure(figsize=(15,8))\n",
    "    image = model_dict_real_world[identifier]['DHDT'].plot(normalizer_list=dataset_dict_real_world[identifier]['normalizer_list'])\n",
    "    display(image)\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plot_tree(model_dict_real_world[identifier]['sklearn'], fontsize=10) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fbd88-44c9-4d7f-b58d-def1eeb6d434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e73582-0f9e-4522-9a94-dc59f4e566e9",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09315b63-50a1-4b2c-bfa9-d7b6d961de34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:48:07.002813Z",
     "iopub.status.busy": "2022-06-09T07:48:07.002686Z",
     "iopub.status.idle": "2022-06-09T07:48:07.085896Z",
     "shell.execute_reply": "2022-06-09T07:48:07.085278Z",
     "shell.execute_reply.started": "2022-06-09T07:48:07.002791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "        'depth': [3],\n",
    "        'learning_rate': [0.05, 0.01, 0.005, 0.001, 0.0005],#[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        \n",
    "        'loss': ['binary_crossentropy'],#['binary_crossentropy', 'rmse'],#'mae',\n",
    "        'optimizer': ['adam', 'sgd'],#['adam', 'sgd'],        \n",
    "        \n",
    "        'beta_1': [10, 50],#[10, 50, 100],\n",
    "        'beta_2': [10, 50],#[10, 50, 100],\n",
    "        \n",
    "        'squeeze_factor': [0.2, 0.5, 1, 2, 5],#[0.2, 0.5, 1, 2, 5],    \n",
    "}\n",
    "\n",
    "parameter_dict = {\n",
    "        #'depth': [3],\n",
    "        #'learning_rate': [0.05, 0.01, 0.005, 0.001, 0.0005],#[0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "        \n",
    "        'loss': ['binary_crossentropy'],#['binary_crossentropy', 'rmse'],#'mae',\n",
    "        'optimizer': ['adam', 'sgd'],#['adam', 'sgd'],        \n",
    "        \n",
    "        #'beta_1': [10, 50],#[10, 50, 100],\n",
    "        #'beta_2': [10, 50],#[10, 50, 100],\n",
    "        \n",
    "        #'squeeze_factor': [0.2, 0.5, 1, 2, 5],#[0.2, 0.5, 1, 2, 5],    \n",
    "}\n",
    "\n",
    "parameter_grid = ParameterGrid(parameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952be342-9751-474b-8780-f185b3f9c29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:48:07.086832Z",
     "iopub.status.busy": "2022-06-09T07:48:07.086702Z",
     "iopub.status.idle": "2022-06-09T07:48:07.117259Z",
     "shell.execute_reply": "2022-06-09T07:48:07.116676Z",
     "shell.execute_reply.started": "2022-06-09T07:48:07.086817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    parallel_hpo_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_hpo_synthetic = parallel_hpo_synthetic(delayed(evaluate_parameter_setting_synthetic)(parameter_setting) for parameter_setting in parameter_grid)\n",
    "\n",
    "    sorted_evaluation_results_hpo_synthetic = sorted(evaluation_results_hpo_synthetic, key=lambda x: x[0], reverse=True)\n",
    "    print(sorted_evaluation_results_hpo_synthetic[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af8512c-cb73-43ac-811a-d7d334db97ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T07:48:07.118177Z",
     "iopub.status.busy": "2022-06-09T07:48:07.118052Z",
     "iopub.status.idle": "2022-06-09T07:48:07.150914Z",
     "shell.execute_reply": "2022-06-09T07:48:07.150387Z",
     "shell.execute_reply.started": "2022-06-09T07:48:07.118161Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    sorted_evaluation_results_hpo_real_dict = {}\n",
    "\n",
    "    for identifier in identifier_list:\n",
    "        parallel_hpo_real = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "        evaluation_results_hpo_real = parallel_hpo_real(delayed(evaluate_parameter_setting_real_world)(parameter_setting, identifier) for parameter_setting in parameter_grid)\n",
    "\n",
    "        sorted_evaluation_results_hpo_real = sorted(evaluation_results_hpo_real, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        sorted_evaluation_results_hpo_real_dict[identifier] = sorted_evaluation_results_hpo_real\n",
    "\n",
    "        print(sorted_evaluation_results_hpo_real[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40147db1-760f-432d-ac2c-0267a8256f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358c9e7-58be-4006-b120-6b14f8364122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
