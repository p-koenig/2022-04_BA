{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3b444-064b-4639-983d-238b3da68ad9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6531aaa-2c26-40f5-8078-d754aceda153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:34:24.307206Z",
     "iopub.status.busy": "2022-06-09T09:34:24.307071Z",
     "iopub.status.idle": "2022-06-09T09:34:24.315013Z",
     "shell.execute_reply": "2022-06-09T09:34:24.314707Z",
     "shell.execute_reply.started": "2022-06-09T09:34:24.307165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dhdt': {\n",
    "        'depth': 3,\n",
    "        'learning_rate': 0.01,#1e-3,\n",
    "        \n",
    "        'initializer': 'he_normal', #GlorotUniform\n",
    "        \n",
    "        'loss': 'binary_crossentropy',#'mae',\n",
    "        'optimizer': 'adam',        \n",
    "        \n",
    "        'beta_1': 10,\n",
    "        'beta_2': 50,\n",
    "        \n",
    "        'squeeze_factor': 1,\n",
    "        \n",
    "        'batch_size': 512,\n",
    "        'epochs': 1_000,\n",
    "        'early_stopping_epochs': 50,\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "    'make_classification': {\n",
    "        'number_of_variables': 10,\n",
    "        'n_samples': 5_000,\n",
    "        'num_eval': 50,\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'trials': 5,\n",
    "        'n_jobs': 50,\n",
    "        'verbosity': 0,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:34:24.316011Z",
     "iopub.status.busy": "2022-06-09T09:34:24.315783Z",
     "iopub.status.idle": "2022-06-09T09:34:29.769255Z",
     "shell.execute_reply": "2022-06-09T09:34:29.768477Z",
     "shell.execute_reply.started": "2022-06-09T09:34:24.315995Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities import *\n",
    "from utilities.DHDT import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397843-32d2-4579-b197-0686f4a99b54",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e7da9-12bb-467f-919f-4fd510ce6463",
   "metadata": {},
   "source": [
    "## make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8a1baf-79d0-4959-8376-fc6a5446a229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:34:29.770216Z",
     "iopub.status.busy": "2022-06-09T09:34:29.770085Z",
     "iopub.status.idle": "2022-06-09T09:35:13.823703Z",
     "shell.execute_reply": "2022-06-09T09:35:13.823089Z",
     "shell.execute_reply.started": "2022-06-09T09:34:29.770199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (10000, 10)\n",
      "Original Data Shape (encoded):  (10000, 10)\n",
      "Original Data Class Distribution:  4993  (true) / 5007  (false)\n",
      "(7000, 10) (7000,)\n",
      "(1000, 10) (1000,)\n",
      "(2000, 10) (2000,)\n",
      "True Ratio:  0.501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8373785d668d4a39a955bc3209d39acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.69344 |\n",
      "Epoch: 01 | Loss: 0.69307 |\n",
      "Epoch: 02 | Loss: 0.69250 |\n",
      "Epoch: 03 | Loss: 0.69021 |\n",
      "Epoch: 04 | Loss: 0.68434 |\n",
      "Epoch: 05 | Loss: 0.67505 |\n",
      "Epoch: 06 | Loss: 0.66785 |\n",
      "Epoch: 07 | Loss: 0.66171 |\n",
      "Epoch: 08 | Loss: 0.65201 |\n",
      "Epoch: 09 | Loss: 0.63913 |\n",
      "Epoch: 10 | Loss: 0.62264 |\n",
      "Epoch: 11 | Loss: 0.61084 |\n",
      "Epoch: 12 | Loss: 0.60216 |\n",
      "Epoch: 13 | Loss: 0.59694 |\n",
      "Epoch: 14 | Loss: 0.59186 |\n",
      "Epoch: 15 | Loss: 0.59031 |\n",
      "Epoch: 16 | Loss: 0.58694 |\n",
      "Epoch: 17 | Loss: 0.58578 |\n",
      "Epoch: 18 | Loss: 0.58523 |\n",
      "Epoch: 19 | Loss: 0.58461 |\n",
      "Epoch: 20 | Loss: 0.58262 |\n",
      "Epoch: 21 | Loss: 0.58175 |\n",
      "Epoch: 22 | Loss: 0.58159 |\n",
      "Epoch: 23 | Loss: 0.58235 |\n",
      "Epoch: 24 | Loss: 0.58160 |\n",
      "Epoch: 25 | Loss: 0.58071 |\n",
      "Epoch: 26 | Loss: 0.58069 |\n",
      "Epoch: 27 | Loss: 0.58045 |\n",
      "Epoch: 28 | Loss: 0.58123 |\n",
      "Epoch: 29 | Loss: 0.58123 |\n",
      "Epoch: 30 | Loss: 0.57994 |\n",
      "Epoch: 31 | Loss: 0.58070 |\n",
      "Epoch: 32 | Loss: 0.58129 |\n",
      "Epoch: 33 | Loss: 0.58151 |\n",
      "Epoch: 34 | Loss: 0.58136 |\n",
      "Epoch: 35 | Loss: 0.57969 |\n",
      "Epoch: 36 | Loss: 0.58020 |\n",
      "Epoch: 37 | Loss: 0.58008 |\n",
      "Epoch: 38 | Loss: 0.58090 |\n",
      "Epoch: 39 | Loss: 0.58087 |\n",
      "Epoch: 40 | Loss: 0.58012 |\n",
      "Epoch: 41 | Loss: 0.57987 |\n",
      "Epoch: 42 | Loss: 0.58002 |\n",
      "Epoch: 43 | Loss: 0.58018 |\n",
      "Epoch: 44 | Loss: 0.58028 |\n",
      "Epoch: 45 | Loss: 0.58097 |\n",
      "Epoch: 46 | Loss: 0.58061 |\n",
      "Epoch: 47 | Loss: 0.57984 |\n",
      "Epoch: 48 | Loss: 0.58051 |\n",
      "Epoch: 49 | Loss: 0.58026 |\n",
      "Epoch: 50 | Loss: 0.58092 |\n",
      "Epoch: 51 | Loss: 0.57968 |\n",
      "Epoch: 52 | Loss: 0.57948 |\n",
      "Epoch: 53 | Loss: 0.58050 |\n",
      "Epoch: 54 | Loss: 0.58058 |\n",
      "Epoch: 55 | Loss: 0.57984 |\n",
      "Epoch: 56 | Loss: 0.58052 |\n",
      "Epoch: 57 | Loss: 0.58000 |\n",
      "Epoch: 58 | Loss: 0.58022 |\n",
      "Epoch: 59 | Loss: 0.57932 |\n",
      "Epoch: 60 | Loss: 0.57991 |\n",
      "Epoch: 61 | Loss: 0.57979 |\n",
      "Epoch: 62 | Loss: 0.57936 |\n",
      "Epoch: 63 | Loss: 0.58087 |\n",
      "Epoch: 64 | Loss: 0.57990 |\n",
      "Epoch: 65 | Loss: 0.58028 |\n",
      "Epoch: 66 | Loss: 0.58036 |\n",
      "Epoch: 67 | Loss: 0.58063 |\n",
      "Epoch: 68 | Loss: 0.57969 |\n",
      "Epoch: 69 | Loss: 0.57976 |\n",
      "Epoch: 70 | Loss: 0.58073 |\n",
      "Epoch: 71 | Loss: 0.57972 |\n",
      "Epoch: 72 | Loss: 0.58030 |\n",
      "Epoch: 73 | Loss: 0.58014 |\n",
      "Epoch: 74 | Loss: 0.57996 |\n",
      "Epoch: 75 | Loss: 0.57944 |\n",
      "Epoch: 76 | Loss: 0.57992 |\n",
      "Epoch: 77 | Loss: 0.58093 |\n",
      "Epoch: 78 | Loss: 0.57999 |\n",
      "Epoch: 79 | Loss: 0.58039 |\n",
      "Epoch: 80 | Loss: 0.58084 |\n",
      "Epoch: 81 | Loss: 0.57865 |\n",
      "Epoch: 82 | Loss: 0.58067 |\n",
      "Epoch: 83 | Loss: 0.57962 |\n",
      "Epoch: 84 | Loss: 0.57971 |\n",
      "Epoch: 85 | Loss: 0.57974 |\n",
      "Epoch: 86 | Loss: 0.57921 |\n",
      "Epoch: 87 | Loss: 0.57859 |\n",
      "Epoch: 88 | Loss: 0.57932 |\n",
      "Epoch: 89 | Loss: 0.57981 |\n",
      "Epoch: 90 | Loss: 0.58065 |\n",
      "Epoch: 91 | Loss: 0.58007 |\n",
      "Epoch: 92 | Loss: 0.58064 |\n",
      "Epoch: 93 | Loss: 0.57950 |\n",
      "Epoch: 94 | Loss: 0.58011 |\n",
      "Epoch: 95 | Loss: 0.57959 |\n",
      "Epoch: 96 | Loss: 0.57953 |\n",
      "Epoch: 97 | Loss: 0.58018 |\n",
      "Epoch: 98 | Loss: 0.58040 |\n",
      "Epoch: 99 | Loss: 0.58088 |\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    \n",
    "    config_test = deepcopy(config)\n",
    "    config_test['make_classification']['n_samples'] = 10_000\n",
    "    config_test['dhdt']['epochs'] = 100\n",
    "\n",
    "    dataset_dict = {}\n",
    "    model_dict = {}\n",
    "\n",
    "    scores_dict = {'sklearn': {},\n",
    "                   'DHDT': {}}\n",
    "\n",
    "    dataset_dict = get_preprocessed_dataset('make_classification',\n",
    "                                            random_seed=config_test['computation']['random_seed'],\n",
    "                                            config=config_test['make_classification'],\n",
    "                                            verbosity=1)\n",
    "\n",
    "    model_dict['sklearn'] = DecisionTreeClassifier(max_depth=3, \n",
    "                                                   random_state=config_test['computation']['random_seed'])\n",
    "\n",
    "    model_dict['sklearn'].fit(dataset_dict['X_train'], \n",
    "                              dataset_dict['y_train'])\n",
    "\n",
    "\n",
    "\n",
    "    model_dict['DHDT'] = DHDT(dataset_dict['X_train'].shape[1],\n",
    "\n",
    "                                depth = config_test['dhdt']['depth'],\n",
    "\n",
    "                                learning_rate = config_test['dhdt']['learning_rate'],\n",
    "                                optimizer = config_test['dhdt']['optimizer'],\n",
    "\n",
    "                                beta_1 = config_test['dhdt']['beta_1'],\n",
    "                                beta_2 = config_test['dhdt']['beta_2'],\n",
    "\n",
    "                                squeeze_factor = config_test['dhdt']['squeeze_factor'],\n",
    "\n",
    "                                loss = config_test['dhdt']['loss'],#'mae',\n",
    "\n",
    "                                random_seed = config_test['computation']['random_seed'],\n",
    "                                verbosity = 2)        \n",
    "\n",
    "\n",
    "    scores_dict['history'] = model_dict['DHDT'].fit(dataset_dict['X_train'], \n",
    "                                                  dataset_dict['y_train'], \n",
    "                                                  batch_size=config_test['dhdt']['batch_size'], \n",
    "                                                  epochs=config_test['dhdt']['epochs'], \n",
    "                                                  early_stopping_epochs=config_test['dhdt']['early_stopping_epochs'], \n",
    "                                                  valid_data=(dataset_dict['X_valid'], dataset_dict['y_valid']))\n",
    "\n",
    "\n",
    "\n",
    "    dataset_dict['y_test_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_test'])\n",
    "    dataset_dict['y_valid_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_valid'])\n",
    "\n",
    "    dataset_dict['y_test_sklearn'] = model_dict['sklearn'].predict(dataset_dict['X_test'])\n",
    "    dataset_dict['y_valid_sklearn'] = model_dict['sklearn'].predict(dataset_dict['X_valid'])     \n",
    "    \n",
    "    for metric in metrics:\n",
    "        \n",
    "        if metric in ['accuracy', 'f1']:\n",
    "            y_test_dhdt = np.round(dataset_dict['y_test_dhdt'])\n",
    "            y_valid_dhdt = np.round(dataset_dict['y_valid_dhdt'])\n",
    "            y_test_sklearn = np.round(dataset_dict['y_test_sklearn'])\n",
    "            y_valid_sklearn = np.round(dataset_dict['y_valid_sklearn'])         \n",
    "        else:\n",
    "            y_test_dhdt = dataset_dict['y_test_dhdt']\n",
    "            y_valid_dhdt = dataset_dict['y_valid_dhdt']\n",
    "            y_test_sklearn = dataset_dict['y_test_sklearn']\n",
    "            y_valid_sklearn =    dataset_dict['y_valid_sklearn']                \n",
    "        \n",
    "        scores_dict['sklearn'][metric + '_test'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_test'], y_test_sklearn)\n",
    "        scores_dict['DHDT'][metric + '_test'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_test'], y_test_dhdt)\n",
    "\n",
    "        scores_dict['sklearn'][metric + '_valid'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_valid'], y_valid_sklearn)   \n",
    "        scores_dict['DHDT'][metric + '_valid'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_valid'], y_valid_dhdt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a13b2a-ca9d-44de-9a52-b8ca61808093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:35:13.824591Z",
     "iopub.status.busy": "2022-06-09T09:35:13.824468Z",
     "iopub.status.idle": "2022-06-09T09:41:55.719276Z",
     "shell.execute_reply": "2022-06-09T09:41:55.718863Z",
     "shell.execute_reply.started": "2022-06-09T09:35:13.824576Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of  50 | elapsed:  4.6min remaining: 111.3min\n",
      "[Parallel(n_jobs=50)]: Done  19 out of  50 | elapsed:  5.3min remaining:  8.6min\n",
      "[Parallel(n_jobs=50)]: Done  36 out of  50 | elapsed:  5.8min remaining:  2.2min\n",
      "[Parallel(n_jobs=50)]: Done  50 out of  50 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.683065</td>\n",
       "      <td>0.720077</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.7782</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.758970</td>\n",
       "      <td>0.845624</td>\n",
       "      <td>0.043327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7418</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.722924</td>\n",
       "      <td>0.826211</td>\n",
       "      <td>0.109143</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.833075</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.047050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7178</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.026195</td>\n",
       "      <td>0.709361</td>\n",
       "      <td>0.736067</td>\n",
       "      <td>0.032590</td>\n",
       "      <td>0.8030</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.818805</td>\n",
       "      <td>0.011556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7476</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.021191</td>\n",
       "      <td>0.738520</td>\n",
       "      <td>0.773784</td>\n",
       "      <td>0.024413</td>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.790421</td>\n",
       "      <td>0.862819</td>\n",
       "      <td>0.036199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.056191</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.697626</td>\n",
       "      <td>0.143660</td>\n",
       "      <td>0.6818</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.699457</td>\n",
       "      <td>0.828083</td>\n",
       "      <td>0.064313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7664</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.047861</td>\n",
       "      <td>0.754428</td>\n",
       "      <td>0.803828</td>\n",
       "      <td>0.071263</td>\n",
       "      <td>0.8292</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.834094</td>\n",
       "      <td>0.844358</td>\n",
       "      <td>0.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7106</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.033073</td>\n",
       "      <td>0.734459</td>\n",
       "      <td>0.752036</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.799261</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.008778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7586</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.039510</td>\n",
       "      <td>0.743214</td>\n",
       "      <td>0.805854</td>\n",
       "      <td>0.037219</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.836330</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8348</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.060970</td>\n",
       "      <td>0.827023</td>\n",
       "      <td>0.865580</td>\n",
       "      <td>0.066096</td>\n",
       "      <td>0.8484</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.855181</td>\n",
       "      <td>0.876279</td>\n",
       "      <td>0.042195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6924</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.032364</td>\n",
       "      <td>0.644781</td>\n",
       "      <td>0.766076</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.799897</td>\n",
       "      <td>0.838894</td>\n",
       "      <td>0.019499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6006</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.079414</td>\n",
       "      <td>0.513540</td>\n",
       "      <td>0.690224</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.6994</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.674231</td>\n",
       "      <td>0.788274</td>\n",
       "      <td>0.057021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.103779</td>\n",
       "      <td>0.745813</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.848223</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>0.023399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.031375</td>\n",
       "      <td>0.653451</td>\n",
       "      <td>0.734545</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.767800</td>\n",
       "      <td>0.788390</td>\n",
       "      <td>0.010295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.077682</td>\n",
       "      <td>0.733630</td>\n",
       "      <td>0.792527</td>\n",
       "      <td>0.049954</td>\n",
       "      <td>0.8010</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.806637</td>\n",
       "      <td>0.817574</td>\n",
       "      <td>0.005469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.6152</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>0.633470</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.035274</td>\n",
       "      <td>0.6604</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.712485</td>\n",
       "      <td>0.768233</td>\n",
       "      <td>0.027874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.7126</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.681126</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.8156</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.818071</td>\n",
       "      <td>0.885655</td>\n",
       "      <td>0.033792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.6374</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.030975</td>\n",
       "      <td>0.631369</td>\n",
       "      <td>0.685832</td>\n",
       "      <td>0.039889</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.795891</td>\n",
       "      <td>0.803980</td>\n",
       "      <td>0.016179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.072715</td>\n",
       "      <td>0.767950</td>\n",
       "      <td>0.865616</td>\n",
       "      <td>0.105931</td>\n",
       "      <td>0.8362</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.835259</td>\n",
       "      <td>0.868821</td>\n",
       "      <td>0.067125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6186</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.028890</td>\n",
       "      <td>0.585562</td>\n",
       "      <td>0.718115</td>\n",
       "      <td>0.091167</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.765342</td>\n",
       "      <td>0.772036</td>\n",
       "      <td>0.003347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.7682</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.748371</td>\n",
       "      <td>0.803859</td>\n",
       "      <td>0.081780</td>\n",
       "      <td>0.8324</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.829874</td>\n",
       "      <td>0.830303</td>\n",
       "      <td>0.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6944</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.062628</td>\n",
       "      <td>0.705774</td>\n",
       "      <td>0.782301</td>\n",
       "      <td>0.102143</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.826788</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.009751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.6666</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.053887</td>\n",
       "      <td>0.638564</td>\n",
       "      <td>0.745690</td>\n",
       "      <td>0.076127</td>\n",
       "      <td>0.7634</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.754074</td>\n",
       "      <td>0.771398</td>\n",
       "      <td>0.008662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.6248</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.055134</td>\n",
       "      <td>0.583067</td>\n",
       "      <td>0.698292</td>\n",
       "      <td>0.118209</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.719984</td>\n",
       "      <td>0.726066</td>\n",
       "      <td>0.012166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.097652</td>\n",
       "      <td>0.566519</td>\n",
       "      <td>0.747338</td>\n",
       "      <td>0.288036</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.769067</td>\n",
       "      <td>0.789523</td>\n",
       "      <td>0.040912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5990</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.038601</td>\n",
       "      <td>0.583198</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.101555</td>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.737229</td>\n",
       "      <td>0.748998</td>\n",
       "      <td>0.023537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.617056</td>\n",
       "      <td>0.725225</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.765450</td>\n",
       "      <td>0.771282</td>\n",
       "      <td>0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.7252</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.032621</td>\n",
       "      <td>0.711574</td>\n",
       "      <td>0.768159</td>\n",
       "      <td>0.037542</td>\n",
       "      <td>0.7872</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.791263</td>\n",
       "      <td>0.796009</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.6802</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.659434</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.073353</td>\n",
       "      <td>0.7466</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.709010</td>\n",
       "      <td>0.719317</td>\n",
       "      <td>0.005154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.6526</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.049431</td>\n",
       "      <td>0.694266</td>\n",
       "      <td>0.777480</td>\n",
       "      <td>0.065070</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.812312</td>\n",
       "      <td>0.817337</td>\n",
       "      <td>0.010052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>0.525782</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>0.102991</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.043357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.712614</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.063352</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.816462</td>\n",
       "      <td>0.836946</td>\n",
       "      <td>0.040969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.044772</td>\n",
       "      <td>0.611016</td>\n",
       "      <td>0.672018</td>\n",
       "      <td>0.044216</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.589275</td>\n",
       "      <td>0.758242</td>\n",
       "      <td>0.084483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.6894</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.084120</td>\n",
       "      <td>0.632296</td>\n",
       "      <td>0.764643</td>\n",
       "      <td>0.145742</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.777901</td>\n",
       "      <td>0.806202</td>\n",
       "      <td>0.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.7186</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.090343</td>\n",
       "      <td>0.650690</td>\n",
       "      <td>0.806030</td>\n",
       "      <td>0.151811</td>\n",
       "      <td>0.8262</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.821255</td>\n",
       "      <td>0.836864</td>\n",
       "      <td>0.031219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.7386</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>0.721635</td>\n",
       "      <td>0.764520</td>\n",
       "      <td>0.055645</td>\n",
       "      <td>0.7970</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.791391</td>\n",
       "      <td>0.847235</td>\n",
       "      <td>0.027922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>0.691718</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.742938</td>\n",
       "      <td>0.843323</td>\n",
       "      <td>0.050192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.070038</td>\n",
       "      <td>0.690854</td>\n",
       "      <td>0.771167</td>\n",
       "      <td>0.099671</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.823881</td>\n",
       "      <td>0.010953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.6716</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.024598</td>\n",
       "      <td>0.615754</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.046983</td>\n",
       "      <td>0.7614</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.0572</td>\n",
       "      <td>0.773117</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.045999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.6382</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.077246</td>\n",
       "      <td>0.572575</td>\n",
       "      <td>0.787585</td>\n",
       "      <td>0.179683</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.730081</td>\n",
       "      <td>0.830800</td>\n",
       "      <td>0.050360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.622041</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.072227</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.748864</td>\n",
       "      <td>0.751790</td>\n",
       "      <td>0.005852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.6268</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.060905</td>\n",
       "      <td>0.539201</td>\n",
       "      <td>0.714829</td>\n",
       "      <td>0.158088</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.771133</td>\n",
       "      <td>0.814176</td>\n",
       "      <td>0.021521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.042166</td>\n",
       "      <td>0.660562</td>\n",
       "      <td>0.754617</td>\n",
       "      <td>0.056578</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.844259</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>0.015630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.7616</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.078980</td>\n",
       "      <td>0.738670</td>\n",
       "      <td>0.845261</td>\n",
       "      <td>0.123730</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.813330</td>\n",
       "      <td>0.823045</td>\n",
       "      <td>0.019431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.6928</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.671070</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.797779</td>\n",
       "      <td>0.833188</td>\n",
       "      <td>0.017704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.053361</td>\n",
       "      <td>0.598191</td>\n",
       "      <td>0.738337</td>\n",
       "      <td>0.105103</td>\n",
       "      <td>0.7242</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.729806</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.007055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6772</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.061985</td>\n",
       "      <td>0.659261</td>\n",
       "      <td>0.760296</td>\n",
       "      <td>0.082782</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.784479</td>\n",
       "      <td>0.848542</td>\n",
       "      <td>0.032032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.028849</td>\n",
       "      <td>0.726197</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.772648</td>\n",
       "      <td>0.794987</td>\n",
       "      <td>0.011169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.5838</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.052082</td>\n",
       "      <td>0.490955</td>\n",
       "      <td>0.639155</td>\n",
       "      <td>0.219953</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.728568</td>\n",
       "      <td>0.742961</td>\n",
       "      <td>0.028786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.6710</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.032631</td>\n",
       "      <td>0.654674</td>\n",
       "      <td>0.695918</td>\n",
       "      <td>0.030854</td>\n",
       "      <td>0.8124</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>0.818689</td>\n",
       "      <td>0.841584</td>\n",
       "      <td>0.045791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.672049</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.069076</td>\n",
       "      <td>0.6626</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.676540</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.018349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  DHDT f1_mean  \\\n",
       "0               0.6976              0.726           0.030689      0.683065   \n",
       "1               0.7418              0.817           0.079300      0.722924   \n",
       "2               0.7178              0.749           0.026195      0.709361   \n",
       "3               0.7476              0.786           0.021191      0.738520   \n",
       "4               0.5994              0.707           0.056191      0.551282   \n",
       "5               0.7664              0.802           0.047861      0.754428   \n",
       "6               0.7106              0.764           0.033073      0.734459   \n",
       "7               0.7586              0.801           0.039510      0.743214   \n",
       "8               0.8348              0.868           0.060970      0.827023   \n",
       "9               0.6924              0.749           0.032364      0.644781   \n",
       "10              0.6006              0.737           0.079414      0.513540   \n",
       "11              0.7220              0.829           0.103779      0.745813   \n",
       "12              0.6600              0.708           0.031375      0.653451   \n",
       "13              0.7068              0.789           0.077682      0.733630   \n",
       "14              0.6152              0.660           0.033325      0.633470   \n",
       "15              0.7126              0.766           0.034203      0.681126   \n",
       "16              0.6374              0.694           0.030975      0.631369   \n",
       "17              0.7814              0.855           0.072715      0.767950   \n",
       "18              0.6186              0.653           0.028890      0.585562   \n",
       "19              0.7682              0.817           0.043943      0.748371   \n",
       "20              0.6944              0.754           0.062628      0.705774   \n",
       "21              0.6666              0.764           0.053887      0.638564   \n",
       "22              0.6248              0.689           0.055134      0.583067   \n",
       "23              0.6620              0.765           0.097652      0.566519   \n",
       "24              0.5990              0.637           0.038601      0.583198   \n",
       "25              0.6446              0.698           0.031759      0.617056   \n",
       "26              0.7252              0.767           0.032621      0.711574   \n",
       "27              0.6802              0.713           0.021160      0.659434   \n",
       "28              0.6526              0.751           0.049431      0.694266   \n",
       "29              0.5900              0.616           0.023452      0.525782   \n",
       "30              0.7378              0.790           0.050586      0.712614   \n",
       "31              0.6688              0.714           0.044772      0.611016   \n",
       "32              0.6894              0.779           0.084120      0.632296   \n",
       "33              0.7186              0.807           0.090343      0.650690   \n",
       "34              0.7386              0.777           0.037781      0.721635   \n",
       "35              0.6736              0.755           0.053608      0.691718   \n",
       "36              0.7128              0.800           0.070038      0.690854   \n",
       "37              0.6716              0.701           0.024598      0.615754   \n",
       "38              0.6382              0.781           0.077246      0.572575   \n",
       "39              0.6938              0.750           0.033564      0.622041   \n",
       "40              0.6268              0.700           0.060905      0.539201   \n",
       "41              0.6900              0.744           0.042166      0.660562   \n",
       "42              0.7616              0.840           0.078980      0.738670   \n",
       "43              0.6928              0.751           0.050878      0.671070   \n",
       "44              0.6502              0.742           0.053361      0.598191   \n",
       "45              0.6772              0.773           0.061985      0.659261   \n",
       "46              0.7196              0.776           0.028849      0.726197   \n",
       "47              0.5838              0.624           0.052082      0.490955   \n",
       "48              0.6710              0.718           0.032631      0.654674   \n",
       "49              0.6494              0.694           0.034511      0.672049   \n",
       "\n",
       "    DHDT f1_max  DHDT f1_std  sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "0      0.720077     0.043618                 0.7782                 0.843   \n",
       "1      0.826211     0.109143                 0.8354                 0.849   \n",
       "2      0.736067     0.032590                 0.8030                 0.815   \n",
       "3      0.773784     0.024413                 0.8078                 0.855   \n",
       "4      0.697626     0.143660                 0.6818                 0.809   \n",
       "5      0.803828     0.071263                 0.8292                 0.840   \n",
       "6      0.752036     0.012276                 0.7714                 0.817   \n",
       "7      0.805854     0.037219                 0.8472                 0.848   \n",
       "8      0.865580     0.066096                 0.8484                 0.867   \n",
       "9      0.766076     0.068281                 0.7958                 0.831   \n",
       "10     0.690224     0.165781                 0.6994                 0.805   \n",
       "11     0.821689     0.066450                 0.8402                 0.856   \n",
       "12     0.734545     0.054008                 0.7372                 0.774   \n",
       "13     0.792527     0.049954                 0.8010                 0.809   \n",
       "14     0.666667     0.035274                 0.6604                 0.714   \n",
       "15     0.777143     0.074944                 0.8156                 0.890   \n",
       "16     0.685832     0.039889                 0.7926                 0.803   \n",
       "17     0.865616     0.105931                 0.8362                 0.862   \n",
       "18     0.718115     0.091167                 0.7406                 0.775   \n",
       "19     0.803859     0.081780                 0.8324                 0.834   \n",
       "20     0.782301     0.102143                 0.8254                 0.832   \n",
       "21     0.745690     0.076127                 0.7634                 0.789   \n",
       "22     0.698292     0.118209                 0.7086                 0.711   \n",
       "23     0.747338     0.288036                 0.7592                 0.775   \n",
       "24     0.668902     0.101555                 0.6898                 0.701   \n",
       "25     0.725225     0.115376                 0.7774                 0.779   \n",
       "26     0.768159     0.037542                 0.7872                 0.816   \n",
       "27     0.736939     0.073353                 0.7466                 0.749   \n",
       "28     0.777480     0.065070                 0.8200                 0.823   \n",
       "29     0.629396     0.102991                 0.6832                 0.752   \n",
       "30     0.785714     0.063352                 0.8110                 0.827   \n",
       "31     0.672018     0.044216                 0.7056                 0.780   \n",
       "32     0.764643     0.145742                 0.7718                 0.800   \n",
       "33     0.806030     0.151811                 0.8262                 0.846   \n",
       "34     0.764520     0.055645                 0.7970                 0.837   \n",
       "35     0.718861     0.029627                 0.7526                 0.851   \n",
       "36     0.771167     0.099671                 0.8158                 0.823   \n",
       "37     0.673203     0.046983                 0.7614                 0.790   \n",
       "38     0.787585     0.179683                 0.7402                 0.833   \n",
       "39     0.728261     0.072227                 0.7814                 0.792   \n",
       "40     0.714829     0.158088                 0.7348                 0.806   \n",
       "41     0.754617     0.056578                 0.8568                 0.880   \n",
       "42     0.845261     0.123730                 0.8200                 0.828   \n",
       "43     0.713464     0.045303                 0.7666                 0.809   \n",
       "44     0.738337     0.105103                 0.7242                 0.741   \n",
       "45     0.760296     0.082782                 0.7822                 0.839   \n",
       "46     0.740741     0.011228                 0.7822                 0.785   \n",
       "47     0.639155     0.219953                 0.7128                 0.717   \n",
       "48     0.695918     0.030854                 0.8124                 0.840   \n",
       "49     0.738462     0.069076                 0.6626                 0.670   \n",
       "\n",
       "    sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  sklearn f1_std  \n",
       "0                 0.0324         0.758970        0.845624        0.043327  \n",
       "1                 0.0272         0.833075        0.856600        0.047050  \n",
       "2                 0.0060         0.795694        0.818805        0.011556  \n",
       "3                 0.0236         0.790421        0.862819        0.036199  \n",
       "4                 0.0636         0.699457        0.828083        0.064313  \n",
       "5                 0.0216         0.834094        0.844358        0.020528  \n",
       "6                 0.0228         0.799261        0.816817        0.008778  \n",
       "7                 0.0016         0.836330        0.836820        0.000245  \n",
       "8                 0.0372         0.855181        0.876279        0.042195  \n",
       "9                 0.0176         0.799897        0.838894        0.019499  \n",
       "10                0.0528         0.674231        0.788274        0.057021  \n",
       "11                0.0316         0.848223        0.859922        0.023399  \n",
       "12                0.0184         0.767800        0.788390        0.010295  \n",
       "13                0.0040         0.806637        0.817574        0.005469  \n",
       "14                0.0268         0.712485        0.768233        0.027874  \n",
       "15                0.0372         0.818071        0.885655        0.033792  \n",
       "16                0.0208         0.795891        0.803980        0.016179  \n",
       "17                0.0516         0.835259        0.868821        0.067125  \n",
       "18                0.0172         0.765342        0.772036        0.003347  \n",
       "19                0.0008         0.829874        0.830303        0.000858  \n",
       "20                0.0132         0.826788        0.831663        0.009751  \n",
       "21                0.0128         0.754074        0.771398        0.008662  \n",
       "22                0.0048         0.719984        0.726066        0.012166  \n",
       "23                0.0316         0.769067        0.789523        0.040912  \n",
       "24                0.0056         0.737229        0.748998        0.023537  \n",
       "25                0.0008         0.765450        0.771282        0.011663  \n",
       "26                0.0144         0.791263        0.796009        0.002373  \n",
       "27                0.0048         0.709010        0.719317        0.005154  \n",
       "28                0.0060         0.812312        0.817337        0.010052  \n",
       "29                0.0344         0.661765        0.748479        0.043357  \n",
       "30                0.0320         0.816462        0.836946        0.040969  \n",
       "31                0.0372         0.589275        0.758242        0.084483  \n",
       "32                0.0564         0.777901        0.806202        0.056601  \n",
       "33                0.0396         0.821255        0.836864        0.031219  \n",
       "34                0.0200         0.791391        0.847235        0.027922  \n",
       "35                0.0492         0.742938        0.843323        0.050192  \n",
       "36                0.0036         0.801975        0.823881        0.010953  \n",
       "37                0.0572         0.773117        0.796117        0.045999  \n",
       "38                0.0464         0.730081        0.830800        0.050360  \n",
       "39                0.0212         0.748864        0.751790        0.005852  \n",
       "40                0.0356         0.771133        0.814176        0.021521  \n",
       "41                0.0116         0.844259        0.875519        0.015630  \n",
       "42                0.0160         0.813330        0.823045        0.019431  \n",
       "43                0.0212         0.797779        0.833188        0.017704  \n",
       "44                0.0084         0.729806        0.733333        0.007055  \n",
       "45                0.0284         0.784479        0.848542        0.032032  \n",
       "46                0.0056         0.772648        0.794987        0.011169  \n",
       "47                0.0084         0.728568        0.742961        0.028786  \n",
       "48                0.0552         0.818689        0.841584        0.045791  \n",
       "49                0.0148         0.676540        0.685714        0.018349  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726</td>\n",
       "      <td>0.720077</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.845624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.817</td>\n",
       "      <td>0.826211</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.736067</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.818805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.786</td>\n",
       "      <td>0.773784</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.862819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.707</td>\n",
       "      <td>0.697626</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.828083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.803828</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.844358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.752036</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.816817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.805854</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.836820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.868</td>\n",
       "      <td>0.865580</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.876279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.766076</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.838894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.690224</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.788274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.829</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.859922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.734545</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.788390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.789</td>\n",
       "      <td>0.792527</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.817574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.768233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.766</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.885655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.685832</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.803980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.855</td>\n",
       "      <td>0.865616</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.868821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.653</td>\n",
       "      <td>0.718115</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.772036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.817</td>\n",
       "      <td>0.803859</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.830303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.782301</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.831663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.745690</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.771398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.689</td>\n",
       "      <td>0.698292</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.726066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.765</td>\n",
       "      <td>0.747338</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.789523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.637</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.748998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.698</td>\n",
       "      <td>0.725225</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.771282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.767</td>\n",
       "      <td>0.768159</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.796009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.713</td>\n",
       "      <td>0.736939</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.719317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.751</td>\n",
       "      <td>0.777480</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.817337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.748479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.836946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.672018</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.758242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.764643</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.806202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.806030</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.836864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.764520</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.847235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.755</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.843323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.771167</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.823881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.701</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.796117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.787585</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.751790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.714829</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.814176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.744</td>\n",
       "      <td>0.754617</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.875519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.845261</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.823045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.751</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.833188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.742</td>\n",
       "      <td>0.738337</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.760296</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.848542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.794987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.624</td>\n",
       "      <td>0.639155</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.742961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.718</td>\n",
       "      <td>0.695918</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.841584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_max  DHDT f1_max  sklearn accuracy_max  sklearn f1_max\n",
       "0               0.726     0.720077                 0.843        0.845624\n",
       "1               0.817     0.826211                 0.849        0.856600\n",
       "2               0.749     0.736067                 0.815        0.818805\n",
       "3               0.786     0.773784                 0.855        0.862819\n",
       "4               0.707     0.697626                 0.809        0.828083\n",
       "5               0.802     0.803828                 0.840        0.844358\n",
       "6               0.764     0.752036                 0.817        0.816817\n",
       "7               0.801     0.805854                 0.848        0.836820\n",
       "8               0.868     0.865580                 0.867        0.876279\n",
       "9               0.749     0.766076                 0.831        0.838894\n",
       "10              0.737     0.690224                 0.805        0.788274\n",
       "11              0.829     0.821689                 0.856        0.859922\n",
       "12              0.708     0.734545                 0.774        0.788390\n",
       "13              0.789     0.792527                 0.809        0.817574\n",
       "14              0.660     0.666667                 0.714        0.768233\n",
       "15              0.766     0.777143                 0.890        0.885655\n",
       "16              0.694     0.685832                 0.803        0.803980\n",
       "17              0.855     0.865616                 0.862        0.868821\n",
       "18              0.653     0.718115                 0.775        0.772036\n",
       "19              0.817     0.803859                 0.834        0.830303\n",
       "20              0.754     0.782301                 0.832        0.831663\n",
       "21              0.764     0.745690                 0.789        0.771398\n",
       "22              0.689     0.698292                 0.711        0.726066\n",
       "23              0.765     0.747338                 0.775        0.789523\n",
       "24              0.637     0.668902                 0.701        0.748998\n",
       "25              0.698     0.725225                 0.779        0.771282\n",
       "26              0.767     0.768159                 0.816        0.796009\n",
       "27              0.713     0.736939                 0.749        0.719317\n",
       "28              0.751     0.777480                 0.823        0.817337\n",
       "29              0.616     0.629396                 0.752        0.748479\n",
       "30              0.790     0.785714                 0.827        0.836946\n",
       "31              0.714     0.672018                 0.780        0.758242\n",
       "32              0.779     0.764643                 0.800        0.806202\n",
       "33              0.807     0.806030                 0.846        0.836864\n",
       "34              0.777     0.764520                 0.837        0.847235\n",
       "35              0.755     0.718861                 0.851        0.843323\n",
       "36              0.800     0.771167                 0.823        0.823881\n",
       "37              0.701     0.673203                 0.790        0.796117\n",
       "38              0.781     0.787585                 0.833        0.830800\n",
       "39              0.750     0.728261                 0.792        0.751790\n",
       "40              0.700     0.714829                 0.806        0.814176\n",
       "41              0.744     0.754617                 0.880        0.875519\n",
       "42              0.840     0.845261                 0.828        0.823045\n",
       "43              0.751     0.713464                 0.809        0.833188\n",
       "44              0.742     0.738337                 0.741        0.733333\n",
       "45              0.773     0.760296                 0.839        0.848542\n",
       "46              0.776     0.740741                 0.785        0.794987\n",
       "47              0.624     0.639155                 0.717        0.742961\n",
       "48              0.718     0.695918                 0.840        0.841584\n",
       "49              0.694     0.738462                 0.670        0.685714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.685896</td>\n",
       "      <td>0.748940</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>0.660412</td>\n",
       "      <td>0.747923</td>\n",
       "      <td>0.084916</td>\n",
       "      <td>0.774644</td>\n",
       "      <td>0.806340</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.774672</td>\n",
       "      <td>0.809056</td>\n",
       "      <td>0.026574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055002</td>\n",
       "      <td>0.056871</td>\n",
       "      <td>0.021304</td>\n",
       "      <td>0.073283</td>\n",
       "      <td>0.054148</td>\n",
       "      <td>0.054057</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.017181</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.045482</td>\n",
       "      <td>0.019852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.490955</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.589275</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.709250</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.616079</td>\n",
       "      <td>0.715651</td>\n",
       "      <td>0.045723</td>\n",
       "      <td>0.740300</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.744420</td>\n",
       "      <td>0.776096</td>\n",
       "      <td>0.010459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.752500</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>0.659998</td>\n",
       "      <td>0.746514</td>\n",
       "      <td>0.071745</td>\n",
       "      <td>0.781800</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.787450</td>\n",
       "      <td>0.818190</td>\n",
       "      <td>0.021025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.719350</td>\n",
       "      <td>0.784750</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>0.719380</td>\n",
       "      <td>0.781096</td>\n",
       "      <td>0.105724</td>\n",
       "      <td>0.815750</td>\n",
       "      <td>0.839750</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.815679</td>\n",
       "      <td>0.840912</td>\n",
       "      <td>0.041889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.834800</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.103779</td>\n",
       "      <td>0.827023</td>\n",
       "      <td>0.865616</td>\n",
       "      <td>0.288036</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.855181</td>\n",
       "      <td>0.885655</td>\n",
       "      <td>0.084483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  DHDT f1_mean  \\\n",
       "count           50.000000          50.000000          50.000000     50.000000   \n",
       "mean             0.685896           0.748940           0.049860      0.660412   \n",
       "std              0.055002           0.056871           0.021304      0.073283   \n",
       "min              0.583800           0.616000           0.021160      0.490955   \n",
       "25%              0.649600           0.709250           0.032624      0.616079   \n",
       "50%              0.689700           0.752500           0.046316      0.659998   \n",
       "75%              0.719350           0.784750           0.061731      0.719380   \n",
       "max              0.834800           0.868000           0.103779      0.827023   \n",
       "\n",
       "       DHDT f1_max  DHDT f1_std  sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "count    50.000000    50.000000              50.000000             50.000000   \n",
       "mean      0.747923     0.084916               0.774644              0.806340   \n",
       "std       0.054148     0.054057               0.052176              0.047979   \n",
       "min       0.629396     0.011228               0.660400              0.670000   \n",
       "25%       0.715651     0.045723               0.740300              0.781250   \n",
       "50%       0.746514     0.071745               0.781800              0.815500   \n",
       "75%       0.781096     0.105724               0.815750              0.839750   \n",
       "max       0.865616     0.288036               0.856800              0.890000   \n",
       "\n",
       "       sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  sklearn f1_std  \n",
       "count             50.000000        50.000000       50.000000       50.000000  \n",
       "mean               0.024224         0.774672        0.809056        0.026574  \n",
       "std                0.017181         0.054718        0.045482        0.019852  \n",
       "min                0.000800         0.589275        0.685714        0.000245  \n",
       "25%                0.009200         0.744420        0.776096        0.010459  \n",
       "50%                0.021200         0.787450        0.818190        0.021025  \n",
       "75%                0.035300         0.815679        0.840912        0.041889  \n",
       "max                0.063600         0.855181        0.885655        0.084483  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    parallel_eval_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_synthetic = parallel_eval_synthetic(delayed(evaluate_synthetic_parallel)(index = index,\n",
    "                                                                                                random_seed_data = config['computation']['random_seed']+index,\n",
    "                                                                                                random_seed_model = config['computation']['random_seed'],#+random_seed_model,\n",
    "                                                                                                config = config,\n",
    "                                                                                                verbosity = -1) for index in range(config['make_classification']['num_eval']))\n",
    "\n",
    "    for i, synthetic_result in enumerate(evaluation_results_synthetic):\n",
    "        if i == 0:\n",
    "            model_dict_synthetic = synthetic_result[0]\n",
    "            scores_dict_synthetic = synthetic_result[1]\n",
    "            dataset_dict_synthetic = synthetic_result[2]\n",
    "        else: \n",
    "            model_dict_synthetic = mergeDict(model_dict_synthetic, synthetic_result[0])\n",
    "            scores_dict_synthetic = mergeDict(scores_dict_synthetic, synthetic_result[1])\n",
    "            dataset_dict_synthetic = mergeDict(dataset_dict_synthetic, synthetic_result[2])    \n",
    "    \n",
    "    del synthetic_result, evaluation_results_synthetic\n",
    "    \n",
    "    \n",
    "    metric_identifer = '_test'\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    index = [i for i in range(config['make_classification']['num_eval'])]\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "\n",
    "    results_DHDT = None\n",
    "    results_sklearn = None\n",
    "    for metric in metrics:\n",
    "        scores_DHDT = [scores_dict_synthetic[i]['DHDT'][metric + metric_identifer] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "        scores_sklearn = [scores_dict_synthetic[i]['sklearn'][metric + metric_identifer] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "        scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "        scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "        results_DHDT_by_metric = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "        results_sklearn_by_metric = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "        if results_DHDT is None and results_sklearn is None:\n",
    "            results_DHDT = results_DHDT_by_metric\n",
    "            results_sklearn = results_sklearn_by_metric\n",
    "        else:\n",
    "            results_DHDT = np.vstack([results_DHDT, results_DHDT_by_metric])\n",
    "            results_sklearn = np.vstack([results_sklearn, results_sklearn_by_metric])\n",
    "\n",
    "    scores_dataframe_synthetic = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)    \n",
    "    \n",
    "    del model_dict_synthetic, scores_dict_synthetic, dataset_dict_synthetic\n",
    "    \n",
    "    display(scores_dataframe_synthetic)\n",
    "    display(scores_dataframe_synthetic[scores_dataframe_synthetic.columns[1::3]])\n",
    "    display(scores_dataframe_synthetic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c6c9-368b-4b75-b0d8-6e7a28e4267c",
   "metadata": {},
   "source": [
    "## Real-World Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb9eca0-b6be-4389-960f-857e68e6b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:41:55.720032Z",
     "iopub.status.busy": "2022-06-09T09:41:55.719923Z",
     "iopub.status.idle": "2022-06-09T09:42:23.026204Z",
     "shell.execute_reply": "2022-06-09T09:42:23.025467Z",
     "shell.execute_reply.started": "2022-06-09T09:41:55.720017Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of   5 | elapsed:   26.7s remaining:   40.1s\n",
      "[Parallel(n_jobs=50)]: Done   5 out of   5 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Habermans Survival</th>\n",
       "      <td>0.695082</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.82009</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  \\\n",
       "Habermans Survival            0.695082           0.704918           0.008031   \n",
       "\n",
       "                    DHDT f1_mean  DHDT f1_max  DHDT f1_std  \\\n",
       "Habermans Survival       0.82009     0.826923     0.005579   \n",
       "\n",
       "                    sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "Habermans Survival               0.688525              0.688525   \n",
       "\n",
       "                    sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  \\\n",
       "Habermans Survival                   0.0         0.791209        0.791209   \n",
       "\n",
       "                    sklearn f1_std  \n",
       "Habermans Survival             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Habermans Survival</th>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DHDT accuracy_max  DHDT f1_max  sklearn accuracy_max  \\\n",
       "Habermans Survival           0.704918     0.826923              0.688525   \n",
       "\n",
       "                    sklearn f1_max  \n",
       "Habermans Survival        0.791209  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    identifier_list = ['Habermans Survival']\n",
    "\n",
    "    parallel_eval_real_world = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_real_world = parallel_eval_real_world(delayed(evaluate_real_world_parallel)(identifier_list=identifier_list, \n",
    "                                                                                                   random_seed_model=config['computation']['random_seed']+i,\n",
    "                                                                                                   config = config,\n",
    "                                                                                                   verbosity = -1) for i in range(config['computation']['trials']))\n",
    "\n",
    "\n",
    "    for i, real_world_result in enumerate(evaluation_results_real_world):\n",
    "        if i == 0:\n",
    "            model_dict_real_world = real_world_result[0]\n",
    "            scores_dict_real_world = real_world_result[1]\n",
    "            dataset_dict_real_world = real_world_result[2]\n",
    "        else: \n",
    "            model_dict_real_world = mergeDict(model_dict_real_world, real_world_result[0])\n",
    "            scores_dict_real_world = mergeDict(scores_dict_real_world, real_world_result[1])\n",
    "            dataset_dict_real_world = mergeDict(dataset_dict_real_world, real_world_result[2])    \n",
    "\n",
    "    del real_world_result, evaluation_results_real_world\n",
    "\n",
    "    metric_identifer = '_test'\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    index = identifier_list\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "\n",
    "    results_DHDT = None\n",
    "    results_sklearn = None\n",
    "    for metric in metrics:\n",
    "        scores_DHDT = [scores_dict_real_world[identifier]['DHDT'][metric + metric_identifer] for identifier in identifier_list]\n",
    "\n",
    "        scores_sklearn = [scores_dict_real_world[identifier]['sklearn'][metric + metric_identifer] for identifier in identifier_list]    \n",
    "\n",
    "        scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "        scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "        results_DHDT_by_metric = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "        results_sklearn_by_metric = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "        if results_DHDT is None and results_sklearn is None:\n",
    "            results_DHDT = results_DHDT_by_metric\n",
    "            results_sklearn = results_sklearn_by_metric\n",
    "        else:\n",
    "            results_DHDT = np.vstack([results_DHDT, results_DHDT_by_metric])\n",
    "            results_sklearn = np.vstack([results_sklearn, results_sklearn_by_metric])\n",
    "\n",
    "    del model_dict_real_world, scores_dict_real_world, dataset_dict_real_world\n",
    "            \n",
    "    scores_dataframe_real_world = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_real_world)\n",
    "    display(scores_dataframe_real_world[scores_dataframe_real_world.columns[1::3]])    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c55370-5fa9-4f3c-a1a8-be597467f808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:42:23.028418Z",
     "iopub.status.busy": "2022-06-09T09:42:23.028170Z",
     "iopub.status.idle": "2022-06-09T09:42:23.034616Z",
     "shell.execute_reply": "2022-06-09T09:42:23.033976Z",
     "shell.execute_reply.started": "2022-06-09T09:42:23.028387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier = identifier_list[0]#\"Absenteeism\"\n",
    "    plt.figure(figsize=(15,8))\n",
    "    image = model_dict_real_world[identifier]['DHDT'].plot(normalizer_list=dataset_dict_real_world[identifier]['normalizer_list'])\n",
    "    display(image)\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plot_tree(model_dict_real_world[identifier]['sklearn'], fontsize=10) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fbd88-44c9-4d7f-b58d-def1eeb6d434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
