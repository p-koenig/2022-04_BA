{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3b444-064b-4639-983d-238b3da68ad9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6531aaa-2c26-40f5-8078-d754aceda153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:36.348594Z",
     "iopub.status.busy": "2022-06-09T09:45:36.347788Z",
     "iopub.status.idle": "2022-06-09T09:45:36.360819Z",
     "shell.execute_reply": "2022-06-09T09:45:36.360215Z",
     "shell.execute_reply.started": "2022-06-09T09:45:36.348511Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dhdt': {\n",
    "        'depth': 3,\n",
    "        'learning_rate': 0.01,#1e-3,\n",
    "        \n",
    "        'initializer': 'he_normal', #GlorotUniform\n",
    "        \n",
    "        'loss': 'binary_crossentropy',#'mae',\n",
    "        'optimizer': 'adam',        \n",
    "        \n",
    "        'beta_1': 10,\n",
    "        'beta_2': 50,\n",
    "        \n",
    "        'squeeze_factor': 1,\n",
    "        \n",
    "        'batch_size': 512,\n",
    "        'epochs': 1_000,\n",
    "        'early_stopping_epochs': 50,\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "    'make_classification': {\n",
    "        'number_of_variables': 20,\n",
    "        'n_samples': 5_000,\n",
    "        'num_eval': 50,\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'trials': 5,\n",
    "        'n_jobs': 50,\n",
    "        'verbosity': 0,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:36.362474Z",
     "iopub.status.busy": "2022-06-09T09:45:36.362105Z",
     "iopub.status.idle": "2022-06-09T09:45:41.598940Z",
     "shell.execute_reply": "2022-06-09T09:45:41.598026Z",
     "shell.execute_reply.started": "2022-06-09T09:45:36.362437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities import *\n",
    "from utilities.DHDT import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2397843-32d2-4579-b197-0686f4a99b54",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e7da9-12bb-467f-919f-4fd510ce6463",
   "metadata": {},
   "source": [
    "## make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8a1baf-79d0-4959-8376-fc6a5446a229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:45:41.600993Z",
     "iopub.status.busy": "2022-06-09T09:45:41.600627Z",
     "iopub.status.idle": "2022-06-09T09:46:26.719494Z",
     "shell.execute_reply": "2022-06-09T09:46:26.718517Z",
     "shell.execute_reply.started": "2022-06-09T09:45:41.600952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (10000, 20)\n",
      "Original Data Shape (encoded):  (10000, 20)\n",
      "Original Data Class Distribution:  4990  (true) / 5010  (false)\n",
      "(7000, 20) (7000,)\n",
      "(1000, 20) (1000,)\n",
      "(2000, 20) (2000,)\n",
      "True Ratio:  0.5008571428571429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484867db277040989a3c7e2e55887bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.69326 |\n",
      "Epoch: 01 | Loss: 0.69328 |\n",
      "Epoch: 02 | Loss: 0.69365 |\n",
      "Epoch: 03 | Loss: 0.69297 |\n",
      "Epoch: 04 | Loss: 0.69146 |\n",
      "Epoch: 05 | Loss: 0.68948 |\n",
      "Epoch: 06 | Loss: 0.68578 |\n",
      "Epoch: 07 | Loss: 0.68183 |\n",
      "Epoch: 08 | Loss: 0.67854 |\n",
      "Epoch: 09 | Loss: 0.67689 |\n",
      "Epoch: 10 | Loss: 0.67602 |\n",
      "Epoch: 11 | Loss: 0.67546 |\n",
      "Epoch: 12 | Loss: 0.67463 |\n",
      "Epoch: 13 | Loss: 0.67363 |\n",
      "Epoch: 14 | Loss: 0.67328 |\n",
      "Epoch: 15 | Loss: 0.67334 |\n",
      "Epoch: 16 | Loss: 0.67288 |\n",
      "Epoch: 17 | Loss: 0.67408 |\n",
      "Epoch: 18 | Loss: 0.67336 |\n",
      "Epoch: 19 | Loss: 0.67302 |\n",
      "Epoch: 20 | Loss: 0.67306 |\n",
      "Epoch: 21 | Loss: 0.67438 |\n",
      "Epoch: 22 | Loss: 0.67353 |\n",
      "Epoch: 23 | Loss: 0.67322 |\n",
      "Epoch: 24 | Loss: 0.67312 |\n",
      "Epoch: 25 | Loss: 0.67332 |\n",
      "Epoch: 26 | Loss: 0.67368 |\n",
      "Epoch: 27 | Loss: 0.67309 |\n",
      "Epoch: 28 | Loss: 0.67356 |\n",
      "Epoch: 29 | Loss: 0.67317 |\n",
      "Epoch: 30 | Loss: 0.67363 |\n",
      "Epoch: 31 | Loss: 0.67322 |\n",
      "Epoch: 32 | Loss: 0.67368 |\n",
      "Epoch: 33 | Loss: 0.67319 |\n",
      "Epoch: 34 | Loss: 0.67321 |\n",
      "Epoch: 35 | Loss: 0.67360 |\n",
      "Epoch: 36 | Loss: 0.67372 |\n",
      "Epoch: 37 | Loss: 0.67323 |\n",
      "Epoch: 38 | Loss: 0.67322 |\n",
      "Epoch: 39 | Loss: 0.67275 |\n",
      "Epoch: 40 | Loss: 0.67327 |\n",
      "Epoch: 41 | Loss: 0.67397 |\n",
      "Epoch: 42 | Loss: 0.67375 |\n",
      "Epoch: 43 | Loss: 0.67338 |\n",
      "Epoch: 44 | Loss: 0.67311 |\n",
      "Epoch: 45 | Loss: 0.67311 |\n",
      "Epoch: 46 | Loss: 0.67305 |\n",
      "Epoch: 47 | Loss: 0.67289 |\n",
      "Epoch: 48 | Loss: 0.67308 |\n",
      "Epoch: 49 | Loss: 0.67307 |\n",
      "Epoch: 50 | Loss: 0.67327 |\n",
      "Epoch: 51 | Loss: 0.67365 |\n",
      "Epoch: 52 | Loss: 0.67320 |\n",
      "Epoch: 53 | Loss: 0.67354 |\n",
      "Epoch: 54 | Loss: 0.67277 |\n",
      "Epoch: 55 | Loss: 0.67296 |\n",
      "Epoch: 56 | Loss: 0.67310 |\n",
      "Epoch: 57 | Loss: 0.67290 |\n",
      "Epoch: 58 | Loss: 0.67352 |\n",
      "Epoch: 59 | Loss: 0.67368 |\n",
      "Epoch: 60 | Loss: 0.67285 |\n",
      "Epoch: 61 | Loss: 0.67348 |\n",
      "Epoch: 62 | Loss: 0.67318 |\n",
      "Epoch: 63 | Loss: 0.67365 |\n",
      "Epoch: 64 | Loss: 0.67315 |\n",
      "Epoch: 65 | Loss: 0.67262 |\n",
      "Epoch: 66 | Loss: 0.67349 |\n",
      "Epoch: 67 | Loss: 0.67277 |\n",
      "Epoch: 68 | Loss: 0.67335 |\n",
      "Epoch: 69 | Loss: 0.67347 |\n",
      "Epoch: 70 | Loss: 0.67351 |\n",
      "Epoch: 71 | Loss: 0.67322 |\n",
      "Epoch: 72 | Loss: 0.67335 |\n",
      "Epoch: 73 | Loss: 0.67381 |\n",
      "Epoch: 74 | Loss: 0.67330 |\n",
      "Epoch: 75 | Loss: 0.67312 |\n",
      "Epoch: 76 | Loss: 0.67260 |\n",
      "Epoch: 77 | Loss: 0.67369 |\n",
      "Epoch: 78 | Loss: 0.67356 |\n",
      "Epoch: 79 | Loss: 0.67297 |\n",
      "Epoch: 80 | Loss: 0.67272 |\n",
      "Epoch: 81 | Loss: 0.67313 |\n",
      "Epoch: 82 | Loss: 0.67288 |\n",
      "Epoch: 83 | Loss: 0.67277 |\n",
      "Epoch: 84 | Loss: 0.67295 |\n",
      "Epoch: 85 | Loss: 0.67320 |\n",
      "Epoch: 86 | Loss: 0.67286 |\n",
      "Epoch: 87 | Loss: 0.67334 |\n",
      "Epoch: 88 | Loss: 0.67287 |\n",
      "Epoch: 89 | Loss: 0.67279 |\n",
      "Epoch: 90 | Loss: 0.67371 |\n",
      "Epoch: 91 | Loss: 0.67312 |\n",
      "Epoch: 92 | Loss: 0.67286 |\n",
      "Epoch: 93 | Loss: 0.67334 |\n",
      "Epoch: 94 | Loss: 0.67300 |\n",
      "Epoch: 95 | Loss: 0.67320 |\n",
      "Epoch: 96 | Loss: 0.67344 |\n",
      "Epoch: 97 | Loss: 0.67331 |\n",
      "Epoch: 98 | Loss: 0.67285 |\n",
      "Epoch: 99 | Loss: 0.67321 |\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    \n",
    "    config_test = deepcopy(config)\n",
    "    config_test['make_classification']['n_samples'] = 10_000\n",
    "    config_test['dhdt']['epochs'] = 100\n",
    "\n",
    "    dataset_dict = {}\n",
    "    model_dict = {}\n",
    "\n",
    "    scores_dict = {'sklearn': {},\n",
    "                   'DHDT': {}}\n",
    "\n",
    "    dataset_dict = get_preprocessed_dataset('make_classification',\n",
    "                                            random_seed=config_test['computation']['random_seed'],\n",
    "                                            config=config_test['make_classification'],\n",
    "                                            verbosity=1)\n",
    "\n",
    "    model_dict['sklearn'] = DecisionTreeClassifier(max_depth=3, \n",
    "                                                   random_state=config_test['computation']['random_seed'])\n",
    "\n",
    "    model_dict['sklearn'].fit(dataset_dict['X_train'], \n",
    "                              dataset_dict['y_train'])\n",
    "\n",
    "\n",
    "\n",
    "    model_dict['DHDT'] = DHDT(dataset_dict['X_train'].shape[1],\n",
    "\n",
    "                                depth = config_test['dhdt']['depth'],\n",
    "\n",
    "                                learning_rate = config_test['dhdt']['learning_rate'],\n",
    "                                optimizer = config_test['dhdt']['optimizer'],\n",
    "\n",
    "                                beta_1 = config_test['dhdt']['beta_1'],\n",
    "                                beta_2 = config_test['dhdt']['beta_2'],\n",
    "\n",
    "                                squeeze_factor = config_test['dhdt']['squeeze_factor'],\n",
    "\n",
    "                                loss = config_test['dhdt']['loss'],#'mae',\n",
    "\n",
    "                                random_seed = config_test['computation']['random_seed'],\n",
    "                                verbosity = 2)        \n",
    "\n",
    "\n",
    "    scores_dict['history'] = model_dict['DHDT'].fit(dataset_dict['X_train'], \n",
    "                                                  dataset_dict['y_train'], \n",
    "                                                  batch_size=config_test['dhdt']['batch_size'], \n",
    "                                                  epochs=config_test['dhdt']['epochs'], \n",
    "                                                  early_stopping_epochs=config_test['dhdt']['early_stopping_epochs'], \n",
    "                                                  valid_data=(dataset_dict['X_valid'], dataset_dict['y_valid']))\n",
    "\n",
    "\n",
    "\n",
    "    dataset_dict['y_test_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_test'])\n",
    "    dataset_dict['y_valid_dhdt'] = model_dict['DHDT'].predict(dataset_dict['X_valid'])\n",
    "\n",
    "    dataset_dict['y_test_sklearn'] = model_dict['sklearn'].predict(dataset_dict['X_test'])\n",
    "    dataset_dict['y_valid_sklearn'] = model_dict['sklearn'].predict(dataset_dict['X_valid'])     \n",
    "    \n",
    "    for metric in metrics:\n",
    "        \n",
    "        if metric in ['accuracy', 'f1']:\n",
    "            y_test_dhdt = np.round(dataset_dict['y_test_dhdt'])\n",
    "            y_valid_dhdt = np.round(dataset_dict['y_valid_dhdt'])\n",
    "            y_test_sklearn = np.round(dataset_dict['y_test_sklearn'])\n",
    "            y_valid_sklearn = np.round(dataset_dict['y_valid_sklearn'])         \n",
    "        else:\n",
    "            y_test_dhdt = dataset_dict['y_test_dhdt']\n",
    "            y_valid_dhdt = dataset_dict['y_valid_dhdt']\n",
    "            y_test_sklearn = dataset_dict['y_test_sklearn']\n",
    "            y_valid_sklearn =    dataset_dict['y_valid_sklearn']                \n",
    "        \n",
    "        scores_dict['sklearn'][metric + '_test'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_test'], y_test_sklearn)\n",
    "        scores_dict['DHDT'][metric + '_test'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_test'], y_test_dhdt)\n",
    "\n",
    "        scores_dict['sklearn'][metric + '_valid'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_valid'], y_valid_sklearn)   \n",
    "        scores_dict['DHDT'][metric + '_valid'] = sklearn.metrics.get_scorer(metric)._score_func(dataset_dict['y_valid'], y_valid_dhdt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a13b2a-ca9d-44de-9a52-b8ca61808093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:46:26.720604Z",
     "iopub.status.busy": "2022-06-09T09:46:26.720486Z",
     "iopub.status.idle": "2022-06-09T09:53:37.951279Z",
     "shell.execute_reply": "2022-06-09T09:53:37.950734Z",
     "shell.execute_reply.started": "2022-06-09T09:46:26.720588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of  50 | elapsed:  5.3min remaining: 126.2min\n",
      "[Parallel(n_jobs=50)]: Done  19 out of  50 | elapsed:  6.0min remaining:  9.8min\n",
      "[Parallel(n_jobs=50)]: Done  36 out of  50 | elapsed:  6.5min remaining:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  50 out of  50 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5658</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.047977</td>\n",
       "      <td>0.476465</td>\n",
       "      <td>0.601179</td>\n",
       "      <td>0.182926</td>\n",
       "      <td>0.6314</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.623860</td>\n",
       "      <td>0.702807</td>\n",
       "      <td>0.039473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.522209</td>\n",
       "      <td>0.592251</td>\n",
       "      <td>0.037229</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.611868</td>\n",
       "      <td>0.688299</td>\n",
       "      <td>0.038215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6256</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.024589</td>\n",
       "      <td>0.568280</td>\n",
       "      <td>0.672535</td>\n",
       "      <td>0.084312</td>\n",
       "      <td>0.7676</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.770586</td>\n",
       "      <td>0.777448</td>\n",
       "      <td>0.013724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6876</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.675633</td>\n",
       "      <td>0.705515</td>\n",
       "      <td>0.022849</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.803321</td>\n",
       "      <td>0.809479</td>\n",
       "      <td>0.012316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.015187</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>0.016190</td>\n",
       "      <td>0.6856</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.690024</td>\n",
       "      <td>0.778243</td>\n",
       "      <td>0.044110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5438</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>0.596350</td>\n",
       "      <td>0.645588</td>\n",
       "      <td>0.050531</td>\n",
       "      <td>0.6522</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.639626</td>\n",
       "      <td>0.653768</td>\n",
       "      <td>0.028284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6684</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.028303</td>\n",
       "      <td>0.625272</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.059461</td>\n",
       "      <td>0.7252</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.707323</td>\n",
       "      <td>0.725230</td>\n",
       "      <td>0.035813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>0.519235</td>\n",
       "      <td>0.586451</td>\n",
       "      <td>0.067822</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.541394</td>\n",
       "      <td>0.733068</td>\n",
       "      <td>0.095837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6764</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.083591</td>\n",
       "      <td>0.623312</td>\n",
       "      <td>0.775439</td>\n",
       "      <td>0.181032</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.766593</td>\n",
       "      <td>0.798464</td>\n",
       "      <td>0.063743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.024519</td>\n",
       "      <td>0.585374</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.039031</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.701030</td>\n",
       "      <td>0.714138</td>\n",
       "      <td>0.026216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.6072</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.054737</td>\n",
       "      <td>0.523970</td>\n",
       "      <td>0.670820</td>\n",
       "      <td>0.200461</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.592931</td>\n",
       "      <td>0.655852</td>\n",
       "      <td>0.125842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.450146</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.108722</td>\n",
       "      <td>0.6654</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.615278</td>\n",
       "      <td>0.634091</td>\n",
       "      <td>0.037627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5622</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.054201</td>\n",
       "      <td>0.458340</td>\n",
       "      <td>0.645897</td>\n",
       "      <td>0.184085</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.679016</td>\n",
       "      <td>0.744643</td>\n",
       "      <td>0.032814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>0.507424</td>\n",
       "      <td>0.577299</td>\n",
       "      <td>0.061325</td>\n",
       "      <td>0.6078</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.581968</td>\n",
       "      <td>0.582106</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5386</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.032727</td>\n",
       "      <td>0.439070</td>\n",
       "      <td>0.605919</td>\n",
       "      <td>0.195261</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.628478</td>\n",
       "      <td>0.027220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6074</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.571238</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.107989</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>0.789298</td>\n",
       "      <td>0.012618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.6708</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.050054</td>\n",
       "      <td>0.648129</td>\n",
       "      <td>0.727077</td>\n",
       "      <td>0.073285</td>\n",
       "      <td>0.7570</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.735647</td>\n",
       "      <td>0.820461</td>\n",
       "      <td>0.042407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6292</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.020193</td>\n",
       "      <td>0.560371</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.052574</td>\n",
       "      <td>0.7126</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.721275</td>\n",
       "      <td>0.726358</td>\n",
       "      <td>0.010166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6434</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.047251</td>\n",
       "      <td>0.611967</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.070174</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.698550</td>\n",
       "      <td>0.752647</td>\n",
       "      <td>0.108193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.6842</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.638670</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.076022</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.709870</td>\n",
       "      <td>0.734450</td>\n",
       "      <td>0.049159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6744</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.043939</td>\n",
       "      <td>0.639466</td>\n",
       "      <td>0.728061</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.771639</td>\n",
       "      <td>0.771784</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5914</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.070859</td>\n",
       "      <td>0.546597</td>\n",
       "      <td>0.700529</td>\n",
       "      <td>0.134267</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.665518</td>\n",
       "      <td>0.787417</td>\n",
       "      <td>0.060949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.6064</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.597197</td>\n",
       "      <td>0.652840</td>\n",
       "      <td>0.066966</td>\n",
       "      <td>0.6318</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.662866</td>\n",
       "      <td>0.669284</td>\n",
       "      <td>0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5998</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.035346</td>\n",
       "      <td>0.636458</td>\n",
       "      <td>0.678273</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.6718</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.670928</td>\n",
       "      <td>0.678466</td>\n",
       "      <td>0.015076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.030858</td>\n",
       "      <td>0.506812</td>\n",
       "      <td>0.655246</td>\n",
       "      <td>0.142650</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.604066</td>\n",
       "      <td>0.673786</td>\n",
       "      <td>0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.6722</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.639274</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>0.038572</td>\n",
       "      <td>0.7934</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.789680</td>\n",
       "      <td>0.819064</td>\n",
       "      <td>0.014692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5896</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.059798</td>\n",
       "      <td>0.527486</td>\n",
       "      <td>0.644769</td>\n",
       "      <td>0.115025</td>\n",
       "      <td>0.6514</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.569297</td>\n",
       "      <td>0.745474</td>\n",
       "      <td>0.088089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.522043</td>\n",
       "      <td>0.575556</td>\n",
       "      <td>0.068327</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.680473</td>\n",
       "      <td>0.723364</td>\n",
       "      <td>0.021446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5552</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.525778</td>\n",
       "      <td>0.645614</td>\n",
       "      <td>0.122553</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.678478</td>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.046791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5654</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.026058</td>\n",
       "      <td>0.566480</td>\n",
       "      <td>0.640424</td>\n",
       "      <td>0.062803</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.584403</td>\n",
       "      <td>0.601081</td>\n",
       "      <td>0.008339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.5980</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.575417</td>\n",
       "      <td>0.663443</td>\n",
       "      <td>0.083885</td>\n",
       "      <td>0.6876</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.707648</td>\n",
       "      <td>0.712946</td>\n",
       "      <td>0.010596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.6414</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.060526</td>\n",
       "      <td>0.611555</td>\n",
       "      <td>0.700935</td>\n",
       "      <td>0.089548</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.733267</td>\n",
       "      <td>0.750270</td>\n",
       "      <td>0.034006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.040415</td>\n",
       "      <td>0.563504</td>\n",
       "      <td>0.636583</td>\n",
       "      <td>0.071079</td>\n",
       "      <td>0.6644</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.676124</td>\n",
       "      <td>0.681510</td>\n",
       "      <td>0.010772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>0.648356</td>\n",
       "      <td>0.779317</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.706884</td>\n",
       "      <td>0.741967</td>\n",
       "      <td>0.017541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.5972</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.026903</td>\n",
       "      <td>0.567039</td>\n",
       "      <td>0.703183</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.739268</td>\n",
       "      <td>0.744223</td>\n",
       "      <td>0.009910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.037847</td>\n",
       "      <td>0.615274</td>\n",
       "      <td>0.691743</td>\n",
       "      <td>0.070336</td>\n",
       "      <td>0.6916</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.718562</td>\n",
       "      <td>0.723254</td>\n",
       "      <td>0.009384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.054547</td>\n",
       "      <td>0.638871</td>\n",
       "      <td>0.714949</td>\n",
       "      <td>0.100652</td>\n",
       "      <td>0.7618</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.771358</td>\n",
       "      <td>0.771623</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.5674</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.064587</td>\n",
       "      <td>0.464709</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.238641</td>\n",
       "      <td>0.7574</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.762772</td>\n",
       "      <td>0.790603</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.038119</td>\n",
       "      <td>0.652467</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.058710</td>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.732560</td>\n",
       "      <td>0.755801</td>\n",
       "      <td>0.046483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.6566</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>0.595321</td>\n",
       "      <td>0.676409</td>\n",
       "      <td>0.094947</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.778075</td>\n",
       "      <td>0.783366</td>\n",
       "      <td>0.010582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.580899</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.106026</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.0524</td>\n",
       "      <td>0.712090</td>\n",
       "      <td>0.833176</td>\n",
       "      <td>0.060543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.7008</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.054657</td>\n",
       "      <td>0.675361</td>\n",
       "      <td>0.747138</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>0.795559</td>\n",
       "      <td>0.823649</td>\n",
       "      <td>0.056181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.6382</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.629615</td>\n",
       "      <td>0.704607</td>\n",
       "      <td>0.051424</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.707245</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>0.011727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.6134</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.055327</td>\n",
       "      <td>0.609356</td>\n",
       "      <td>0.726916</td>\n",
       "      <td>0.112394</td>\n",
       "      <td>0.7342</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.738775</td>\n",
       "      <td>0.803452</td>\n",
       "      <td>0.032338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.602499</td>\n",
       "      <td>0.637153</td>\n",
       "      <td>0.035691</td>\n",
       "      <td>0.6316</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.677104</td>\n",
       "      <td>0.697521</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.053935</td>\n",
       "      <td>0.551233</td>\n",
       "      <td>0.696395</td>\n",
       "      <td>0.098755</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.627923</td>\n",
       "      <td>0.731849</td>\n",
       "      <td>0.051963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.6664</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.065420</td>\n",
       "      <td>0.662457</td>\n",
       "      <td>0.760891</td>\n",
       "      <td>0.094425</td>\n",
       "      <td>0.7746</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.783289</td>\n",
       "      <td>0.806232</td>\n",
       "      <td>0.045885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>0.658296</td>\n",
       "      <td>0.726813</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.740838</td>\n",
       "      <td>0.744094</td>\n",
       "      <td>0.006514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.7114</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>0.716268</td>\n",
       "      <td>0.741834</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>0.7766</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.790254</td>\n",
       "      <td>0.810256</td>\n",
       "      <td>0.010001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.6058</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.025357</td>\n",
       "      <td>0.578647</td>\n",
       "      <td>0.636201</td>\n",
       "      <td>0.062052</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.657915</td>\n",
       "      <td>0.711596</td>\n",
       "      <td>0.026840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  DHDT f1_mean  \\\n",
       "0               0.5658              0.636           0.047977      0.476465   \n",
       "1               0.5848              0.615           0.026828      0.522209   \n",
       "2               0.6256              0.647           0.024589      0.568280   \n",
       "3               0.6876              0.717           0.022132      0.675633   \n",
       "4               0.6154              0.642           0.015187      0.660878   \n",
       "5               0.5438              0.587           0.030262      0.596350   \n",
       "6               0.6684              0.699           0.028303      0.625272   \n",
       "7               0.5742              0.631           0.034822      0.519235   \n",
       "8               0.6764              0.754           0.083591      0.623312   \n",
       "9               0.6250              0.647           0.024519      0.585374   \n",
       "10              0.6072              0.683           0.054737      0.523970   \n",
       "11              0.5780              0.617           0.029455      0.450146   \n",
       "12              0.5622              0.650           0.054201      0.458340   \n",
       "13              0.5714              0.622           0.027038      0.507424   \n",
       "14              0.5386              0.584           0.032727      0.439070   \n",
       "15              0.6074              0.675           0.038422      0.571238   \n",
       "16              0.6708              0.747           0.050054      0.648129   \n",
       "17              0.6292              0.652           0.020193      0.560371   \n",
       "18              0.6434              0.696           0.047251      0.611967   \n",
       "19              0.6842              0.746           0.046387      0.638670   \n",
       "20              0.6744              0.749           0.043939      0.639466   \n",
       "21              0.5914              0.717           0.070859      0.546597   \n",
       "22              0.6064              0.629           0.018715      0.597197   \n",
       "23              0.5998              0.637           0.035346      0.636458   \n",
       "24              0.5556              0.602           0.030858      0.506812   \n",
       "25              0.6722              0.715           0.033391      0.639274   \n",
       "26              0.5896              0.708           0.059798      0.527486   \n",
       "27              0.5890              0.618           0.020396      0.522043   \n",
       "28              0.5552              0.596           0.031390      0.525778   \n",
       "29              0.5654              0.590           0.026058      0.566480   \n",
       "30              0.5980              0.652           0.036205      0.575417   \n",
       "31              0.6414              0.705           0.060526      0.611555   \n",
       "32              0.5878              0.634           0.040415      0.563504   \n",
       "33              0.6680              0.761           0.052196      0.648356   \n",
       "34              0.5972              0.639           0.026903      0.567039   \n",
       "35              0.5900              0.664           0.037847      0.615274   \n",
       "36              0.6548              0.711           0.054547      0.638871   \n",
       "37              0.5674              0.651           0.064587      0.464709   \n",
       "38              0.6634              0.715           0.038119      0.652467   \n",
       "39              0.6566              0.696           0.040207      0.595321   \n",
       "40              0.6264              0.752           0.064509      0.580899   \n",
       "41              0.7008              0.757           0.054657      0.675361   \n",
       "42              0.6382              0.708           0.045332      0.629615   \n",
       "43              0.6134              0.722           0.055327      0.609356   \n",
       "44              0.5920              0.614           0.014711      0.602499   \n",
       "45              0.5728              0.680           0.053935      0.551233   \n",
       "46              0.6664              0.753           0.065420      0.662457   \n",
       "47              0.6226              0.676           0.035058      0.658296   \n",
       "48              0.7114              0.755           0.026613      0.716268   \n",
       "49              0.6058              0.645           0.025357      0.578647   \n",
       "\n",
       "    DHDT f1_max  DHDT f1_std  sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "0      0.601179     0.182926                 0.6314                 0.693   \n",
       "1      0.592251     0.037229                 0.6486                 0.683   \n",
       "2      0.672535     0.084312                 0.7676                 0.775   \n",
       "3      0.705515     0.022849                 0.7960                 0.799   \n",
       "4      0.679335     0.016190                 0.6856                 0.788   \n",
       "5      0.645588     0.050531                 0.6522                 0.660   \n",
       "6      0.693795     0.059461                 0.7252                 0.731   \n",
       "7      0.586451     0.067822                 0.6048                 0.732   \n",
       "8      0.775439     0.181032                 0.7674                 0.790   \n",
       "9      0.637931     0.039031                 0.7032                 0.723   \n",
       "10     0.670820     0.200461                 0.6736                 0.703   \n",
       "11     0.601626     0.108722                 0.6654                 0.678   \n",
       "12     0.645897     0.184085                 0.6324                 0.714   \n",
       "13     0.577299     0.061325                 0.6078                 0.631   \n",
       "14     0.605919     0.195261                 0.5546                 0.589   \n",
       "15     0.712644     0.107989                 0.7988                 0.811   \n",
       "16     0.727077     0.073285                 0.7570                 0.821   \n",
       "17     0.640496     0.052574                 0.7126                 0.728   \n",
       "18     0.686598     0.070174                 0.7102                 0.743   \n",
       "19     0.717778     0.076022                 0.7538                 0.778   \n",
       "20     0.728061     0.070403                 0.7778                 0.780   \n",
       "21     0.700529     0.134267                 0.6538                 0.777   \n",
       "22     0.652840     0.066966                 0.6318                 0.663   \n",
       "23     0.678273     0.026432                 0.6718                 0.673   \n",
       "24     0.655246     0.142650                 0.6408                 0.664   \n",
       "25     0.678223     0.038572                 0.7934                 0.795   \n",
       "26     0.644769     0.115025                 0.6514                 0.761   \n",
       "27     0.575556     0.068327                 0.6552                 0.704   \n",
       "28     0.645614     0.122553                 0.6328                 0.752   \n",
       "29     0.640424     0.062803                 0.6102                 0.631   \n",
       "30     0.663443     0.083885                 0.6876                 0.694   \n",
       "31     0.700935     0.089548                 0.7520                 0.769   \n",
       "32     0.636583     0.071079                 0.6644                 0.671   \n",
       "33     0.779317     0.071071                 0.7166                 0.735   \n",
       "34     0.703183     0.133561                 0.6780                 0.679   \n",
       "35     0.691743     0.070336                 0.6916                 0.710   \n",
       "36     0.714949     0.100652                 0.7618                 0.765   \n",
       "37     0.673203     0.238641                 0.7574                 0.795   \n",
       "38     0.748899     0.058710                 0.7522                 0.779   \n",
       "39     0.676409     0.094947                 0.7698                 0.776   \n",
       "40     0.733333     0.106026                 0.7182                 0.823   \n",
       "41     0.747138     0.056500                 0.7926                 0.827   \n",
       "42     0.704607     0.051424                 0.7152                 0.719   \n",
       "43     0.726916     0.112394                 0.7342                 0.795   \n",
       "44     0.637153     0.035691                 0.6316                 0.634   \n",
       "45     0.696395     0.098755                 0.6494                 0.723   \n",
       "46     0.760891     0.094425                 0.7746                 0.801   \n",
       "47     0.726813     0.046700                 0.7368                 0.740   \n",
       "48     0.741834     0.022079                 0.7766                 0.815   \n",
       "49     0.636201     0.062052                 0.6938                 0.709   \n",
       "\n",
       "    sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  sklearn f1_std  \n",
       "0                 0.0308         0.623860        0.702807        0.039473  \n",
       "1                 0.0172         0.611868        0.688299        0.038215  \n",
       "2                 0.0148         0.770586        0.777448        0.013724  \n",
       "3                 0.0060         0.803321        0.809479        0.012316  \n",
       "4                 0.0512         0.690024        0.778243        0.044110  \n",
       "5                 0.0156         0.639626        0.653768        0.028284  \n",
       "6                 0.0116         0.707323        0.725230        0.035813  \n",
       "7                 0.0636         0.541394        0.733068        0.095837  \n",
       "8                 0.0452         0.766593        0.798464        0.063743  \n",
       "9                 0.0396         0.701030        0.714138        0.026216  \n",
       "10                0.0588         0.592931        0.655852        0.125842  \n",
       "11                0.0252         0.615278        0.634091        0.037627  \n",
       "12                0.0408         0.679016        0.744643        0.032814  \n",
       "13                0.0116         0.581968        0.582106        0.000069  \n",
       "14                0.0172         0.614868        0.628478        0.027220  \n",
       "15                0.0244         0.782989        0.789298        0.012618  \n",
       "16                0.0320         0.735647        0.820461        0.042407  \n",
       "17                0.0308         0.721275        0.726358        0.010166  \n",
       "18                0.0656         0.698550        0.752647        0.108193  \n",
       "19                0.0484         0.709870        0.734450        0.049159  \n",
       "20                0.0044         0.771639        0.771784        0.000290  \n",
       "21                0.0616         0.665518        0.787417        0.060949  \n",
       "22                0.0156         0.662866        0.669284        0.003209  \n",
       "23                0.0024         0.670928        0.678466        0.015076  \n",
       "24                0.0116         0.604066        0.673786        0.034860  \n",
       "25                0.0008         0.789680        0.819064        0.014692  \n",
       "26                0.0548         0.569297        0.745474        0.088089  \n",
       "27                0.0244         0.680473        0.723364        0.021446  \n",
       "28                0.0596         0.678478        0.772059        0.046791  \n",
       "29                0.0104         0.584403        0.601081        0.008339  \n",
       "30                0.0128         0.707648        0.712946        0.010596  \n",
       "31                0.0340         0.733267        0.750270        0.034006  \n",
       "32                0.0132         0.676124        0.681510        0.010772  \n",
       "33                0.0092         0.706884        0.741967        0.017541  \n",
       "34                0.0020         0.739268        0.744223        0.009910  \n",
       "35                0.0092         0.718562        0.723254        0.009384  \n",
       "36                0.0016         0.771358        0.771623        0.000132  \n",
       "37                0.0188         0.762772        0.790603        0.013915  \n",
       "38                0.0536         0.732560        0.755801        0.046483  \n",
       "39                0.0124         0.778075        0.783366        0.010582  \n",
       "40                0.0524         0.712090        0.833176        0.060543  \n",
       "41                0.0688         0.795559        0.823649        0.056181  \n",
       "42                0.0076         0.707245        0.730700        0.011727  \n",
       "43                0.0304         0.738775        0.803452        0.032338  \n",
       "44                0.0012         0.677104        0.697521        0.010208  \n",
       "45                0.0368         0.627923        0.731849        0.051963  \n",
       "46                0.0528         0.783289        0.806232        0.045885  \n",
       "47                0.0064         0.740838        0.744094        0.006514  \n",
       "48                0.0192         0.790254        0.810256        0.010001  \n",
       "49                0.0076         0.657915        0.711596        0.026840  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.636</td>\n",
       "      <td>0.601179</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.702807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.615</td>\n",
       "      <td>0.592251</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.688299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.672535</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.777448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.705515</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.809479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.778243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.587</td>\n",
       "      <td>0.645588</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.653768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.725230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.631</td>\n",
       "      <td>0.586451</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.733068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.775439</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.798464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.714138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.683</td>\n",
       "      <td>0.670820</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.655852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.634091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.645897</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.744643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.577299</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.582106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.584</td>\n",
       "      <td>0.605919</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.628478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.789298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.727077</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.820461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.652</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.726358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.686598</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.752647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.734450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.728061</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.771784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.700529</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.787417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.629</td>\n",
       "      <td>0.652840</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.669284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.637</td>\n",
       "      <td>0.678273</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.678466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.602</td>\n",
       "      <td>0.655246</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.673786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.819064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.644769</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.745474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.575556</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.723364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.596</td>\n",
       "      <td>0.645614</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.640424</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.601081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.652</td>\n",
       "      <td>0.663443</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.712946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.705</td>\n",
       "      <td>0.700935</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.750270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.636583</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.681510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.779317</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.741967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.639</td>\n",
       "      <td>0.703183</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.744223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0.691743</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.723254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714949</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.771623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.790603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.755801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.676409</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.783366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.833176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.747138</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.823649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.704607</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.722</td>\n",
       "      <td>0.726916</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.803452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.614</td>\n",
       "      <td>0.637153</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.697521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.680</td>\n",
       "      <td>0.696395</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.731849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.753</td>\n",
       "      <td>0.760891</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.806232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.676</td>\n",
       "      <td>0.726813</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.744094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.755</td>\n",
       "      <td>0.741834</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.810256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.645</td>\n",
       "      <td>0.636201</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.711596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DHDT accuracy_max  DHDT f1_max  sklearn accuracy_max  sklearn f1_max\n",
       "0               0.636     0.601179                 0.693        0.702807\n",
       "1               0.615     0.592251                 0.683        0.688299\n",
       "2               0.647     0.672535                 0.775        0.777448\n",
       "3               0.717     0.705515                 0.799        0.809479\n",
       "4               0.642     0.679335                 0.788        0.778243\n",
       "5               0.587     0.645588                 0.660        0.653768\n",
       "6               0.699     0.693795                 0.731        0.725230\n",
       "7               0.631     0.586451                 0.732        0.733068\n",
       "8               0.754     0.775439                 0.790        0.798464\n",
       "9               0.647     0.637931                 0.723        0.714138\n",
       "10              0.683     0.670820                 0.703        0.655852\n",
       "11              0.617     0.601626                 0.678        0.634091\n",
       "12              0.650     0.645897                 0.714        0.744643\n",
       "13              0.622     0.577299                 0.631        0.582106\n",
       "14              0.584     0.605919                 0.589        0.628478\n",
       "15              0.675     0.712644                 0.811        0.789298\n",
       "16              0.747     0.727077                 0.821        0.820461\n",
       "17              0.652     0.640496                 0.728        0.726358\n",
       "18              0.696     0.686598                 0.743        0.752647\n",
       "19              0.746     0.717778                 0.778        0.734450\n",
       "20              0.749     0.728061                 0.780        0.771784\n",
       "21              0.717     0.700529                 0.777        0.787417\n",
       "22              0.629     0.652840                 0.663        0.669284\n",
       "23              0.637     0.678273                 0.673        0.678466\n",
       "24              0.602     0.655246                 0.664        0.673786\n",
       "25              0.715     0.678223                 0.795        0.819064\n",
       "26              0.708     0.644769                 0.761        0.745474\n",
       "27              0.618     0.575556                 0.704        0.723364\n",
       "28              0.596     0.645614                 0.752        0.772059\n",
       "29              0.590     0.640424                 0.631        0.601081\n",
       "30              0.652     0.663443                 0.694        0.712946\n",
       "31              0.705     0.700935                 0.769        0.750270\n",
       "32              0.634     0.636583                 0.671        0.681510\n",
       "33              0.761     0.779317                 0.735        0.741967\n",
       "34              0.639     0.703183                 0.679        0.744223\n",
       "35              0.664     0.691743                 0.710        0.723254\n",
       "36              0.711     0.714949                 0.765        0.771623\n",
       "37              0.651     0.673203                 0.795        0.790603\n",
       "38              0.715     0.748899                 0.779        0.755801\n",
       "39              0.696     0.676409                 0.776        0.783366\n",
       "40              0.752     0.733333                 0.823        0.833176\n",
       "41              0.757     0.747138                 0.827        0.823649\n",
       "42              0.708     0.704607                 0.719        0.730700\n",
       "43              0.722     0.726916                 0.795        0.803452\n",
       "44              0.614     0.637153                 0.634        0.697521\n",
       "45              0.680     0.696395                 0.723        0.731849\n",
       "46              0.753     0.760891                 0.801        0.806232\n",
       "47              0.676     0.726813                 0.740        0.744094\n",
       "48              0.755     0.741834                 0.815        0.810256\n",
       "49              0.645     0.636201                 0.709        0.711596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.616656</td>\n",
       "      <td>0.673960</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>0.583341</td>\n",
       "      <td>0.677513</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.700432</td>\n",
       "      <td>0.734580</td>\n",
       "      <td>0.026920</td>\n",
       "      <td>0.696457</td>\n",
       "      <td>0.736784</td>\n",
       "      <td>0.032262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.065130</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0.061110</td>\n",
       "      <td>0.058665</td>\n",
       "      <td>0.020653</td>\n",
       "      <td>0.067457</td>\n",
       "      <td>0.059481</td>\n",
       "      <td>0.028037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.538600</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.439070</td>\n",
       "      <td>0.575556</td>\n",
       "      <td>0.016190</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.541394</td>\n",
       "      <td>0.582106</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.585550</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.026937</td>\n",
       "      <td>0.532264</td>\n",
       "      <td>0.641564</td>\n",
       "      <td>0.057053</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.693250</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.659153</td>\n",
       "      <td>0.705004</td>\n",
       "      <td>0.010640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.607300</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.037026</td>\n",
       "      <td>0.590348</td>\n",
       "      <td>0.678248</td>\n",
       "      <td>0.071075</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.707065</td>\n",
       "      <td>0.743031</td>\n",
       "      <td>0.027030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.656150</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.638117</td>\n",
       "      <td>0.714373</td>\n",
       "      <td>0.108539</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.779750</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.740445</td>\n",
       "      <td>0.782085</td>\n",
       "      <td>0.045441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>0.083591</td>\n",
       "      <td>0.716268</td>\n",
       "      <td>0.779317</td>\n",
       "      <td>0.238641</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.803321</td>\n",
       "      <td>0.833176</td>\n",
       "      <td>0.125842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  DHDT f1_mean  \\\n",
       "count           50.000000          50.000000          50.000000     50.000000   \n",
       "mean             0.616656           0.673960           0.040038      0.583341   \n",
       "std              0.044567           0.052833           0.015856      0.065130   \n",
       "min              0.538600           0.584000           0.014711      0.439070   \n",
       "25%              0.585550           0.634500           0.026937      0.532264   \n",
       "50%              0.607300           0.669500           0.037026      0.590348   \n",
       "75%              0.656150           0.715000           0.053500      0.638117   \n",
       "max              0.711400           0.761000           0.083591      0.716268   \n",
       "\n",
       "       DHDT f1_max  DHDT f1_std  sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "count    50.000000    50.000000              50.000000             50.000000   \n",
       "mean      0.677513     0.088355               0.700432              0.734580   \n",
       "std       0.051497     0.050749               0.061110              0.058665   \n",
       "min       0.575556     0.016190               0.554600              0.589000   \n",
       "25%       0.641564     0.057053               0.651600              0.693250   \n",
       "50%       0.678248     0.071075               0.698500              0.733500   \n",
       "75%       0.714373     0.108539               0.756200              0.779750   \n",
       "max       0.779317     0.238641               0.798800              0.827000   \n",
       "\n",
       "       sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  sklearn f1_std  \n",
       "count             50.000000        50.000000       50.000000       50.000000  \n",
       "mean               0.026920         0.696457        0.736784        0.032262  \n",
       "std                0.020653         0.067457        0.059481        0.028037  \n",
       "min                0.000800         0.541394        0.582106        0.000069  \n",
       "25%                0.010700         0.659153        0.705004        0.010640  \n",
       "50%                0.019000         0.707065        0.743031        0.027030  \n",
       "75%                0.044100         0.740445        0.782085        0.045441  \n",
       "max                0.068800         0.803321        0.833176        0.125842  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    parallel_eval_synthetic = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_synthetic = parallel_eval_synthetic(delayed(evaluate_synthetic_parallel)(index = index,\n",
    "                                                                                                random_seed_data = config['computation']['random_seed']+index,\n",
    "                                                                                                random_seed_model = config['computation']['random_seed'],#+random_seed_model,\n",
    "                                                                                                config = config,\n",
    "                                                                                                verbosity = -1) for index in range(config['make_classification']['num_eval']))\n",
    "\n",
    "    for i, synthetic_result in enumerate(evaluation_results_synthetic):\n",
    "        if i == 0:\n",
    "            model_dict_synthetic = synthetic_result[0]\n",
    "            scores_dict_synthetic = synthetic_result[1]\n",
    "            dataset_dict_synthetic = synthetic_result[2]\n",
    "        else: \n",
    "            model_dict_synthetic = mergeDict(model_dict_synthetic, synthetic_result[0])\n",
    "            scores_dict_synthetic = mergeDict(scores_dict_synthetic, synthetic_result[1])\n",
    "            dataset_dict_synthetic = mergeDict(dataset_dict_synthetic, synthetic_result[2])    \n",
    "    \n",
    "    del synthetic_result, evaluation_results_synthetic\n",
    "    \n",
    "    \n",
    "    metric_identifer = '_test'\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    index = [i for i in range(config['make_classification']['num_eval'])]\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "\n",
    "    results_DHDT = None\n",
    "    results_sklearn = None\n",
    "    for metric in metrics:\n",
    "        scores_DHDT = [scores_dict_synthetic[i]['DHDT'][metric + metric_identifer] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "        scores_sklearn = [scores_dict_synthetic[i]['sklearn'][metric + metric_identifer] for i in range(config['make_classification']['num_eval'])]\n",
    "\n",
    "        scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "        scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "        results_DHDT_by_metric = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "        results_sklearn_by_metric = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "        if results_DHDT is None and results_sklearn is None:\n",
    "            results_DHDT = results_DHDT_by_metric\n",
    "            results_sklearn = results_sklearn_by_metric\n",
    "        else:\n",
    "            results_DHDT = np.vstack([results_DHDT, results_DHDT_by_metric])\n",
    "            results_sklearn = np.vstack([results_sklearn, results_sklearn_by_metric])\n",
    "\n",
    "    scores_dataframe_synthetic = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)    \n",
    "    \n",
    "    del model_dict_synthetic, scores_dict_synthetic, dataset_dict_synthetic\n",
    "    \n",
    "    display(scores_dataframe_synthetic)\n",
    "    display(scores_dataframe_synthetic[scores_dataframe_synthetic.columns[1::3]])\n",
    "    display(scores_dataframe_synthetic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c6c9-368b-4b75-b0d8-6e7a28e4267c",
   "metadata": {},
   "source": [
    "## Real-World Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb9eca0-b6be-4389-960f-857e68e6b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:53:37.952649Z",
     "iopub.status.busy": "2022-06-09T09:53:37.952513Z",
     "iopub.status.idle": "2022-06-09T09:54:05.015602Z",
     "shell.execute_reply": "2022-06-09T09:54:05.014591Z",
     "shell.execute_reply.started": "2022-06-09T09:53:37.952631Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done   2 out of   5 | elapsed:   26.8s remaining:   40.2s\n",
      "[Parallel(n_jobs=50)]: Done   5 out of   5 | elapsed:   27.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_mean</th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT accuracy_std</th>\n",
       "      <th>DHDT f1_mean</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>DHDT f1_std</th>\n",
       "      <th>sklearn accuracy_mean</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn accuracy_std</th>\n",
       "      <th>sklearn f1_mean</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "      <th>sklearn f1_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Habermans Survival</th>\n",
       "      <td>0.695082</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.82009</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DHDT accuracy_mean  DHDT accuracy_max  DHDT accuracy_std  \\\n",
       "Habermans Survival            0.695082           0.704918           0.008031   \n",
       "\n",
       "                    DHDT f1_mean  DHDT f1_max  DHDT f1_std  \\\n",
       "Habermans Survival       0.82009     0.826923     0.005579   \n",
       "\n",
       "                    sklearn accuracy_mean  sklearn accuracy_max  \\\n",
       "Habermans Survival               0.688525              0.688525   \n",
       "\n",
       "                    sklearn accuracy_std  sklearn f1_mean  sklearn f1_max  \\\n",
       "Habermans Survival                   0.0         0.791209        0.791209   \n",
       "\n",
       "                    sklearn f1_std  \n",
       "Habermans Survival             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHDT accuracy_max</th>\n",
       "      <th>DHDT f1_max</th>\n",
       "      <th>sklearn accuracy_max</th>\n",
       "      <th>sklearn f1_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Habermans Survival</th>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.791209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DHDT accuracy_max  DHDT f1_max  sklearn accuracy_max  \\\n",
       "Habermans Survival           0.704918     0.826923              0.688525   \n",
       "\n",
       "                    sklearn f1_max  \n",
       "Habermans Survival        0.791209  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    identifier_list = [\n",
    "                        'Adult',#: 32,\n",
    "                        'Bank Marketing',#: 32,\n",
    "                        'Loan Credit',#: 32,\n",
    "\n",
    "                        'Credit Card',#: 23, \n",
    "                        'Car',#: 21,\n",
    "\n",
    "\n",
    "                        'Absenteeism',#: 15,\n",
    "                        'Loan House',#: 15,\n",
    "                        'Cervical Cancer',#: 15,\n",
    "\n",
    "                        'Heart Disease',#: 13,           \n",
    "\n",
    "                        'Titanic',#: 10,\n",
    "                        'Medical Insurance',#: 10,\n",
    "                        'Wisconsin Breast Cancer Original',#: 10,\n",
    "                        'Wisconsin Diagnostic Breast Cancer',#: 10,\n",
    "                        'Wisconsin Prognostic Breast Cancer',#: 10,\n",
    "                        'Abalone',#: 10,\n",
    "\n",
    "                        'Habermans Survival',#: 3, \n",
    "                      ]\n",
    "\n",
    "    identifier_list = ['Habermans Survival']\n",
    "\n",
    "    parallel_eval_real_world = Parallel(n_jobs=config['computation']['n_jobs'], verbose=3, backend='loky') #loky #sequential multiprocessing\n",
    "    evaluation_results_real_world = parallel_eval_real_world(delayed(evaluate_real_world_parallel)(identifier_list=identifier_list, \n",
    "                                                                                                   random_seed_model=config['computation']['random_seed']+i,\n",
    "                                                                                                   config = config,\n",
    "                                                                                                   verbosity = -1) for i in range(config['computation']['trials']))\n",
    "\n",
    "\n",
    "    for i, real_world_result in enumerate(evaluation_results_real_world):\n",
    "        if i == 0:\n",
    "            model_dict_real_world = real_world_result[0]\n",
    "            scores_dict_real_world = real_world_result[1]\n",
    "            dataset_dict_real_world = real_world_result[2]\n",
    "        else: \n",
    "            model_dict_real_world = mergeDict(model_dict_real_world, real_world_result[0])\n",
    "            scores_dict_real_world = mergeDict(scores_dict_real_world, real_world_result[1])\n",
    "            dataset_dict_real_world = mergeDict(dataset_dict_real_world, real_world_result[2])    \n",
    "\n",
    "    del real_world_result, evaluation_results_real_world\n",
    "\n",
    "    metric_identifer = '_test'\n",
    "    metrics = ['accuracy', 'f1']\n",
    "    index = identifier_list\n",
    "    columns = flatten_list([[[approach + ' ' + metric + '_mean', approach + ' ' + metric + '_max', approach + ' ' + metric + '_std'] for metric in metrics] for approach in ['DHDT', 'sklearn']])\n",
    "\n",
    "\n",
    "    results_DHDT = None\n",
    "    results_sklearn = None\n",
    "    for metric in metrics:\n",
    "        scores_DHDT = [scores_dict_real_world[identifier]['DHDT'][metric + metric_identifer] for identifier in identifier_list]\n",
    "\n",
    "        scores_sklearn = [scores_dict_real_world[identifier]['sklearn'][metric + metric_identifer] for identifier in identifier_list]    \n",
    "\n",
    "        scores_DHDT_mean = np.mean(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_mean = np.mean(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_max = np.max(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else scores_DHDT\n",
    "        scores_sklearn_max = np.max(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else scores_sklearn\n",
    "\n",
    "        scores_DHDT_std = np.std(scores_DHDT, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "        scores_sklearn_std = np.std(scores_sklearn, axis=1) if config['computation']['trials'] > 1 else np.array([0.0] * config['computation']['trials'])\n",
    "\n",
    "        results_DHDT_by_metric = np.vstack([scores_DHDT_mean, scores_DHDT_max, scores_DHDT_std])\n",
    "        results_sklearn_by_metric = np.vstack([scores_sklearn_mean, scores_sklearn_max, scores_sklearn_std])\n",
    "\n",
    "        if results_DHDT is None and results_sklearn is None:\n",
    "            results_DHDT = results_DHDT_by_metric\n",
    "            results_sklearn = results_sklearn_by_metric\n",
    "        else:\n",
    "            results_DHDT = np.vstack([results_DHDT, results_DHDT_by_metric])\n",
    "            results_sklearn = np.vstack([results_sklearn, results_sklearn_by_metric])\n",
    "\n",
    "    del model_dict_real_world, scores_dict_real_world, dataset_dict_real_world\n",
    "            \n",
    "    scores_dataframe_real_world = pd.DataFrame(data=np.vstack([results_DHDT, results_sklearn]).T, index = index, columns = columns)\n",
    "    display(scores_dataframe_real_world)\n",
    "    display(scores_dataframe_real_world[scores_dataframe_real_world.columns[1::3]])    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c55370-5fa9-4f3c-a1a8-be597467f808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T09:54:05.016808Z",
     "iopub.status.busy": "2022-06-09T09:54:05.016673Z",
     "iopub.status.idle": "2022-06-09T09:54:05.021279Z",
     "shell.execute_reply": "2022-06-09T09:54:05.020618Z",
     "shell.execute_reply.started": "2022-06-09T09:54:05.016792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    identifier = identifier_list[0]#\"Absenteeism\"\n",
    "    plt.figure(figsize=(15,8))\n",
    "    image = model_dict_real_world[identifier]['DHDT'].plot(normalizer_list=dataset_dict_real_world[identifier]['normalizer_list'])\n",
    "    display(image)\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plot_tree(model_dict_real_world[identifier]['sklearn'], fontsize=10) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fbd88-44c9-4d7f-b58d-def1eeb6d434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
