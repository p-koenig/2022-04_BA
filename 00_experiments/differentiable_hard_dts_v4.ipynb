{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:32.514368Z",
     "iopub.status.busy": "2022-06-04T09:01:32.513982Z",
     "iopub.status.idle": "2022-06-04T09:01:36.218831Z",
     "shell.execute_reply": "2022-06-04T09:01:36.217823Z",
     "shell.execute_reply.started": "2022-06-04T09:01:32.514278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def sigmoid_squeeze(x):\n",
    "    x = 1/(1+K.exp(-3*x))\n",
    "    return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa08eca3-ed45-4eca-bd81-d8d679cb13bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.220006Z",
     "iopub.status.busy": "2022-06-04T09:01:36.219877Z",
     "iopub.status.idle": "2022-06-04T09:01:36.223994Z",
     "shell.execute_reply": "2022-06-04T09:01:36.223315Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.219991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0c3ac7-7321-4224-8ffd-b103b1b669fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.226075Z",
     "iopub.status.busy": "2022-06-04T09:01:36.225481Z",
     "iopub.status.idle": "2022-06-04T09:01:36.259948Z",
     "shell.execute_reply": "2022-06-04T09:01:36.259179Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.226053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_real_world_data(X_data):\n",
    "    normalizer_list = []\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        for column_name in X_data:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_data[column_name].values.reshape(-1, 1))\n",
    "            X_data[column_name] = scaler.transform(X_data[column_name].values.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "    else:\n",
    "        for i, column in enumerate(X_data.T):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(column.reshape(-1, 1))\n",
    "            X_data[:,i] = scaler.transform(column.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "        \n",
    "    return X_data, normalizer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58193a0-9fea-4be3-8aca-d8f14b3480e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.260793Z",
     "iopub.status.busy": "2022-06-04T09:01:36.260686Z",
     "iopub.status.idle": "2022-06-04T09:01:36.411656Z",
     "shell.execute_reply": "2022-06-04T09:01:36.410659Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.260780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.activations.sparsemax([1.,3.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1576279-fe14-4861-964f-1f8ebb0a1608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.412814Z",
     "iopub.status.busy": "2022-06-04T09:01:36.412678Z",
     "iopub.status.idle": "2022-06-04T09:01:36.445359Z",
     "shell.execute_reply": "2022-06-04T09:01:36.444558Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.412798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.seq2seq.hardmax([1.,3.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86901d32-132c-4737-83fa-a789d46e3fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.446788Z",
     "iopub.status.busy": "2022-06-04T09:01:36.446546Z",
     "iopub.status.idle": "2022-06-04T09:01:36.570424Z",
     "shell.execute_reply": "2022-06-04T09:01:36.569521Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.446758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.activations.hard_sigmoid(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7d2dfc-8d2e-4c37-b257-e33fa853aaeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.571871Z",
     "iopub.status.busy": "2022-06-04T09:01:36.571627Z",
     "iopub.status.idle": "2022-06-04T09:01:36.653163Z",
     "shell.execute_reply": "2022-06-04T09:01:36.652128Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.571842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sigmoid(1000*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf86db3b-ecdc-4135-8887-7308e9eff00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.654957Z",
     "iopub.status.busy": "2022-06-04T09:01:36.654695Z",
     "iopub.status.idle": "2022-06-04T09:01:36.736990Z",
     "shell.execute_reply": "2022-06-04T09:01:36.736383Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.654917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa.activations.sparsemax([1., 9, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f2e596-9bb2-4026-890e-7b42365e81f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.738632Z",
     "iopub.status.busy": "2022-06-04T09:01:36.738385Z",
     "iopub.status.idle": "2022-06-04T09:01:36.827589Z",
     "shell.execute_reply": "2022-06-04T09:01:36.827212Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.738587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHDT(tf.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            loss='binary_crossentropy',#'mae',\n",
    "            optimizer = 'adam',\n",
    "            random_seed=42,\n",
    "            verbosity=1):    \n",
    "        \n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = tf.keras.losses.get(loss)\n",
    "        self.seed = random_seed\n",
    "        self.verbosity = verbosity\n",
    "        self.function_representation_type = function_representation_type\n",
    "        self.number_of_variables = number_of_variables\n",
    "        \n",
    "        self.internal_node_num_ = 2 ** self.depth - 1 \n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "        \n",
    "        tf.random.set_seed(self.seed)\n",
    "        \n",
    "        function_representation_length = ( \n",
    "          (2 ** self.depth - 1) * 2 + (2 ** self.depth)  if self.function_representation_type == 1 \n",
    "          else (2 ** self.depth - 1) + ((2 ** self.depth - 1) * self.number_of_variables) + (2 ** self.depth) if self.function_representation_type == 2 \n",
    "          else ((2 ** self.depth - 1) * self.number_of_variables * 2) + (2 ** self.depth)  if self.function_representation_type >= 3 \n",
    "          else None\n",
    "                                      )        \n",
    "        \n",
    "        self.dt_params =  tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(function_representation_length,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        \n",
    "        tf.print(self.dt_params)\n",
    "        \n",
    "        maximum_depth = self.depth\n",
    "        leaf_node_num_ = 2 ** maximum_depth\n",
    "        internal_node_num_ = 2 ** maximum_depth - 1\n",
    "        \n",
    "        #dt_params_activation = self.dt_params#self.apply_activation(self.dt_params)\n",
    "        \n",
    "        #internal_nodes, leaf_nodes = self.get_shaped_parameters_for_decision_tree(dt_params_activation)\n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_         \n",
    "        \n",
    "        self.split_values = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_values_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        #tf.sigmoid(self.dt_params[:split_values_num_params])\n",
    "        self.split_index_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_index_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        #self.dt_params[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "        self.leaf_classes_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(leaf_classes_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        #tf.sigmoid(self.dt_params[split_values_num_params+split_index_num_params:])        \n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        self.optimizer.learning_rate = self.learning_rate\n",
    "        \n",
    "    def fit(self, X, y, batch_size=32, epochs=100, early_stopping_epochs=5):\n",
    "        \n",
    "        minimum_loss_epoch = np.inf\n",
    "        epochs_without_improvement = 0        \n",
    "        \n",
    "        for current_epoch in tqdm(range(epochs)):\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            X = tf.random.shuffle(X, seed=self.seed + current_epoch)\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            y = tf.random.shuffle(y, seed=self.seed + current_epoch)\n",
    "            \n",
    "            loss_list = []\n",
    "            for index, (X_batch, y_batch) in enumerate(zip(make_batch(X, batch_size), make_batch(y, batch_size))):\n",
    "                current_loss = self.backward(X_batch, y_batch)\n",
    "                loss_list.append(float(current_loss))\n",
    "                \n",
    "                if self.verbosity >= 2:\n",
    "                    batch_idx = (index+1)*batch_size\n",
    "                    msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    print(msg.format(current_epoch, batch_idx, current_loss))                   \n",
    "                  \n",
    "            if self.verbosity > 0:    \n",
    "                msg = \"Epoch: {:02d} | Loss: {:.5f} |\"\n",
    "                print(msg.format(current_epoch, np.mean(loss_list)))              \n",
    "            \n",
    "            current_loss_epoch = np.mean(loss_list)\n",
    "\n",
    "            if current_loss_epoch < minimum_loss_epoch:\n",
    "                minimum_loss_epoch = current_loss_epoch\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= early_stopping_epochs:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        if True:\n",
    "            internal_node_num_ = self.internal_node_num_\n",
    "            leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "            split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "            split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "            leaf_classes_num_params = self.leaf_node_num_             \n",
    "            \n",
    "            paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "            #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "            function_values_dhdt = []\n",
    "            for entry in X:\n",
    "                \n",
    "                result = 0\n",
    "                for leaf_index, path in enumerate(paths):\n",
    "                    path_result_left = 1\n",
    "                    path_result_right = 1\n",
    "                    for internal_node_index in path: \n",
    "                        #tf.print(path, internal_node_index)\n",
    "                        #split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                        split_index = tfa.activations.sparsemax(10 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                        split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "\n",
    "                        \n",
    "                        internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                        respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "                                                \n",
    "                        #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                        #tf.print('respective_input_value', respective_input_value)\n",
    "                        \n",
    "                        #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                        split_decision = tf.sigmoid(1000 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "                        \n",
    "                        #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                        path_result_left *= split_decision\n",
    "                        path_result_right *= (1 - split_decision)\n",
    "\n",
    "                        #tf.print('path_result_left', path_result_left)\n",
    "                        #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                    \n",
    "                    result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print('RESULT', result)\n",
    "                    \n",
    "                function_values_dhdt.append(result)\n",
    "            function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "            #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "            \n",
    "        elif False:\n",
    "\n",
    "            internal_node_num_ = self.internal_node_num_\n",
    "            leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "            split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "            split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "            leaf_classes_num_params = self.leaf_node_num_ \n",
    "\n",
    "            #split_values = tf.sigmoid(self.dt_params[:split_values_num_params])\n",
    "            #split_index_array = self.dt_params[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "            #leaf_classes_array = tf.sigmoid(self.dt_params[split_values_num_params+split_index_num_params:])\n",
    "\n",
    "            split_values = tf.sigmoid(self.split_values)\n",
    "            split_index_array = self.split_index_array   \n",
    "            leaf_classes_array = tf.sigmoid(self.leaf_classes_array)\n",
    "\n",
    "            tf.print('split_values', split_values, summarize=-1)\n",
    "            tf.print('split_index_array', split_index_array, summarize=-1)\n",
    "            tf.print('leaf_classes_array', leaf_classes_array, summarize=-1)\n",
    "        \n",
    "            split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "            #split_index_array = tf.reshape(split_index_array, (internal_node_num_, -1))#tfa.activations.sparsemax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "            split_values_selected = tf.reduce_sum(split_index_array * tf.reshape(split_values, (internal_node_num_, -1)), axis=1)\n",
    "\n",
    "            X_extended = []\n",
    "            for entry in X:\n",
    "                X_extended.append([entry]*internal_node_num_)\n",
    "            X_extended = tf.stack(X_extended)\n",
    "            X_extended_reduced = tf.reduce_sum(split_index_array * X_extended, axis=2)\n",
    "\n",
    "            split_results = tf.round(tf.sigmoid(X_extended_reduced - split_values_selected))\n",
    "\n",
    "            tf.print('split_results', split_results, summarize=-1)\n",
    "\n",
    "                        \n",
    "            \n",
    "            if True:\n",
    "                tree_extended = []\n",
    "                for i in range(self.depth):\n",
    "                    duplicate_factor = 2**(self.depth-i)//2\n",
    "                    row = tf.reshape(tf.constant([], tf.float32), shape=(split_results.shape[0],0))\n",
    "                    for j in range(2**(i)):\n",
    "                        value = split_results[:,2**(i)-1+j]\n",
    "                        inverse_value = 1-split_results[:,2**(i)-1+j]\n",
    "\n",
    "                        #row.extend(tf.stack([value]*duplicate_factor, axis=1))\n",
    "                        #row.extend(tf.stack([inverse_value]*duplicate_factor, axis=1))\n",
    "\n",
    "                        value_extended = tf.stack([value]*duplicate_factor, axis=1)\n",
    "                        inverse_value_extended = tf.stack([inverse_value]*duplicate_factor, axis=1)\n",
    "\n",
    "                        new_values = tf.concat([value_extended, inverse_value_extended], axis=1)\n",
    "\n",
    "                        row = tf.concat([row, new_values], axis=1)\n",
    "                        #row = tf.stack(value_extended, inverse_value_extended)\n",
    "                        #for _ in range(duplicate_factor):\n",
    "                        #    row.extend([value, inverse_value])\n",
    "                    #print(row)\n",
    "                    #print(tf.stack(row))\n",
    "                    tree_extended.append(tf.stack(row))\n",
    "                tree_extended = tf.stack(tree_extended)\n",
    "                tree_extended = tf.transpose(tree_extended, perm=[1,0,2])\n",
    "                tf.print(tree_extended)   \n",
    "                tree_leaf_identifier = tf.reduce_prod(tree_extended, axis=1)\n",
    "                #print(tree_leaf_identifier)\n",
    "                tree_leaf_output = tree_leaf_identifier * leaf_classes_array\n",
    "                #tf.print(tree_leaf_output)\n",
    "                function_values_dhdt = tf.reduce_max(tree_leaf_output, axis= 1)\n",
    "            else:\n",
    "                for split_result in split_results:\n",
    "\n",
    "                    path_list = []\n",
    "                    #add_factor == 0\n",
    "                    leaf_counter = 0\n",
    "                    internal_counter = 0\n",
    "                    for i in range(self.depth):\n",
    "                        print('i', i)\n",
    "                        index = 2**(i)-1+internal_counter\n",
    "                        print('index', index)\n",
    "                        value = split_result[index]\n",
    "                        print('value', value)\n",
    "                        if value == 0:\n",
    "                            leaf_counter += 2**(self.depth-i-1)\n",
    "                            internal_counter = internal_counter ** 2 + 1\n",
    "                            print('internal_counter', internal_counter)\n",
    "                            #add_factor += 0\n",
    "                        #else:\n",
    "                            #add_factor += 1\n",
    "\n",
    "                            #print(value)\n",
    "                    print(leaf_counter)\n",
    "                    function_values_dhdt = leaf_classes_array[leaf_counter]\n",
    "                    print(leaf_counter)\n",
    "        else:\n",
    "            internal_node_num_ = self.internal_node_num_\n",
    "            leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "            split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "            split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "            leaf_classes_num_params = self.leaf_node_num_ \n",
    "\n",
    "            #split_values = tf.sigmoid(self.dt_params[:split_values_num_params])\n",
    "            #split_index_array = self.dt_params[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "            #leaf_classes_array = tf.sigmoid(self.dt_params[split_values_num_params+split_index_num_params:])\n",
    "\n",
    "            split_values = tf.sigmoid(self.split_values)\n",
    "            split_index_array = self.split_index_array   \n",
    "            leaf_classes_array = tf.sigmoid(self.leaf_classes_array)\n",
    "\n",
    "            tf.print('split_values', split_values, summarize=-1)\n",
    "            tf.print('split_index_array', split_index_array, summarize=-1)\n",
    "            tf.print('leaf_classes_array', leaf_classes_array, summarize=-1)            \n",
    "            \n",
    "            \n",
    "            for entry in X[:3]:\n",
    "\n",
    "                path_list = []\n",
    "                #add_factor == 0\n",
    "                leaf_counter = 0\n",
    "                internal_counter = 0\n",
    "                for i in range(self.depth):\n",
    "                    print('i', i)\n",
    "                    internal_index = 2**(i)-1+internal_counter\n",
    "                    print('internal_index', internal_index)\n",
    "\n",
    "                    split_index_for_internal = tf.argmax(split_index_array[self.number_of_variables*internal_index:self.number_of_variables*(internal_index+1)])\n",
    "                    split_values_for_internal = split_values[self.number_of_variables*internal_index+split_index_for_internal]\n",
    "\n",
    "                    entry_for_internal = entry[split_index_for_internal]\n",
    "\n",
    "\n",
    "                    #value = tf.reduce_max(split_values)#(split_index_array)\n",
    "                    value = tf.round(tf.sigmoid(entry_for_internal - split_values_for_internal))\n",
    "                    \n",
    "                    print('value', value)\n",
    "                    if value < 0.5:\n",
    "                        leaf_counter += 2**(self.depth-i-1)\n",
    "                        internal_counter = internal_counter ** 2 + 1\n",
    "                        print('internal_counter', internal_counter)            \n",
    "                function_values_dhdt = leaf_classes_array[leaf_counter]\n",
    "    \n",
    "        #tf.print(function_values_dhdt)        \n",
    "\n",
    "        return function_values_dhdt  \n",
    "           \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def backward(self, x,y):\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)#tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            #tape.watch(self.dt_params)\n",
    "            predicted = self.forward(x)\n",
    "            current_loss = self.loss(y, predicted)\n",
    "            \n",
    "        tf.print('predicted', predicted)\n",
    "        tf.print('current_loss', current_loss, summarize=-1)\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.leaf_classes_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.leaf_classes_array]))\n",
    "        tf.print('grads', grads, summarize=-1)        \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.split_values)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_values]))\n",
    "        tf.print('grads', grads, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.split_index_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_index_array]))\n",
    "        tf.print('grads', grads, summarize=-1)\n",
    "\n",
    "        #optimizer.apply_gradients(zip(grads, self.dt_params),\n",
    "        #                          global_step=tf.compat.v1.train.get_or_create_global_step())     \n",
    "        \n",
    "        #self.optimizer.apply_gradients(zip([grads], [self.dt_params]))\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        return current_loss\n",
    "        \n",
    "    \n",
    "    def apply_activation(self, dt_params):\n",
    "\n",
    "        dt_params_activation = dt_params\n",
    "        \n",
    "        if self.function_representation_type == 1:\n",
    "            pass\n",
    "        elif self.function_representation_type == 2:\n",
    "            pass\n",
    "        elif self.function_representation_type >= 3:\n",
    "            outputs_coeff_neurons_num_ = self.internal_node_num_ * self.number_of_variables\n",
    "\n",
    "            if self.function_representation_type == 3:\n",
    "                dt_params_activation[:outputs_coeff_neurons_num_].assign(tf.math.sigmoid(dt_params[:outputs_coeff_neurons_num_]))\n",
    "            elif self.function_representation_type == 4:\n",
    "                dt_params_activation[:outputs_coeff_neurons_num_].assign(sigmoid_squeeze(dt_params[:outputs_coeff_neurons_num_]))\n",
    "\n",
    "            current_position = outputs_coeff_neurons_num_\n",
    "            for outputs_index in range(self.internal_node_num_):\n",
    "                outputs_identifer_neurons = self.number_of_variables\n",
    "\n",
    "                dt_params_activation[current_position:current_position+outputs_identifer_neurons].assign(tf.math.softmax(dt_params[current_position:current_position+outputs_identifer_neurons]))\n",
    "                current_position += outputs_identifer_neurons\n",
    "\n",
    "            dt_params_activation[current_position:].assign(tf.math.sigmoid(dt_params[current_position:]))\n",
    "\n",
    "        \n",
    "        return dt_params_activation\n",
    "    \n",
    "    def get_shaped_parameters_for_decision_tree(self, parameter_array):\n",
    "\n",
    "        internal_node_num_ = 2 ** self.depth - 1 \n",
    "        leaf_node_num_ = 2 ** self.depth\n",
    "\n",
    "        if self.function_representation_type == 1:\n",
    "\n",
    "            splits_coeff = parameter_array[:internal_node_num_]\n",
    "            splits_coeff = tf.clip_by_value(splits_coeff, clip_value_min=0, clip_value_max=1)\n",
    "            splits_coeff_list = tf.split(splits_coeff, internal_node_num_)\n",
    "            splits_index = tf.cast(tf.clip_by_value(tf.round(parameter_array[internal_node_num_:internal_node_num_*2]), clip_value_min=0, clip_value_max=self.number_of_variables-1), tf.int64)\n",
    "            splits_index_list = tf.split(splits_index, internal_node_num_)\n",
    "\n",
    "            splits_list = []\n",
    "            for values_node, indices_node in zip(splits_coeff_list, splits_index_list):\n",
    "                sparse_tensor = tf.sparse.SparseTensor(indices=tf.expand_dims(indices_node, axis=1), values=values_node, dense_shape=[self.number_of_variables])\n",
    "                dense_tensor = tf.sparse.to_dense(sparse_tensor)\n",
    "                splits_list.append(dense_tensor)             \n",
    "\n",
    "            splits = tf.stack(splits_list)            \n",
    "\n",
    "            leaf_classes = parameter_array[internal_node_num_*2:]  \n",
    "            leaf_classes = tf.clip_by_value(leaf_classes, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "        elif self.function_representation_type == 2:\n",
    "\n",
    "            split_values_num_params = internal_node_num_ \n",
    "            split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "            leaf_classes_num_params = leaf_node_num_ \n",
    "\n",
    "            split_values = parameter_array[:split_values_num_params]\n",
    "            split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "            split_index_array = parameter_array[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "            split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)\n",
    "            split_index_list_by_internal_node_by_decision_sparsity = []\n",
    "            for tensor in split_index_list_by_internal_node:\n",
    "                split_tensor = tf.split(tensor, 1)\n",
    "                split_index_list_by_internal_node_by_decision_sparsity.append(split_tensor)\n",
    "            split_index_list_by_internal_node_by_decision_sparsity_argmax = tf.split(tf.argmax(split_index_list_by_internal_node_by_decision_sparsity, axis=2), internal_node_num_)\n",
    "            split_index_list_by_internal_node_by_decision_sparsity_argmax_new = []\n",
    "            for tensor in split_index_list_by_internal_node_by_decision_sparsity_argmax:\n",
    "                tensor_squeeze = tf.squeeze(tensor, axis=0)\n",
    "                split_index_list_by_internal_node_by_decision_sparsity_argmax_new.append(tensor_squeeze)\n",
    "            split_index_list_by_internal_node_by_decision_sparsity_argmax = split_index_list_by_internal_node_by_decision_sparsity_argmax_new    \n",
    "            dense_tensor_list = []\n",
    "            for indices_node, values_node in zip(split_index_list_by_internal_node_by_decision_sparsity_argmax,  split_values_list_by_internal_node):\n",
    "                sparse_tensor = tf.sparse.SparseTensor(indices=tf.expand_dims(indices_node, axis=1), values=values_node, dense_shape=[self.number_of_variables])\n",
    "                dense_tensor = tf.sparse.to_dense(sparse_tensor)\n",
    "                dense_tensor_list.append(dense_tensor) \n",
    "            splits = tf.stack(dense_tensor_list)\n",
    "\n",
    "            leaf_classes_array = parameter_array[split_values_num_params+split_index_num_params:]  \n",
    "            split_index_list_by_leaf_node = tf.split(leaf_classes_array, leaf_node_num_)\n",
    "\n",
    "            leaf_classes = tf.squeeze(tf.stack(split_index_list_by_leaf_node))\n",
    "\n",
    "        elif self.function_representation_type >= 3:\n",
    "\n",
    "            split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "            split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "            leaf_classes_num_params = leaf_node_num_ \n",
    "\n",
    "            split_values = parameter_array[:split_values_num_params]\n",
    "            split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "            split_index_array = parameter_array[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "            split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "            split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "            splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "            leaf_classes_array = parameter_array[split_values_num_params+split_index_num_params:]  \n",
    "            split_index_list_by_leaf_node = tf.split(leaf_classes_array, leaf_node_num_)\n",
    "\n",
    "            leaf_classes = tf.squeeze(tf.stack(split_index_list_by_leaf_node))\n",
    "\n",
    "\n",
    "\n",
    "        return splits, leaf_classes\n",
    "\n",
    "\n",
    "    def calculate_function_value_from_vanilla_decision_tree_parameter_single_sample_wrapper(self, \n",
    "                                                                                            internal_nodes, \n",
    "                                                                                            leaf_nodes, \n",
    "                                                                                            leaf_node_num_, \n",
    "                                                                                            internal_node_num_, \n",
    "                                                                                            maximum_depth, \n",
    "                                                                                            number_of_variables):\n",
    "\n",
    "        #self.internal_nodes = tf.cast(self.internal_nodes, tf.float32)\n",
    "        #self.leaf_nodes = tf.cast(self.leaf_nodes, tf.float32)   \n",
    "        \n",
    "        tf.print('internal_nodes', internal_nodes, summarize=-1)\n",
    "        tf.print('leaf_nodes', leaf_nodes, summarize=-1)\n",
    "        def calculate_function_value_from_vanilla_decision_tree_parameter_single_sample(x):\n",
    "\n",
    "            x = tf.cast(x, tf.float32)     \n",
    "            \n",
    "            internal_nodes_split = tf.split(internal_nodes, internal_node_num_)\n",
    "            internal_nodes_split_new = [[] for _ in range(maximum_depth)]\n",
    "            for i, tensor in enumerate(internal_nodes_split):\n",
    "                current_depth = np.ceil(np.log2((i+1)+1)).astype(np.int32)\n",
    "\n",
    "                internal_nodes_split_new[current_depth-1].append(tf.squeeze(tensor, axis=0))\n",
    "\n",
    "            internal_nodes_split = internal_nodes_split_new\n",
    "\n",
    "            split_value_list = []\n",
    "            for i in range(maximum_depth):\n",
    "                current_depth = i+1\n",
    "                num_nodes_current_layer = 2**current_depth - 1 - (2**(current_depth-1) - 1)\n",
    "                split_value_list_per_depth = []\n",
    "                for j in range(num_nodes_current_layer):\n",
    "                    zero_identifier = tf.not_equal(internal_nodes_split[i][j], tf.zeros_like(internal_nodes_split[i][j]))\n",
    "                    split_complete = tf.greater(x, tf.math.sigmoid(internal_nodes_split[i][j]))\n",
    "                    split_value = tf.reduce_any(tf.logical_and(zero_identifier, split_complete))\n",
    "                    split_value_filled = tf.fill( [2**(maximum_depth-current_depth)] , split_value)\n",
    "                    split_value_neg_filled = tf.fill([2**(maximum_depth-current_depth)], tf.logical_not(split_value))\n",
    "                    split_value_list_per_depth.append(tf.keras.backend.flatten(tf.stack([split_value_neg_filled, split_value_filled])))        \n",
    "                split_value_list.append(tf.keras.backend.flatten(tf.stack(split_value_list_per_depth)))\n",
    "\n",
    "            split_values = tf.cast(tf.reduce_all(tf.stack(split_value_list), axis=0), tf.float32)    \n",
    "            leaf_classes = tf.cast(tf.math.sigmoid(leaf_nodes), tf.float32)\n",
    "            #final_class_probability = 1-tf.reduce_max(split_values)                                                                                                                                        \n",
    "            final_class_probability = 1-tf.reduce_max(tf.multiply(leaf_classes, split_values))                                                                                                                                            \n",
    "            return final_class_probability\n",
    "\n",
    "        return calculate_function_value_from_vanilla_decision_tree_parameter_single_sample\n",
    "\n",
    "\n",
    "    def plot(self, normalizer_list=None, path='./dt_plot.png'):\n",
    "        from anytree import Node, RenderTree\n",
    "        from anytree.exporter import DotExporter\n",
    "        \n",
    "\n",
    "        parameter_array = self.apply_activation(self.dt_params)\n",
    "        \n",
    "        splits, leaf_classes = self.get_shaped_parameters_for_decision_tree(parameter_array)\n",
    "\n",
    "        splits = splits.numpy()\n",
    "        leaf_classes = leaf_classes.numpy()\n",
    "\n",
    "\n",
    "        if normalizer_list is not None: \n",
    "            transpose = splits.transpose()\n",
    "            transpose_normalized = []\n",
    "            for i, column in enumerate(transpose):\n",
    "                column_new = column\n",
    "                if len(column_new[column_new != 0]) != 0:\n",
    "                    column_new[column_new != 0] = normalizer_list[i].inverse_transform(column[column != 0].reshape(-1, 1)).ravel()\n",
    "                #column_new = normalizer_list[i].inverse_transform(column.reshape(-1, 1)).ravel()\n",
    "                transpose_normalized.append(column_new)\n",
    "            splits = np.array(transpose_normalized).transpose()\n",
    "\n",
    "        splits_by_layer = []\n",
    "        for i in range(self.depth+1):\n",
    "            start = 2**i - 1\n",
    "            end = 2**(i+1) -1\n",
    "            splits_by_layer.append(splits[start:end])\n",
    "\n",
    "        nodes = {\n",
    "        }\n",
    "        #tree = Tree()\n",
    "        for i, splits in enumerate(splits_by_layer):\n",
    "            for j, split in enumerate(splits):\n",
    "                if i == 0:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, data=None)            \n",
    "                else:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "                    parent_name = 'n' + str(parent_node_id)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)\n",
    "\n",
    "        for j, leaf_class in enumerate(leaf_classes):\n",
    "            i = self.depth\n",
    "            current_node_id = int(2**i - 1 + j)\n",
    "            name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "            parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "            parent_name = 'n' + str(parent_node_id)\n",
    "            #split_variable = np.argmax(np.abs(split))\n",
    "            #split_value = np.round(split[split_variable], 3)\n",
    "            split_description = str(np.round((1-leaf_class), 3))#'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "            nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "            #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)        \n",
    "\n",
    "            DotExporter(nodes['n0'], nodeattrfunc=lambda node: 'label=\"{}\"'.format(node.display_name)).to_picture(path)\n",
    "\n",
    "\n",
    "        return Image(path)#, nodes#nodes#tree        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cd948-2ff4-41bb-8f2e-3c92ab05ae0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7298793a-3131-489d-88c2-8c9da85d6db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.828146Z",
     "iopub.status.busy": "2022-06-04T09:01:36.828042Z",
     "iopub.status.idle": "2022-06-04T09:01:36.890435Z",
     "shell.execute_reply": "2022-06-04T09:01:36.890093Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.828134Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10_000, n_features=5, n_informative=2, n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "#todo: anpassen, dass nur basierend auf train data normalized\n",
    "X, normalizer_list = normalize_real_world_data(X)\n",
    "\n",
    "train_samples = 128#1000  # Samples used for training the models\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    shuffle=False,\n",
    "    test_size=10_000 - train_samples,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46fb31cb-037d-43de-9387-c1eb289a0f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:36.891153Z",
     "iopub.status.busy": "2022-06-04T09:01:36.890926Z",
     "iopub.status.idle": "2022-06-04T09:01:37.007809Z",
     "shell.execute_reply": "2022-06-04T09:01:37.007196Z",
     "shell.execute_reply.started": "2022-06-04T09:01:36.891130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461304700162074"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "model_sklearn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a1f5bb-1e8e-469c-b349-1964adc00cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:37.011410Z",
     "iopub.status.busy": "2022-06-04T09:01:37.011166Z",
     "iopub.status.idle": "2022-06-04T09:01:37.073962Z",
     "shell.execute_reply": "2022-06-04T09:01:37.073364Z",
     "shell.execute_reply.started": "2022-06-04T09:01:37.011373Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29962307, 0.34420182, 0.78918445, 0.32522398, 0.32361171],\n",
       "       [0.48974096, 0.41928906, 0.42401393, 0.66532031, 0.76704839],\n",
       "       [0.65659485, 0.60011619, 0.54951077, 0.48750183, 0.59648464],\n",
       "       ...,\n",
       "       [0.20006322, 0.18973097, 0.53357224, 0.62434102, 0.41132694],\n",
       "       [0.49538044, 0.49099686, 0.68280369, 0.38745923, 0.32775163],\n",
       "       [0.4299303 , 0.40670348, 0.58172411, 0.51258715, 0.43861696]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d043614c-319b-4ce4-b6ec-bda1162fe3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T09:01:37.075175Z",
     "iopub.status.busy": "2022-06-04T09:01:37.074944Z",
     "iopub.status.idle": "2022-06-04T09:01:39.115621Z",
     "shell.execute_reply": "2022-06-04T09:01:39.113500Z",
     "shell.execute_reply.started": "2022-06-04T09:01:37.075139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.147681668 -0.171174631 0.0654113144 ... 0.0471678823 -0.117765374 -0.0936949179]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12a8a5b398a4e589ca0816543357d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_2567093/1609930587.py\", line 163, in forward  *\n        function_values_dhdt = tf.stack(function_values_dhdt)\n\n    InaccessibleTensorError: <tf.Tensor 'while/add_31:0' shape=() dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/add_31:0' shape=() dtype=float32> was defined here:\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/runpy.py\", line 192, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/runpy.py\", line 85, in _run_code\n          exec(code, run_globals)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n          app.start()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n          self.io_loop.start()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/base_events.py\", line 563, in run_forever\n          self._run_once()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/base_events.py\", line 1844, in _run_once\n          handle._run()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/events.py\", line 81, in _run\n          self._context.run(self._callback, *self._args)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n          await self.process_one()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n          await dispatch(*args)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n          await result\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n          reply_content = await reply_content\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n          return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n          result = self._run_cell(\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n          return runner(coro)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n          coro.send(None)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n          if (await self.run_code(code, result,  async_=asy)):\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"/tmp/ipykernel_2567093/2201969884.py\", line 10, in <module>\n          model_dhdt.fit(X_train, y_train, batch_size=64, epochs=10, early_stopping_epochs=50)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 85, in fit\n          current_loss = self.backward(X_batch, y_batch)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 319, in backward\n          predicted = self.forward(x)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 114, in forward\n          if True:\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 126, in forward\n          for entry in X:\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 129, in forward\n          for leaf_index, path in enumerate(paths):\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 159, in forward\n          result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n    \n    The tensor <tf.Tensor 'while/add_31:0' shape=() dtype=float32> cannot be accessed from FuncGraph(name=forward, id=139930238363200), because it was defined in FuncGraph(name=while_body_214, id=139929706349424), which is out of scope.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2567093/2201969884.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             verbosity=1)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_test_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2567093/1609930587.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, epochs, early_stopping_epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2567093/1609930587.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m#tape.watch(self.dt_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_2567093/1609930587.py\", line 163, in forward  *\n        function_values_dhdt = tf.stack(function_values_dhdt)\n\n    InaccessibleTensorError: <tf.Tensor 'while/add_31:0' shape=() dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/add_31:0' shape=() dtype=float32> was defined here:\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/runpy.py\", line 192, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/runpy.py\", line 85, in _run_code\n          exec(code, run_globals)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n          app.launch_new_instance()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n          app.start()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n          self.io_loop.start()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n          self.asyncio_loop.run_forever()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/base_events.py\", line 563, in run_forever\n          self._run_once()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/base_events.py\", line 1844, in _run_once\n          handle._run()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/asyncio/events.py\", line 81, in _run\n          self._context.run(self._callback, *self._args)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n          await self.process_one()\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n          await dispatch(*args)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n          await result\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n          reply_content = await reply_content\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n          res = shell.run_cell(code, store_history=store_history, silent=silent)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n          return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n          result = self._run_cell(\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n          return runner(coro)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n          coro.send(None)\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n          if (await self.run_code(code, result,  async_=asy)):\n        File \"/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"/tmp/ipykernel_2567093/2201969884.py\", line 10, in <module>\n          model_dhdt.fit(X_train, y_train, batch_size=64, epochs=10, early_stopping_epochs=50)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 85, in fit\n          current_loss = self.backward(X_batch, y_batch)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 319, in backward\n          predicted = self.forward(x)\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 114, in forward\n          if True:\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 126, in forward\n          for entry in X:\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 129, in forward\n          for leaf_index, path in enumerate(paths):\n        File \"/tmp/ipykernel_2567093/1609930587.py\", line 159, in forward\n          result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n    \n    The tensor <tf.Tensor 'while/add_31:0' shape=() dtype=float32> cannot be accessed from FuncGraph(name=forward, id=139930238363200), because it was defined in FuncGraph(name=while_body_214, id=139929706349424), which is out of scope.\n"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-2,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=10, early_stopping_epochs=50)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a1b12-9086-4a12-9582-f03233cd35b2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.116781Z",
     "iopub.status.idle": "2022-06-04T09:01:39.117005Z",
     "shell.execute_reply": "2022-06-04T09:01:39.116903Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.116889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_array = model_dhdt.dt_params\n",
    "parameter_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa73b83-1b2e-4797-9a20-18fa2d5e19eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.118718Z",
     "iopub.status.idle": "2022-06-04T09:01:39.118925Z",
     "shell.execute_reply": "2022-06-04T09:01:39.118817Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.118807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_node_num_ = model_dhdt.internal_node_num_\n",
    "leaf_node_num_ = model_dhdt.leaf_node_num_\n",
    "\n",
    "split_values_num_params = model_dhdt.number_of_variables * internal_node_num_\n",
    "split_index_num_params = model_dhdt.number_of_variables * internal_node_num_\n",
    "leaf_classes_num_params = leaf_node_num_ \n",
    "\n",
    "split_values = parameter_array[:split_values_num_params]\n",
    "split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "split_index_array = parameter_array[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "leaf_classes_array = parameter_array[split_values_num_params+split_index_num_params:]  \n",
    "split_index_list_by_leaf_node = tf.split(leaf_classes_array, leaf_node_num_)\n",
    "\n",
    "print(split_values)\n",
    "print(split_index_array)\n",
    "\n",
    "print(leaf_classes_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d38e5-05e0-4723-ba52-89667281a697",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.120130Z",
     "iopub.status.idle": "2022-06-04T09:01:39.120301Z",
     "shell.execute_reply": "2022-06-04T09:01:39.120221Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.120212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "split_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d30d1-274b-42ef-994e-13207f9dff05",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.121588Z",
     "iopub.status.idle": "2022-06-04T09:01:39.122067Z",
     "shell.execute_reply": "2022-06-04T09:01:39.121938Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.121922Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_index_array * tf.reshape(split_values, (internal_node_num_, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3307ab3-4ad0-48d3-a1bd-f152207ceb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49446f-f80d-4c87-9d79-a48f97809e4f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.122880Z",
     "iopub.status.idle": "2022-06-04T09:01:39.123061Z",
     "shell.execute_reply": "2022-06-04T09:01:39.122968Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.122959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_values_selected = tf.reduce_sum(split_index_array * tf.reshape(split_values, (internal_node_num_, -1)), axis=1)+0.4\n",
    "split_values_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896548f0-31ed-4e70-839c-a26806c13845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1516863-aa3e-48ce-b2f2-72cd33a20d2c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.124289Z",
     "iopub.status.idle": "2022-06-04T09:01:39.124487Z",
     "shell.execute_reply": "2022-06-04T09:01:39.124381Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.124373Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_extended = []\n",
    "for entry in X_train[:3]:\n",
    "    X_train_extended.append([entry]*internal_node_num_)\n",
    "X_train_extended = np.array(X_train_extended)\n",
    "X_train_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcd8c7-0af2-42fe-9837-7fa249f4ac10",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.125324Z",
     "iopub.status.idle": "2022-06-04T09:01:39.125502Z",
     "shell.execute_reply": "2022-06-04T09:01:39.125411Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.125403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_extended_reduced = tf.reduce_sum(split_index_array * X_train_extended, axis=2)\n",
    "X_train_extended_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc84c3-bd91-4dfa-8e68-2780f8637d8c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.126769Z",
     "iopub.status.idle": "2022-06-04T09:01:39.126963Z",
     "shell.execute_reply": "2022-06-04T09:01:39.126857Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.126848Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_results = tf.round(tf.sigmoid(X_train_extended_reduced/2 - split_values_selected))\n",
    "split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73897e16-03d9-4ad0-adb9-4bfaeede30d1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.127821Z",
     "iopub.status.idle": "2022-06-04T09:01:39.127992Z",
     "shell.execute_reply": "2022-06-04T09:01:39.127903Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.127895Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.greater(X_train_extended_reduced/2, split_values_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd38008-3eff-4be5-99a5-042c16a3aee1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.129376Z",
     "iopub.status.idle": "2022-06-04T09:01:39.129573Z",
     "shell.execute_reply": "2022-06-04T09:01:39.129466Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.129457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086edb0c-421c-4522-b8a4-12d8924570c0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.130307Z",
     "iopub.status.idle": "2022-06-04T09:01:39.130468Z",
     "shell.execute_reply": "2022-06-04T09:01:39.130389Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.130381Z"
    }
   },
   "outputs": [],
   "source": [
    "print(split_values)\n",
    "print(split_index_array)\n",
    "\n",
    "print(leaf_classes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf177660-1415-4d12-a5bc-1a4c531551bb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.131502Z",
     "iopub.status.idle": "2022-06-04T09:01:39.131668Z",
     "shell.execute_reply": "2022-06-04T09:01:39.131587Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.131579Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_index_array = parameter_array[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "split_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ad08f-364f-4119-9a8a-976407d52642",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.133324Z",
     "iopub.status.idle": "2022-06-04T09:01:39.133526Z",
     "shell.execute_reply": "2022-06-04T09:01:39.133416Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.133408Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "split_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8556c6-5dad-4c96-b83e-21395506b5d6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.134359Z",
     "iopub.status.idle": "2022-06-04T09:01:39.134534Z",
     "shell.execute_reply": "2022-06-04T09:01:39.134447Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.134438Z"
    }
   },
   "outputs": [],
   "source": [
    "split_index_for_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c704707-da5b-4033-a9f0-68f1d458325d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.135764Z",
     "iopub.status.idle": "2022-06-04T09:01:39.136242Z",
     "shell.execute_reply": "2022-06-04T09:01:39.136109Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.136093Z"
    }
   },
   "outputs": [],
   "source": [
    "split_values_for_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a6c33-5622-4258-bbc1-9a89768eaa76",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.136950Z",
     "iopub.status.idle": "2022-06-04T09:01:39.137114Z",
     "shell.execute_reply": "2022-06-04T09:01:39.137039Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.137030Z"
    }
   },
   "outputs": [],
   "source": [
    "split_index_array[internal_node_num_*internal_index:internal_node_num_*(internal_index+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b376d-185d-46a2-9f23-3360f0e2c827",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.138197Z",
     "iopub.status.idle": "2022-06-04T09:01:39.138393Z",
     "shell.execute_reply": "2022-06-04T09:01:39.138286Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.138278Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb8bbd-2bf8-42d7-8648-78d68e617f63",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.139452Z",
     "iopub.status.idle": "2022-06-04T09:01:39.139634Z",
     "shell.execute_reply": "2022-06-04T09:01:39.139543Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.139535Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entry in X_train[:3]:\n",
    "    \n",
    "    path_list = []\n",
    "    #add_factor == 0\n",
    "    leaf_counter = 0\n",
    "    internal_counter = 0\n",
    "    for i in range(depth):\n",
    "        print('i', i)\n",
    "        internal_index = 2**(i)-1+internal_counter\n",
    "        print('internal_index', internal_index)\n",
    "        \n",
    "        split_index_for_internal = np.argmax(split_index_array[5*internal_index:5*(internal_index+1)])\n",
    "        split_values_for_internal = split_values[5*internal_index+split_index_for_internal]\n",
    "        \n",
    "        entry_for_internal = entry[split_index_for_internal]\n",
    "        \n",
    "        \n",
    "        value = tf.round(tf.sigmoid(entry_for_internal - split_values_for_internal))\n",
    "        print('value', value)\n",
    "        if value == 0:\n",
    "            leaf_counter += 2**(depth-i-1)\n",
    "            internal_counter = internal_counter ** 2 + 1\n",
    "            print('internal_counter', internal_counter)\n",
    "            #add_factor += 0\n",
    "        #else:\n",
    "            #add_factor += 1\n",
    "\n",
    "            #print(value)\n",
    "    print(leaf_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa283c92-8cd9-480c-a70f-8d7733c8c202",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.140755Z",
     "iopub.status.idle": "2022-06-04T09:01:39.140945Z",
     "shell.execute_reply": "2022-06-04T09:01:39.140842Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.140833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split_result in split_results:\n",
    "    \n",
    "    path_list = []\n",
    "    #add_factor == 0\n",
    "    leaf_counter = 0\n",
    "    internal_counter = 0\n",
    "    for i in range(depth):\n",
    "        print('i', i)\n",
    "        for j in (2):#range(2**(i)):\n",
    "            print('i+j', 2**(i)-1+j)\n",
    "            value = split_result[2**(i)-1+j+internal_counter]\n",
    "            print('value', value)\n",
    "            if value == 0:\n",
    "                print(depth-i-1)\n",
    "                leaf_counter += 2**(depth-i-1)\n",
    "                internal_counter = internal_counter\n",
    "                #add_factor += 0\n",
    "            #else:\n",
    "                #add_factor += 1\n",
    "\n",
    "            #print(value)\n",
    "    print(leaf_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101f22f-1b2e-4a67-b2d7-e625a77544d4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.141927Z",
     "iopub.status.idle": "2022-06-04T09:01:39.142119Z",
     "shell.execute_reply": "2022-06-04T09:01:39.142015Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.142006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth = 3\n",
    "\n",
    "for split_result in split_results:\n",
    "\n",
    "    tree_extended = []\n",
    "    for i in range(depth):\n",
    "        duplicate_factor = 2**(depth-i)//2\n",
    "        row = []\n",
    "        for j in range(2**(i)):\n",
    "            value = split_result[2**(i)-1+j]\n",
    "            inverse_value = 1-split_result[2**(i)-1+j]\n",
    "\n",
    "            row.extend([value]*duplicate_factor)\n",
    "            row.extend([inverse_value]*duplicate_factor)\n",
    "            #for _ in range(duplicate_factor):\n",
    "            #    row.extend([value, inverse_value])\n",
    "        #print(tf.stack(row))\n",
    "        tree_extended.append(tf.stack(row))\n",
    "    tree_extended = tf.stack(tree_extended)\n",
    "    print(tree_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c9a1c-dfa1-43b5-9340-b933d3ff1f0f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.143444Z",
     "iopub.status.idle": "2022-06-04T09:01:39.143644Z",
     "shell.execute_reply": "2022-06-04T09:01:39.143533Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.143524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da738503-75e8-4c66-9623-469df7fa77ea",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.144695Z",
     "iopub.status.idle": "2022-06-04T09:01:39.144889Z",
     "shell.execute_reply": "2022-06-04T09:01:39.144783Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.144775Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84792d97-8f00-4d57-9a60-56d757bf29f3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.145779Z",
     "iopub.status.idle": "2022-06-04T09:01:39.145969Z",
     "shell.execute_reply": "2022-06-04T09:01:39.145865Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.145856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_results[:,i+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602543ad-76ea-442e-9657-1eeddd279c59",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.147008Z",
     "iopub.status.idle": "2022-06-04T09:01:39.147198Z",
     "shell.execute_reply": "2022-06-04T09:01:39.147094Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.147085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd681c1-f323-4dd0-bdd9-bc1714a0038d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.148022Z",
     "iopub.status.idle": "2022-06-04T09:01:39.148211Z",
     "shell.execute_reply": "2022-06-04T09:01:39.148122Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.148113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09498c4f-f078-457e-927f-965a5e741e72",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.149296Z",
     "iopub.status.idle": "2022-06-04T09:01:39.149493Z",
     "shell.execute_reply": "2022-06-04T09:01:39.149386Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.149377Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d1e81-34f1-4805-8b0b-bf0a6c6d7dc3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.150687Z",
     "iopub.status.idle": "2022-06-04T09:01:39.150882Z",
     "shell.execute_reply": "2022-06-04T09:01:39.150775Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.150766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.stack([value]*4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c715a8-ca99-483e-b1bc-c2fbaea88c77",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.151863Z",
     "iopub.status.idle": "2022-06-04T09:01:39.152058Z",
     "shell.execute_reply": "2022-06-04T09:01:39.151952Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.151944Z"
    }
   },
   "outputs": [],
   "source": [
    "value_extended = tf.stack([value]*duplicate_factor, axis=1)\n",
    "inverse_value_extended = tf.stack([inverse_value]*duplicate_factor, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e0671-c52d-468a-89e4-ad14e90ddd89",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.153135Z",
     "iopub.status.idle": "2022-06-04T09:01:39.153331Z",
     "shell.execute_reply": "2022-06-04T09:01:39.153222Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.153214Z"
    }
   },
   "outputs": [],
   "source": [
    "value_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f1613-4687-4566-9d02-caffc3ff2132",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.153912Z",
     "iopub.status.idle": "2022-06-04T09:01:39.154061Z",
     "shell.execute_reply": "2022-06-04T09:01:39.153990Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.153982Z"
    }
   },
   "outputs": [],
   "source": [
    "inverse_value_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5162a13-1a57-415c-bf23-9bff581cc26b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.155149Z",
     "iopub.status.idle": "2022-06-04T09:01:39.155316Z",
     "shell.execute_reply": "2022-06-04T09:01:39.155236Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.155228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_values = tf.concat([value_extended, inverse_value_extended], axis=1)\n",
    "new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673e8cc-08d9-4092-9ed8-6133770b0178",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.156391Z",
     "iopub.status.idle": "2022-06-04T09:01:39.156576Z",
     "shell.execute_reply": "2022-06-04T09:01:39.156480Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.156472Z"
    }
   },
   "outputs": [],
   "source": [
    "new_values = tf.concat([value_extended, inverse_value_extended], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8691a-5734-4b16-914d-a8595f42a158",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.157504Z",
     "iopub.status.idle": "2022-06-04T09:01:39.157673Z",
     "shell.execute_reply": "2022-06-04T09:01:39.157587Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.157579Z"
    }
   },
   "outputs": [],
   "source": [
    "new_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645dc66-bba9-433a-a13a-a5a703f28498",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.158714Z",
     "iopub.status.idle": "2022-06-04T09:01:39.158884Z",
     "shell.execute_reply": "2022-06-04T09:01:39.158799Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.158790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.reshape(tf.constant([], tf.float32), shape=(3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90d96-e4d0-4eab-a113-2cb863aa438a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.159802Z",
     "iopub.status.idle": "2022-06-04T09:01:39.159963Z",
     "shell.execute_reply": "2022-06-04T09:01:39.159884Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.159876Z"
    }
   },
   "outputs": [],
   "source": [
    "new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef26503-2db7-4411-bee2-24e9184252e1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.161138Z",
     "iopub.status.idle": "2022-06-04T09:01:39.161332Z",
     "shell.execute_reply": "2022-06-04T09:01:39.161229Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.161220Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.concat([tf.reshape(tf.constant([], tf.float32), shape=(3,0)), new_values], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7ef61-05e2-4379-a7c6-95ae4287f016",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.162197Z",
     "iopub.status.idle": "2022-06-04T09:01:39.162371Z",
     "shell.execute_reply": "2022-06-04T09:01:39.162281Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.162273Z"
    }
   },
   "outputs": [],
   "source": [
    "new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3de252-1c2d-4fde-83ee-f7c7763a275e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.163221Z",
     "iopub.status.idle": "2022-06-04T09:01:39.163380Z",
     "shell.execute_reply": "2022-06-04T09:01:39.163302Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.163294Z"
    }
   },
   "outputs": [],
   "source": [
    "split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "split_values_selected = tf.reduce_sum(split_index_array * tf.reshape(split_values, (internal_node_num_, -1)), axis=1)\n",
    "\n",
    "X_train_extended = []\n",
    "for entry in X_train[:3]:\n",
    "    X_train_extended.append([entry]*internal_node_num_)\n",
    "X_train_extended = np.array(X_train_extended)\n",
    "X_train_extended_reduced = tf.reduce_sum(split_index_array * X_train_extended, axis=2)\n",
    "\n",
    "split_results = tf.round(tf.sigmoid(X_train_extended_reduced/2 - split_values_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f725e-b594-4648-837b-1131788e0a26",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.164353Z",
     "iopub.status.idle": "2022-06-04T09:01:39.164522Z",
     "shell.execute_reply": "2022-06-04T09:01:39.164442Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.164433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tree_extended = []\n",
    "for i in range(depth):\n",
    "    duplicate_factor = 2**(depth-i)//2\n",
    "    row = tf.reshape(tf.constant([], tf.float32), shape=(split_results.shape[0],0))\n",
    "    for j in range(2**(i)):\n",
    "        value = split_results[:,i+j]\n",
    "        inverse_value = 1-split_results[:,i+j]\n",
    "\n",
    "        #row.extend(tf.stack([value]*duplicate_factor, axis=1))\n",
    "        #row.extend(tf.stack([inverse_value]*duplicate_factor, axis=1))\n",
    "        \n",
    "        value_extended = tf.stack([value]*duplicate_factor, axis=1)\n",
    "        inverse_value_extended = tf.stack([inverse_value]*duplicate_factor, axis=1)\n",
    "        \n",
    "        new_values = tf.concat([value_extended, inverse_value_extended], axis=1)\n",
    "        \n",
    "        row = tf.concat([row, new_values], axis=1)\n",
    "        #row = tf.stack(value_extended, inverse_value_extended)\n",
    "        #for _ in range(duplicate_factor):\n",
    "        #    row.extend([value, inverse_value])\n",
    "    #print(row)\n",
    "    #print(tf.stack(row))\n",
    "    tree_extended.append(tf.stack(row))\n",
    "tree_extended = tf.stack(tree_extended)\n",
    "tree_extended = tf.transpose(tree_extended, perm=[1,0,2])\n",
    "print(tree_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0babc-dd75-493f-9627-7bbe680776a2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.165642Z",
     "iopub.status.idle": "2022-06-04T09:01:39.165831Z",
     "shell.execute_reply": "2022-06-04T09:01:39.165725Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.165717Z"
    }
   },
   "outputs": [],
   "source": [
    "leaf_classes_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268788b2-2f01-4a3b-8f3f-b6999f88fc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc4c04-7186-4318-b1c7-166aa813d8f9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.166769Z",
     "iopub.status.idle": "2022-06-04T09:01:39.166951Z",
     "shell.execute_reply": "2022-06-04T09:01:39.166855Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.166847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_leaf_identifier = tf.reduce_prod(tree_extended, axis=1)\n",
    "#print(tree_leaf_identifier)\n",
    "tree_leaf_output = tree_leaf_identifier * leaf_classes_array\n",
    "#print(tree_leaf_identifier_output)\n",
    "y_pred = tf.reduce_max(tree_leaf_output, axis= 1)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acdc19-dc47-459e-954a-2521607fb5b3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.167842Z",
     "iopub.status.idle": "2022-06-04T09:01:39.168003Z",
     "shell.execute_reply": "2022-06-04T09:01:39.167924Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.167917Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_extended = []\n",
    "for i in range(depth):\n",
    "    duplicate_factor = 2**(depth-i)//2\n",
    "    row = tf.reshape(tf.constant([], tf.float32), shape=(split_results.shape[0],0))\n",
    "    for j in range(2**(i)):\n",
    "        value = split_results[:,i+j]\n",
    "        inverse_value = 1-split_results[:,i+j]\n",
    "\n",
    "        #row.extend(tf.stack([value]*duplicate_factor, axis=1))\n",
    "        #row.extend(tf.stack([inverse_value]*duplicate_factor, axis=1))\n",
    "        \n",
    "        value_extended = tf.stack([value]*duplicate_factor, axis=1)\n",
    "        inverse_value_extended = tf.stack([inverse_value]*duplicate_factor, axis=1)\n",
    "        \n",
    "        new_values = tf.concat([value_extended, inverse_value_extended], axis=1)\n",
    "        \n",
    "        row = tf.concat([row, new_values], axis=1)\n",
    "        #row = tf.stack(value_extended, inverse_value_extended)\n",
    "        #for _ in range(duplicate_factor):\n",
    "        #    row.extend([value, inverse_value])\n",
    "    #print(row)\n",
    "    #print(tf.stack(row))\n",
    "    tree_extended.append(tf.stack(row))\n",
    "tree_extended = tf.stack(tree_extended)\n",
    "tree_extended = tf.transpose(tree_extended, perm=[1,0,2])\n",
    "print(tree_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e1838-5ba0-41e7-b9d3-5328dff8c2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d8492-e106-4b06-a0fa-824ccb58d31e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.169241Z",
     "iopub.status.idle": "2022-06-04T09:01:39.169435Z",
     "shell.execute_reply": "2022-06-04T09:01:39.169330Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.169322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.reshape(tree_extended, (3,3,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936955e-045c-4252-a88e-11f37e6cb526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e95644-0bd4-4273-ba72-6a283887907b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.170267Z",
     "iopub.status.idle": "2022-06-04T09:01:39.170452Z",
     "shell.execute_reply": "2022-06-04T09:01:39.170350Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.170342Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "internal_nodes_extended = model_dhdt.get_shaped_parameters_for_decision_tree(model_dhdt.dt_params)[0]\n",
    "internal_nodes_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f0e9f-f882-459d-b710-e06ac2cc11e5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.171105Z",
     "iopub.status.idle": "2022-06-04T09:01:39.171253Z",
     "shell.execute_reply": "2022-06-04T09:01:39.171179Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.171171Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_paths --> go through paths to finde true path --> get leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aed819-aa4a-4b09-9c9b-571404804314",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.172213Z",
     "iopub.status.idle": "2022-06-04T09:01:39.172377Z",
     "shell.execute_reply": "2022-06-04T09:01:39.172298Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.172289Z"
    }
   },
   "outputs": [],
   "source": [
    "for tensor in tf.unstack(internal_nodes_extended):\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bd1ec-8152-4d75-a886-49d81571f137",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.173579Z",
     "iopub.status.idle": "2022-06-04T09:01:39.173769Z",
     "shell.execute_reply": "2022-06-04T09:01:39.173666Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.173658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-2,\n",
    "            loss='mae',#'binary_crossentropy',\n",
    "            random_seed=42,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=2, early_stopping_epochs=50)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa034252-a9b6-4e75-8522-4e4b95c93c92",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.174679Z",
     "iopub.status.idle": "2022-06-04T09:01:39.174853Z",
     "shell.execute_reply": "2022-06-04T09:01:39.174762Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.174754Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-2,\n",
    "            loss='mae',#'binary_crossentropy',\n",
    "            random_seed=42,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=10, early_stopping_epochs=50)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0dfba-21f2-4c6b-9c34-e836e7dadf9d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T09:01:39.175710Z",
     "iopub.status.idle": "2022-06-04T09:01:39.175869Z",
     "shell.execute_reply": "2022-06-04T09:01:39.175792Z",
     "shell.execute_reply.started": "2022-06-04T09:01:39.175784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "image = model_dhdt.plot()\n",
    "display(image)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(model_sklearn, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496ff4f-2a6f-4315-86fe-b9065042722b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb7ba1-0892-4f4d-b5b5-ded8137595e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
