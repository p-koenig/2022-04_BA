{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:04.506671Z",
     "iopub.status.busy": "2022-06-04T15:02:04.506404Z",
     "iopub.status.idle": "2022-06-04T15:02:09.295379Z",
     "shell.execute_reply": "2022-06-04T15:02:09.294284Z",
     "shell.execute_reply.started": "2022-06-04T15:02:04.506589Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '' #'true'\n",
    "\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def sigmoid_squeeze(x, factor=3):\n",
    "    x = 1/(1+K.exp(-factor*x))\n",
    "    return x  \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa08eca3-ed45-4eca-bd81-d8d679cb13bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.297349Z",
     "iopub.status.busy": "2022-06-04T15:02:09.297205Z",
     "iopub.status.idle": "2022-06-04T15:02:09.300881Z",
     "shell.execute_reply": "2022-06-04T15:02:09.300292Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.297326Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0c3ac7-7321-4224-8ffd-b103b1b669fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.301872Z",
     "iopub.status.busy": "2022-06-04T15:02:09.301757Z",
     "iopub.status.idle": "2022-06-04T15:02:09.378249Z",
     "shell.execute_reply": "2022-06-04T15:02:09.377692Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.301859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_real_world_data(X_data):\n",
    "    normalizer_list = []\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        for column_name in X_data:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_data[column_name].values.reshape(-1, 1))\n",
    "            X_data[column_name] = scaler.transform(X_data[column_name].values.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "    else:\n",
    "        for i, column in enumerate(X_data.T):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(column.reshape(-1, 1))\n",
    "            X_data[:,i] = scaler.transform(column.reshape(-1, 1)).ravel()\n",
    "            normalizer_list.append(scaler)\n",
    "        \n",
    "    return X_data, normalizer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df68f3-1897-43c0-929e-33c3ca4442dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f2e596-9bb2-4026-890e-7b42365e81f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.379751Z",
     "iopub.status.busy": "2022-06-04T15:02:09.379635Z",
     "iopub.status.idle": "2022-06-04T15:02:09.445192Z",
     "shell.execute_reply": "2022-06-04T15:02:09.444456Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.379739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHDT(tf.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            squeeze_factor = 5,\n",
    "            learning_rate=1e-3,\n",
    "            loss='binary_crossentropy',#'mae',\n",
    "            optimizer = 'adam',\n",
    "            random_seed=42,\n",
    "            verbosity=1):    \n",
    "        \n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = tf.keras.losses.get(loss)\n",
    "        self.seed = random_seed\n",
    "        self.verbosity = verbosity\n",
    "        self.number_of_variables = number_of_variables\n",
    "        self.squeeze_factor = squeeze_factor\n",
    "        \n",
    "        self.internal_node_num_ = 2 ** self.depth - 1 \n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "        \n",
    "        tf.random.set_seed(self.seed)\n",
    "                        \n",
    "        maximum_depth = self.depth\n",
    "        leaf_node_num_ = 2 ** maximum_depth\n",
    "        internal_node_num_ = 2 ** maximum_depth - 1\n",
    "                \n",
    "        #internal_nodes, leaf_nodes = self.get_shaped_parameters_for_decision_tree(dt_params_activation)\n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_         \n",
    "        \n",
    "        self.split_values = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_values_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_values')\n",
    "        self.split_index_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_index_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_index_array')\n",
    "        self.leaf_classes_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(leaf_classes_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='leaf_classes_array')\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        self.optimizer.learning_rate = self.learning_rate\n",
    "                \n",
    "        self.plotlosses = PlotLosses()    \n",
    "        \n",
    "    def fit(self, X_train, y_train, batch_size=32, epochs=100, early_stopping_epochs=5, valid_data=None):\n",
    "        \n",
    "        minimum_loss_epoch = np.inf\n",
    "        epochs_without_improvement = 0        \n",
    "        \n",
    "        for current_epoch in tqdm(range(epochs)):\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            X_train = tf.random.shuffle(X_train, seed=self.seed + current_epoch)\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            y_train = tf.random.shuffle(y_train, seed=self.seed + current_epoch)\n",
    "            \n",
    "            loss_list = []\n",
    "            for index, (X_batch, y_batch) in enumerate(zip(make_batch(X_train, batch_size), make_batch(y_train, batch_size))):\n",
    "                current_loss = self.backward(X_batch, y_batch)\n",
    "                loss_list.append(float(current_loss))\n",
    "                \n",
    "                if self.verbosity >= 2:\n",
    "                    batch_idx = (index+1)*batch_size\n",
    "                    msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    print(msg.format(current_epoch, batch_idx, current_loss))                   \n",
    "                  \n",
    "            current_loss_epoch = np.mean(loss_list)\n",
    "            if self.verbosity > 0:    \n",
    "                msg = \"Epoch: {:02d} | Loss: {:.5f} |\"\n",
    "                print(msg.format(current_epoch, current_loss_epoch))              \n",
    "\n",
    "            \n",
    "            if False:\n",
    "                loss_dict = {'loss': current_loss_epoch}\n",
    "\n",
    "                loss_dict['acc'] = accuracy_score(y_train, np.round(self.forward_hard(X_train)))\n",
    "                if valid_data is not None:\n",
    "                    loss_dict['val_loss'] = self.loss(valid_data[1], self.forward(valid_data[0]))\n",
    "                    loss_dict['val_acc'] = accuracy_score(valid_data[1], np.round(self.forward_hard(valid_data[0])))\n",
    "\n",
    "                self.plotlosses.update(loss_dict)#({'acc': 0.0, 'val_acc': 0.0, 'loss': np.mean(loss_list), 'val_loss': 0.0})\n",
    "                self.plotlosses.send()            \n",
    "\n",
    "            if current_loss_epoch < minimum_loss_epoch:\n",
    "                minimum_loss_epoch = current_loss_epoch\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= early_stopping_epochs:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    #split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    split_index = tfa.activations.sparsemax(100 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward_hard(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    #split_index = tfa.activations.sparsemax(10 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    #split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.round(tf.sigmoid(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return tf.sigmoid(self.forward_hard(X))\n",
    "        \n",
    "    def backward(self, x,y):\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)#tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            predicted = self.forward(x)\n",
    "            if self.loss.__name__  == 'binary_crossentropy':\n",
    "                current_loss = self.loss(y, predicted, from_logits=True)\n",
    "            else:\n",
    "                current_loss = self.loss(y, predicted, from_logits=True)\n",
    "        #tf.print('predicted', predicted)\n",
    "        #tf.print('current_loss', current_loss, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.leaf_classes_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.leaf_classes_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)        \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.split_values)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_values]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.split_index_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_index_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "\n",
    "        #                          global_step=tf.compat.v1.train.get_or_create_global_step())     \n",
    "        \n",
    "        return current_loss\n",
    "        \n",
    "    def plot(self, normalizer_list=None, path='./dt_plot.png'):\n",
    "        from anytree import Node, RenderTree\n",
    "        from anytree.exporter import DotExporter\n",
    "\n",
    "        internal_node_num_ = 2 ** self.depth - 1 \n",
    "        \n",
    "        #split_values = self.split_values\n",
    "        split_values = sigmoid_squeeze(self.split_values, self.squeeze_factor)\n",
    "        split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "        split_index_array = self.split_index_array \n",
    "        split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "        split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "        splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "        \n",
    "        splits = splits.numpy()\n",
    "        leaf_classes = tf.sigmoid(self.leaf_classes_array).numpy()\n",
    "\n",
    "\n",
    "        if normalizer_list is not None: \n",
    "            transpose = splits.transpose()\n",
    "            transpose_normalized = []\n",
    "            for i, column in enumerate(transpose):\n",
    "                column_new = column\n",
    "                if len(column_new[column_new != 0]) != 0:\n",
    "                    column_new[column_new != 0] = normalizer_list[i].inverse_transform(column[column != 0].reshape(-1, 1)).ravel()\n",
    "                #column_new = normalizer_list[i].inverse_transform(column.reshape(-1, 1)).ravel()\n",
    "                transpose_normalized.append(column_new)\n",
    "            splits = np.array(transpose_normalized).transpose()\n",
    "\n",
    "        splits_by_layer = []\n",
    "        for i in range(self.depth+1):\n",
    "            start = 2**i - 1\n",
    "            end = 2**(i+1) -1\n",
    "            splits_by_layer.append(splits[start:end])\n",
    "\n",
    "        nodes = {\n",
    "        }\n",
    "        #tree = Tree()\n",
    "        for i, splits in enumerate(splits_by_layer):\n",
    "            for j, split in enumerate(splits):\n",
    "                if i == 0:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, data=None)            \n",
    "                else:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "                    parent_name = 'n' + str(parent_node_id)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)\n",
    "\n",
    "        for j, leaf_class in enumerate(leaf_classes):\n",
    "            i = self.depth\n",
    "            current_node_id = int(2**i - 1 + j)\n",
    "            name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "            parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "            parent_name = 'n' + str(parent_node_id)\n",
    "            #split_variable = np.argmax(np.abs(split))\n",
    "            #split_value = np.round(split[split_variable], 3)\n",
    "            split_description = str(np.round((leaf_class), 3))#'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "            nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "            #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)        \n",
    "\n",
    "            DotExporter(nodes['n0'], nodeattrfunc=lambda node: 'label=\"{}\"'.format(node.display_name)).to_picture(path)\n",
    "\n",
    "\n",
    "        return Image(path)#, nodes#nodes#tree        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca95a2e-38a4-49da-8515-e4df6cd002bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.446269Z",
     "iopub.status.busy": "2022-06-04T15:02:09.446155Z",
     "iopub.status.idle": "2022-06-04T15:02:09.491349Z",
     "shell.execute_reply": "2022-06-04T15:02:09.490407Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.446256Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHDT(tf.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=3,\n",
    "            function_representation_type = 3,\n",
    "            number_of_variables = 5,\n",
    "            squeeze_factor = 5,\n",
    "            learning_rate=1e-3,\n",
    "            loss='binary_crossentropy',#'mae',\n",
    "            optimizer = 'adam',\n",
    "            random_seed=42,\n",
    "            verbosity=1):    \n",
    "        \n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = tf.keras.losses.get(loss)\n",
    "        self.seed = random_seed\n",
    "        self.verbosity = verbosity\n",
    "        self.function_representation_type = function_representation_type\n",
    "        self.number_of_variables = number_of_variables\n",
    "        self.squeeze_factor = squeeze_factor\n",
    "        \n",
    "        self.internal_node_num_ = 2 ** self.depth - 1 \n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "        \n",
    "        tf.random.set_seed(self.seed)\n",
    "        \n",
    "        function_representation_length = ( \n",
    "          (2 ** self.depth - 1) * 2 + (2 ** self.depth)  if self.function_representation_type == 1 \n",
    "          else (2 ** self.depth - 1) + ((2 ** self.depth - 1) * self.number_of_variables) + (2 ** self.depth) if self.function_representation_type == 2 \n",
    "          else ((2 ** self.depth - 1) * self.number_of_variables * 2) + (2 ** self.depth)  if self.function_representation_type >= 3 \n",
    "          else None\n",
    "                                      )        \n",
    "        \n",
    "        self.dt_params =  tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(function_representation_length,)),\n",
    "                                      trainable=True,\n",
    "                                      name='dt_params')\n",
    "        \n",
    "        tf.print(self.dt_params)\n",
    "        \n",
    "        maximum_depth = self.depth\n",
    "        leaf_node_num_ = 2 ** maximum_depth\n",
    "        internal_node_num_ = 2 ** maximum_depth - 1\n",
    "        \n",
    "        #dt_params_activation = self.dt_params#self.apply_activation(self.dt_params)\n",
    "        \n",
    "        #internal_nodes, leaf_nodes = self.get_shaped_parameters_for_decision_tree(dt_params_activation)\n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_         \n",
    "        \n",
    "        self.split_values = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_values_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_values')\n",
    "        #tf.sigmoid(self.dt_params[:split_values_num_params])\n",
    "        self.split_index_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_index_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_index_array')\n",
    "        #self.dt_params[split_values_num_params:split_values_num_params+split_index_num_params]    \n",
    "        self.leaf_classes_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(leaf_classes_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='leaf_classes_array')\n",
    "        #tf.sigmoid(self.dt_params[split_values_num_params+split_index_num_params:])        \n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        self.optimizer.learning_rate = self.learning_rate\n",
    "                \n",
    "    def fit(self, X, y, batch_size=32, epochs=100, early_stopping_epochs=5):\n",
    "        \n",
    "        minimum_loss_epoch = np.inf\n",
    "        epochs_without_improvement = 0        \n",
    "        \n",
    "        for current_epoch in tqdm(range(epochs)):\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            X = tf.random.shuffle(X, seed=self.seed + current_epoch)\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            y = tf.random.shuffle(y, seed=self.seed + current_epoch)\n",
    "            \n",
    "            loss_list = []\n",
    "            for index, (X_batch, y_batch) in enumerate(zip(make_batch(X, batch_size), make_batch(y, batch_size))):\n",
    "                current_loss = self.backward(X_batch, y_batch)\n",
    "                loss_list.append(float(current_loss))\n",
    "                \n",
    "                if self.verbosity >= 2:\n",
    "                    batch_idx = (index+1)*batch_size\n",
    "                    msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    print(msg.format(current_epoch, batch_idx, current_loss))                   \n",
    "                  \n",
    "            if self.verbosity > 0:    \n",
    "                msg = \"Epoch: {:02d} | Loss: {:.5f} |\"\n",
    "                print(msg.format(current_epoch, np.mean(loss_list)))              \n",
    "            \n",
    "            current_loss_epoch = np.mean(loss_list)\n",
    "\n",
    "            if current_loss_epoch < minimum_loss_epoch:\n",
    "                minimum_loss_epoch = current_loss_epoch\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= early_stopping_epochs:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    #split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    split_index = tfa.activations.sparsemax(10 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "    def predict(self, X):\n",
    "        return tf.sigmoid(self.forward(X))\n",
    "        \n",
    "    def backward(self, x,y):\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)#tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            #tape.watch(self.dt_params)\n",
    "            predicted = self.forward(x)\n",
    "            current_loss = self.loss(y, predicted, from_logits=True)\n",
    "            \n",
    "        #tf.print('predicted', predicted)\n",
    "        #tf.print('current_loss', current_loss, summarize=-1)\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.leaf_classes_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.leaf_classes_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)        \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.split_values)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_values]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.split_index_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_index_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "\n",
    "        #optimizer.apply_gradients(zip(grads, self.dt_params),\n",
    "        #                          global_step=tf.compat.v1.train.get_or_create_global_step())     \n",
    "        \n",
    "        #self.optimizer.apply_gradients(zip([grads], [self.dt_params]))\n",
    "        #tf.print('self.dt_params', self.dt_params, summarize=-1)\n",
    "        return current_loss\n",
    "        \n",
    "    def plot(self, normalizer_list=None, path='./dt_plot.png'):\n",
    "        from anytree import Node, RenderTree\n",
    "        from anytree.exporter import DotExporter\n",
    "\n",
    "        internal_node_num_ = 2 ** self.depth - 1 \n",
    "        \n",
    "        #split_values = self.split_values\n",
    "        split_values = sigmoid_squeeze(self.split_values, self.squeeze_factor)\n",
    "        split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "        split_index_array = self.split_index_array \n",
    "        split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "        split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "        splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "        \n",
    "        splits = splits.numpy()\n",
    "        leaf_classes = tf.sigmoid(self.leaf_classes_array).numpy()\n",
    "\n",
    "\n",
    "        if normalizer_list is not None: \n",
    "            transpose = splits.transpose()\n",
    "            transpose_normalized = []\n",
    "            for i, column in enumerate(transpose):\n",
    "                column_new = column\n",
    "                if len(column_new[column_new != 0]) != 0:\n",
    "                    column_new[column_new != 0] = normalizer_list[i].inverse_transform(column[column != 0].reshape(-1, 1)).ravel()\n",
    "                #column_new = normalizer_list[i].inverse_transform(column.reshape(-1, 1)).ravel()\n",
    "                transpose_normalized.append(column_new)\n",
    "            splits = np.array(transpose_normalized).transpose()\n",
    "\n",
    "        splits_by_layer = []\n",
    "        for i in range(self.depth+1):\n",
    "            start = 2**i - 1\n",
    "            end = 2**(i+1) -1\n",
    "            splits_by_layer.append(splits[start:end])\n",
    "\n",
    "        nodes = {\n",
    "        }\n",
    "        #tree = Tree()\n",
    "        for i, splits in enumerate(splits_by_layer):\n",
    "            for j, split in enumerate(splits):\n",
    "                if i == 0:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, data=None)            \n",
    "                else:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "                    parent_name = 'n' + str(parent_node_id)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)\n",
    "\n",
    "        for j, leaf_class in enumerate(leaf_classes):\n",
    "            i = self.depth\n",
    "            current_node_id = int(2**i - 1 + j)\n",
    "            name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "            parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "            parent_name = 'n' + str(parent_node_id)\n",
    "            #split_variable = np.argmax(np.abs(split))\n",
    "            #split_value = np.round(split[split_variable], 3)\n",
    "            split_description = str(np.round((leaf_class), 3))#'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "            nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "            #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)        \n",
    "\n",
    "            DotExporter(nodes['n0'], nodeattrfunc=lambda node: 'label=\"{}\"'.format(node.display_name)).to_picture(path)\n",
    "\n",
    "\n",
    "        return Image(path)#, nodes#nodes#tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53da89ed-8e38-4b55-aa06-414733182ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:05:53.953406Z",
     "iopub.status.busy": "2022-06-04T15:05:53.952902Z",
     "iopub.status.idle": "2022-06-04T15:05:54.016784Z",
     "shell.execute_reply": "2022-06-04T15:05:54.015917Z",
     "shell.execute_reply.started": "2022-06-04T15:05:53.953369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHDT(tf.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            squeeze_factor = 5,\n",
    "            learning_rate=1e-3,\n",
    "            loss='binary_crossentropy',#'mae',\n",
    "            optimizer = 'adam',\n",
    "            random_seed=42,\n",
    "            verbosity=1):    \n",
    "        \n",
    "        \n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = tf.keras.losses.get(loss)\n",
    "        self.seed = random_seed\n",
    "        self.verbosity = verbosity\n",
    "        self.number_of_variables = number_of_variables\n",
    "        self.squeeze_factor = squeeze_factor\n",
    "        \n",
    "        self.internal_node_num_ = 2 ** self.depth - 1 \n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "        \n",
    "        tf.random.set_seed(self.seed)\n",
    "        \n",
    "        function_representation_length = ((2 ** self.depth - 1) * self.number_of_variables * 2) + (2 ** self.depth)\n",
    "        \n",
    "        #dt_params =  tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(function_representation_length,)),\n",
    "        #                              trainable=True,\n",
    "        #                              name='dt_params')\n",
    "        \n",
    "        \n",
    "        maximum_depth = self.depth\n",
    "        leaf_node_num_ = 2 ** maximum_depth\n",
    "        internal_node_num_ = 2 ** maximum_depth - 1\n",
    "        \n",
    "        #dt_params_activation = self.dt_params#self.apply_activation(self.dt_params)\n",
    "        \n",
    "        #internal_nodes, leaf_nodes = self.get_shaped_parameters_for_decision_tree(dt_params_activation)\n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_         \n",
    "        \n",
    "        self.split_values = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_values_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_values')\n",
    "        tf.print(self.split_values, summarize=-1)\n",
    "        self.split_index_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(split_index_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='split_index_array')\n",
    "        tf.print(self.split_index_array, summarize=-1)\n",
    "        self.leaf_classes_array = tf.Variable(tf.keras.initializers.GlorotUniform(seed=self.seed)(shape=(leaf_classes_num_params,)),\n",
    "                                      trainable=True,\n",
    "                                      name='leaf_classes_array')\n",
    "        tf.print(self.leaf_classes_array, summarize=-1)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        self.optimizer.learning_rate = self.learning_rate  \n",
    "        \n",
    "    def fit(self, X, y, batch_size=32, epochs=100, early_stopping_epochs=5):\n",
    "        \n",
    "        minimum_loss_epoch = np.inf\n",
    "        epochs_without_improvement = 0        \n",
    "        \n",
    "        for current_epoch in tqdm(range(epochs)):\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            X = tf.random.shuffle(X, seed=self.seed + current_epoch)\n",
    "            tf.random.set_seed(self.seed + current_epoch)\n",
    "            y = tf.random.shuffle(y, seed=self.seed + current_epoch)\n",
    "            \n",
    "            loss_list = []\n",
    "            for index, (X_batch, y_batch) in enumerate(zip(make_batch(X, batch_size), make_batch(y, batch_size))):\n",
    "                current_loss = self.backward(X_batch, y_batch)\n",
    "                loss_list.append(float(current_loss))\n",
    "                \n",
    "                if self.verbosity >= 2:\n",
    "                    batch_idx = (index+1)*batch_size\n",
    "                    msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    print(msg.format(current_epoch, batch_idx, current_loss))                   \n",
    "                  \n",
    "            if self.verbosity > 0:    \n",
    "                msg = \"Epoch: {:02d} | Loss: {:.5f} |\"\n",
    "                print(msg.format(current_epoch, np.mean(loss_list)))              \n",
    "            \n",
    "            current_loss_epoch = np.mean(loss_list)\n",
    "\n",
    "            if current_loss_epoch < minimum_loss_epoch:\n",
    "                minimum_loss_epoch = current_loss_epoch\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= early_stopping_epochs:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    #split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    split_index = tfa.activations.sparsemax(100 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "    \n",
    "    @tf.function(jit_compile=True)                    \n",
    "    def forward_hard(self, X):\n",
    "        X = tf.dtypes.cast(tf.convert_to_tensor(X), tf.float32)       \n",
    "\n",
    "        internal_node_num_ = self.internal_node_num_\n",
    "        leaf_node_num_ = self.leaf_node_num_\n",
    "\n",
    "        split_values_num_params = self.number_of_variables * internal_node_num_\n",
    "        split_index_num_params = self.number_of_variables * internal_node_num_\n",
    "        leaf_classes_num_params = self.leaf_node_num_             \n",
    "\n",
    "        paths = [[0,1,3], [0,1,4], [0,2,5], [0,2,6]]\n",
    "\n",
    "        #split_index_array = tfa.seq2seq.hardmax(tf.reshape(split_index_array, (internal_node_num_, -1)))\n",
    "        #function_values_dhdt = tf.reshape(tf.constant([], tf.float32), shape=(0,)) #[]\n",
    "        #function_values_dhdt = tf.zeros(shape=(X.shape[0],)) #[]\n",
    "        #entry_index = 0\n",
    "        #for entry in tf.unstack(X):\n",
    "            \n",
    "\n",
    "\n",
    "        def process(entry):\n",
    "            result = 0\n",
    "            for leaf_index, path in enumerate(paths):\n",
    "                path_result_left = 1\n",
    "                path_result_right = 1\n",
    "                for internal_node_index in path: \n",
    "                    #tf.print(path, internal_node_index)\n",
    "                    split_index = tfa.seq2seq.hardmax(self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])\n",
    "                    #split_index = tfa.activations.sparsemax(10 * self.split_index_array[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)])                        \n",
    "                    \n",
    "                    #split_values = tf.sigmoid(self.split_values)[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    split_values = sigmoid_squeeze(self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]-0.5, self.squeeze_factor)\n",
    "                    #split_values = self.split_values[self.number_of_variables*internal_node_index:self.number_of_variables*(internal_node_index+1)]\n",
    "                    \n",
    "                    internal_node_split_value = tf.reduce_sum(split_index*split_values)\n",
    "                    respective_input_value = tf.reduce_sum(split_index*entry)\n",
    "\n",
    "\n",
    "                    #tf.print('internal_node_split_value', internal_node_split_value)\n",
    "                    #tf.print('respective_input_value', respective_input_value)\n",
    "\n",
    "                    #split_decision = tf.keras.activations.relu(tf.math.sign(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    #split_decision = tf.sigmoid(100 * (respective_input_value - internal_node_split_value - 0.5))\n",
    "                    split_decision = tf.round(tf.sigmoid(respective_input_value - internal_node_split_value - 0.5))\n",
    "                    #tf.print('split_decision', split_decision)\n",
    "\n",
    "\n",
    "                    path_result_left *= split_decision\n",
    "                    path_result_right *= (1 - split_decision)\n",
    "\n",
    "                    #tf.print('path_result_left', path_result_left)\n",
    "                    #tf.print('path_result_right', path_result_right)\n",
    "\n",
    "                #tf.print('path_result_left', path_result_left, summarize=-1)\n",
    "                #tf.print('path_result_right', path_result_right, summarize=-1)\n",
    "                #tf.print('tf.sigmoid(self.leaf_classes_array)', tf.sigmoid(self.leaf_classes_array), summarize=-1)\n",
    "                \n",
    "                #result += tf.sigmoid(self.leaf_classes_array)[leaf_index*2] * path_result_left + tf.sigmoid(self.leaf_classes_array)[leaf_index*2+1] * path_result_right\n",
    "                result += self.leaf_classes_array[leaf_index*2] * path_result_left + self.leaf_classes_array[leaf_index*2+1] * path_result_right\n",
    "                #tf.print(result)\n",
    "            return result\n",
    "            #tf.print('RESULT', result)\n",
    "\n",
    "            #function_values_dhdt.append(result)\n",
    "            #tf.autograph.experimental.set_loop_options(\n",
    "            #        shape_invariants=[(function_values_dhdt, tf.TensorShape([None]))]\n",
    "            #    )            \n",
    "            #function_values_dhdt = tf.concat([function_values_dhdt, [result]], 0)\n",
    "            #function_values_dhdt[entry_index] = result\n",
    "            #entry_index += 1\n",
    "        #function_values_dhdt = tf.stack(function_values_dhdt)\n",
    "        #tf.print('function_values_dhdt', function_values_dhdt)\n",
    "\n",
    "        function_values_dhdt = tf.vectorized_map(process, X)\n",
    "        \n",
    "        return function_values_dhdt  \n",
    "           \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return tf.sigmoid(self.forward_hard(X))\n",
    "        \n",
    "    def backward(self, x,y):\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)#tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            predicted = self.forward(x)\n",
    "            if self.loss.__name__  == 'binary_crossentropy':\n",
    "                current_loss = self.loss(y, predicted, from_logits=True)\n",
    "            else:\n",
    "                current_loss = self.loss(y, predicted, from_logits=True)\n",
    "        #tf.print('predicted', predicted)\n",
    "        #tf.print('current_loss', current_loss, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.leaf_classes_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.leaf_classes_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)        \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.split_values)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_values]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "        grads = tape.gradient(current_loss, self.split_index_array)\n",
    "        self.optimizer.apply_gradients(zip([grads], [self.split_index_array]))\n",
    "        #tf.print('grads', grads, summarize=-1)\n",
    "\n",
    "        #                          global_step=tf.compat.v1.train.get_or_create_global_step())     \n",
    "        \n",
    "        return current_loss\n",
    "        \n",
    "    def plot(self, normalizer_list=None, path='./dt_plot.png'):\n",
    "        from anytree import Node, RenderTree\n",
    "        from anytree.exporter import DotExporter\n",
    "\n",
    "        internal_node_num_ = 2 ** self.depth - 1 \n",
    "        \n",
    "        #split_values = self.split_values\n",
    "        split_values = sigmoid_squeeze(self.split_values, self.squeeze_factor)\n",
    "        split_values_list_by_internal_node = tf.split(split_values, internal_node_num_)\n",
    "\n",
    "        split_index_array = self.split_index_array \n",
    "        split_index_list_by_internal_node = tf.split(split_index_array, internal_node_num_)         \n",
    "\n",
    "        split_index_list_by_internal_node_max = tfa.seq2seq.hardmax(split_index_list_by_internal_node)#tfa.activations.sparsemax(split_index_list_by_internal_node)\n",
    "\n",
    "        splits = tf.stack(tf.multiply(split_values_list_by_internal_node, split_index_list_by_internal_node_max))\n",
    "\n",
    "        \n",
    "        splits = splits.numpy()\n",
    "        leaf_classes = tf.sigmoid(self.leaf_classes_array).numpy()\n",
    "\n",
    "\n",
    "        if normalizer_list is not None: \n",
    "            transpose = splits.transpose()\n",
    "            transpose_normalized = []\n",
    "            for i, column in enumerate(transpose):\n",
    "                column_new = column\n",
    "                if len(column_new[column_new != 0]) != 0:\n",
    "                    column_new[column_new != 0] = normalizer_list[i].inverse_transform(column[column != 0].reshape(-1, 1)).ravel()\n",
    "                #column_new = normalizer_list[i].inverse_transform(column.reshape(-1, 1)).ravel()\n",
    "                transpose_normalized.append(column_new)\n",
    "            splits = np.array(transpose_normalized).transpose()\n",
    "\n",
    "        splits_by_layer = []\n",
    "        for i in range(self.depth+1):\n",
    "            start = 2**i - 1\n",
    "            end = 2**(i+1) -1\n",
    "            splits_by_layer.append(splits[start:end])\n",
    "\n",
    "        nodes = {\n",
    "        }\n",
    "        #tree = Tree()\n",
    "        for i, splits in enumerate(splits_by_layer):\n",
    "            for j, split in enumerate(splits):\n",
    "                if i == 0:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, data=None)            \n",
    "                else:\n",
    "                    current_node_id = int(2**i - 1 + j)\n",
    "                    name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "                    parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "                    parent_name = 'n' + str(parent_node_id)\n",
    "                    split_variable = np.argmax(np.abs(split))\n",
    "                    split_value = np.round(split[split_variable], 3)\n",
    "                    split_description = 'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "\n",
    "                    nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "\n",
    "                    #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)\n",
    "\n",
    "        for j, leaf_class in enumerate(leaf_classes):\n",
    "            i = self.depth\n",
    "            current_node_id = int(2**i - 1 + j)\n",
    "            name = 'n' + str(current_node_id)#'l' + str(i) + 'n' + str(j)\n",
    "            parent_node_id = int(np.floor((current_node_id-1)/2))\n",
    "            parent_name = 'n' + str(parent_node_id)\n",
    "            #split_variable = np.argmax(np.abs(split))\n",
    "            #split_value = np.round(split[split_variable], 3)\n",
    "            split_description = str(np.round((leaf_class), 3))#'x' + str(split_variable) + ' <= '  + str(split_value)\n",
    "            nodes[name] = Node(name=name, parent=nodes[parent_name], display_name=split_description)\n",
    "            #tree.create_node(tag=split_description, identifier=name, parent=parent_name, data=None)        \n",
    "\n",
    "            DotExporter(nodes['n0'], nodeattrfunc=lambda node: 'label=\"{}\"'.format(node.display_name)).to_picture(path)\n",
    "\n",
    "\n",
    "        return Image(path)#, nodes#nodes#tree        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7298793a-3131-489d-88c2-8c9da85d6db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.543396Z",
     "iopub.status.busy": "2022-06-04T15:02:09.543282Z",
     "iopub.status.idle": "2022-06-04T15:02:09.591700Z",
     "shell.execute_reply": "2022-06-04T15:02:09.591069Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.543383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10_000, n_features=5, n_informative=2, n_redundant=2, random_state=42\n",
    ")\n",
    "\n",
    "#todo: anpassen, dass nur basierend auf train data normalized\n",
    "X, normalizer_list = normalize_real_world_data(X)\n",
    "\n",
    "train_samples = 1_000#1000  # Samples used for training the models\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    shuffle=False,\n",
    "    test_size=10_000 - train_samples,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46fb31cb-037d-43de-9387-c1eb289a0f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.592634Z",
     "iopub.status.busy": "2022-06-04T15:02:09.592520Z",
     "iopub.status.idle": "2022-06-04T15:02:09.628373Z",
     "shell.execute_reply": "2022-06-04T15:02:09.627707Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.592621Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867777777777778"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sklearn = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "model_sklearn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb72512-dfbf-442d-93fd-49f96f2460f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:05:59.303779Z",
     "iopub.status.busy": "2022-06-04T15:05:59.302786Z",
     "iopub.status.idle": "2022-06-04T15:07:17.786657Z",
     "shell.execute_reply": "2022-06-04T15:07:17.786040Z",
     "shell.execute_reply.started": "2022-06-04T15:05:59.303716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0153064132 -0.0897364467 -0.255949974 0.102142185 -0.0305482149 0.0117638409 -0.0961396396 -0.0327639878 -0.00911542773 -0.276941836 -0.288011432 -0.149366185 0.0131245553 0.0817540586 0.171625912 0.0424082279 -0.147876471 0.215894341 -0.150019452 0.277546763 -0.122481659 -0.0410619676 0.288627446 0.134136051 0.21903801 0.232999921 -0.234535635 0.0170893669 -0.0714436173 -0.206922442 -0.241696537 -0.194186896 -0.0684360713 -0.195885241 -0.151363343]\n",
      "[0.189929694 -0.124219164 -0.239768669 -0.128166318 0.201738775 0.202801287 -0.085945785 0.107319593 -0.23032847 0.0156517923 0.281928062 -0.253920019 -0.171836779 -0.241361693 -0.0246061087 0.24662739 -0.0789196789 -0.230450898 -0.0202791095 0.225266933 0.24908185 0.0753646791 0.155470759 0.142386407 -0.187254876 -0.0962447673 0.218839943 -0.243292749 -0.0288981497 0.156297654 -0.245937482 -0.0587791353 0.0103982389 -0.290770262 0.283521712]\n",
      "[-0.107723773 0.0664796233 -0.384002119 0.42544347 -0.265771687 -0.113815188 -0.606380284 -0.571728826]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd159a17b8492ca668634a0fe33691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.76033 |\n",
      "Epoch: 01 | Loss: 0.73876 |\n",
      "Epoch: 02 | Loss: 0.71627 |\n",
      "Epoch: 03 | Loss: 0.69645 |\n",
      "Epoch: 04 | Loss: 0.67766 |\n",
      "Epoch: 05 | Loss: 0.66109 |\n",
      "Epoch: 06 | Loss: 0.64623 |\n",
      "Epoch: 07 | Loss: 0.63128 |\n",
      "Epoch: 08 | Loss: 0.61912 |\n",
      "Epoch: 09 | Loss: 0.60673 |\n",
      "Epoch: 10 | Loss: 0.59432 |\n",
      "Epoch: 11 | Loss: 0.58153 |\n",
      "Epoch: 12 | Loss: 0.57466 |\n",
      "Epoch: 13 | Loss: 0.56440 |\n",
      "Epoch: 14 | Loss: 0.55809 |\n",
      "Epoch: 15 | Loss: 0.55187 |\n",
      "Epoch: 16 | Loss: 0.54527 |\n",
      "Epoch: 17 | Loss: 0.53455 |\n",
      "Epoch: 18 | Loss: 0.52688 |\n",
      "Epoch: 19 | Loss: 0.51489 |\n",
      "Epoch: 20 | Loss: 0.50788 |\n",
      "Epoch: 21 | Loss: 0.50093 |\n",
      "Epoch: 22 | Loss: 0.49079 |\n",
      "Epoch: 23 | Loss: 0.48354 |\n",
      "Epoch: 24 | Loss: 0.48060 |\n",
      "Epoch: 25 | Loss: 0.47382 |\n",
      "Epoch: 26 | Loss: 0.46606 |\n",
      "Epoch: 27 | Loss: 0.46154 |\n",
      "Epoch: 28 | Loss: 0.45523 |\n",
      "Epoch: 29 | Loss: 0.45242 |\n",
      "Epoch: 30 | Loss: 0.44982 |\n",
      "Epoch: 31 | Loss: 0.44446 |\n",
      "Epoch: 32 | Loss: 0.44293 |\n",
      "Epoch: 33 | Loss: 0.43974 |\n",
      "Epoch: 34 | Loss: 0.43670 |\n",
      "Epoch: 35 | Loss: 0.43261 |\n",
      "Epoch: 36 | Loss: 0.42907 |\n",
      "Epoch: 37 | Loss: 0.42811 |\n",
      "Epoch: 38 | Loss: 0.42415 |\n",
      "Epoch: 39 | Loss: 0.42580 |\n",
      "Epoch: 40 | Loss: 0.41821 |\n",
      "Epoch: 41 | Loss: 0.41828 |\n",
      "Epoch: 42 | Loss: 0.41316 |\n",
      "Epoch: 43 | Loss: 0.41202 |\n",
      "Epoch: 44 | Loss: 0.40921 |\n",
      "Epoch: 45 | Loss: 0.40711 |\n",
      "Epoch: 46 | Loss: 0.40593 |\n",
      "Epoch: 47 | Loss: 0.40780 |\n",
      "Epoch: 48 | Loss: 0.40206 |\n",
      "Epoch: 49 | Loss: 0.39965 |\n",
      "Epoch: 50 | Loss: 0.39996 |\n",
      "Epoch: 51 | Loss: 0.39904 |\n",
      "Epoch: 52 | Loss: 0.39752 |\n",
      "Epoch: 53 | Loss: 0.39881 |\n",
      "Epoch: 54 | Loss: 0.39581 |\n",
      "Epoch: 55 | Loss: 0.39330 |\n",
      "Epoch: 56 | Loss: 0.39185 |\n",
      "Epoch: 57 | Loss: 0.38996 |\n",
      "Epoch: 58 | Loss: 0.38770 |\n",
      "Epoch: 59 | Loss: 0.38785 |\n",
      "Epoch: 60 | Loss: 0.38548 |\n",
      "Epoch: 61 | Loss: 0.38405 |\n",
      "Epoch: 62 | Loss: 0.38375 |\n",
      "Epoch: 63 | Loss: 0.38233 |\n",
      "Epoch: 64 | Loss: 0.38120 |\n",
      "Epoch: 65 | Loss: 0.38114 |\n",
      "Epoch: 66 | Loss: 0.38152 |\n",
      "Epoch: 67 | Loss: 0.37727 |\n",
      "Epoch: 68 | Loss: 0.37957 |\n",
      "Epoch: 69 | Loss: 0.37850 |\n",
      "Epoch: 70 | Loss: 0.37409 |\n",
      "Epoch: 71 | Loss: 0.37782 |\n",
      "Epoch: 72 | Loss: 0.37286 |\n",
      "Epoch: 73 | Loss: 0.36996 |\n",
      "Epoch: 74 | Loss: 0.36992 |\n",
      "Epoch: 75 | Loss: 0.37203 |\n",
      "Epoch: 76 | Loss: 0.36880 |\n",
      "Epoch: 77 | Loss: 0.37171 |\n",
      "Epoch: 78 | Loss: 0.36610 |\n",
      "Epoch: 79 | Loss: 0.36753 |\n",
      "Epoch: 80 | Loss: 0.36480 |\n",
      "Epoch: 81 | Loss: 0.36289 |\n",
      "Epoch: 82 | Loss: 0.36322 |\n",
      "Epoch: 83 | Loss: 0.36375 |\n",
      "Epoch: 84 | Loss: 0.36396 |\n",
      "Epoch: 85 | Loss: 0.35923 |\n",
      "Epoch: 86 | Loss: 0.36248 |\n",
      "Epoch: 87 | Loss: 0.36132 |\n",
      "Epoch: 88 | Loss: 0.36669 |\n",
      "Epoch: 89 | Loss: 0.35963 |\n",
      "Epoch: 90 | Loss: 0.35809 |\n",
      "Epoch: 91 | Loss: 0.36097 |\n",
      "Epoch: 92 | Loss: 0.35852 |\n",
      "Epoch: 93 | Loss: 0.35837 |\n",
      "Epoch: 94 | Loss: 0.35689 |\n",
      "Epoch: 95 | Loss: 0.35728 |\n",
      "Epoch: 96 | Loss: 0.35851 |\n",
      "Epoch: 97 | Loss: 0.35763 |\n",
      "Epoch: 98 | Loss: 0.35983 |\n",
      "Epoch: 99 | Loss: 0.35532 |\n",
      "Epoch: 100 | Loss: 0.35399 |\n",
      "Epoch: 101 | Loss: 0.35730 |\n",
      "Epoch: 102 | Loss: 0.35332 |\n",
      "Epoch: 103 | Loss: 0.35361 |\n",
      "Epoch: 104 | Loss: 0.35123 |\n",
      "Epoch: 105 | Loss: 0.34908 |\n",
      "Epoch: 106 | Loss: 0.35169 |\n",
      "Epoch: 107 | Loss: 0.35356 |\n",
      "Epoch: 108 | Loss: 0.35411 |\n",
      "Epoch: 109 | Loss: 0.34847 |\n",
      "Epoch: 110 | Loss: 0.34859 |\n",
      "Epoch: 111 | Loss: 0.35152 |\n",
      "Epoch: 112 | Loss: 0.34825 |\n",
      "Epoch: 113 | Loss: 0.34808 |\n",
      "Epoch: 114 | Loss: 0.34695 |\n",
      "Epoch: 115 | Loss: 0.34986 |\n",
      "Epoch: 116 | Loss: 0.34761 |\n",
      "Epoch: 117 | Loss: 0.34980 |\n",
      "Epoch: 118 | Loss: 0.34370 |\n",
      "Epoch: 119 | Loss: 0.34864 |\n",
      "Epoch: 120 | Loss: 0.34469 |\n",
      "Epoch: 121 | Loss: 0.34459 |\n",
      "Epoch: 122 | Loss: 0.34773 |\n",
      "Epoch: 123 | Loss: 0.34521 |\n",
      "Epoch: 124 | Loss: 0.34476 |\n",
      "Epoch: 125 | Loss: 0.34369 |\n",
      "Epoch: 126 | Loss: 0.34623 |\n",
      "Epoch: 127 | Loss: 0.34468 |\n",
      "Epoch: 128 | Loss: 0.34364 |\n",
      "Epoch: 129 | Loss: 0.34525 |\n",
      "Epoch: 130 | Loss: 0.34332 |\n",
      "Epoch: 131 | Loss: 0.34280 |\n",
      "Epoch: 132 | Loss: 0.34522 |\n",
      "Epoch: 133 | Loss: 0.34038 |\n",
      "Epoch: 134 | Loss: 0.34386 |\n",
      "Epoch: 135 | Loss: 0.34330 |\n",
      "Epoch: 136 | Loss: 0.34385 |\n",
      "Epoch: 137 | Loss: 0.34186 |\n",
      "Epoch: 138 | Loss: 0.34322 |\n",
      "Epoch: 139 | Loss: 0.34201 |\n",
      "Epoch: 140 | Loss: 0.33751 |\n",
      "Epoch: 141 | Loss: 0.34087 |\n",
      "Epoch: 142 | Loss: 0.34391 |\n",
      "Epoch: 143 | Loss: 0.34299 |\n",
      "Epoch: 144 | Loss: 0.34072 |\n",
      "Epoch: 145 | Loss: 0.34080 |\n",
      "Epoch: 146 | Loss: 0.34212 |\n",
      "Epoch: 147 | Loss: 0.34226 |\n",
      "Epoch: 148 | Loss: 0.34086 |\n",
      "Epoch: 149 | Loss: 0.33960 |\n",
      "Epoch: 150 | Loss: 0.33833 |\n",
      "Epoch: 151 | Loss: 0.34007 |\n",
      "Epoch: 152 | Loss: 0.34254 |\n",
      "Epoch: 153 | Loss: 0.33765 |\n",
      "Epoch: 154 | Loss: 0.34405 |\n",
      "Epoch: 155 | Loss: 0.33839 |\n",
      "Epoch: 156 | Loss: 0.33878 |\n",
      "Epoch: 157 | Loss: 0.34017 |\n",
      "Epoch: 158 | Loss: 0.34012 |\n",
      "Epoch: 159 | Loss: 0.34077 |\n",
      "Epoch: 160 | Loss: 0.34068 |\n",
      "Test Accuracy 0.8705555555555555\n"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=40,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6803a78e-bb06-4387-a53b-ed0170c76eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:07:17.787787Z",
     "iopub.status.busy": "2022-06-04T15:07:17.787674Z",
     "iopub.status.idle": "2022-06-04T15:08:51.277876Z",
     "shell.execute_reply": "2022-06-04T15:08:51.277061Z",
     "shell.execute_reply.started": "2022-06-04T15:07:17.787772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.220465124 -0.255536348 0.0976486206 -0.00916129351 -0.261932015 0.0230374038 0.288691342 0.170627534 -0.0870554894 0.0270647407 0.110997587 0.0621287823 0.132869 0.00538137555 0.148039192 -0.0770659447 0.265260041 0.0907332 -0.18583867 -0.235048532 0.178994954 -0.00596177578 -0.0612244904 0.125658631 0.137933135 0.250172377 -0.129651144 0.209979832 -0.282800853 0.132009596 -0.253193587 0.117978752 0.0385412872 0.142327785 0.246568859]\n",
      "[0.126232684 -0.0315164328 -0.292304039 -0.138535246 0.243391097 -0.034647882 -0.114259422 -0.0921471864 0.264761925 0.0376400054 -0.0463365316 -0.14340657 -0.104664683 0.204574943 -0.118991494 0.0154892206 -0.154772684 -0.113403723 -0.278707117 0.134550452 0.209358096 -0.1278 0.00668478 -0.142500341 -0.109868616 -0.00778451562 0.0010073185 0.228656173 -0.0195489824 -0.0541208386 0.047627449 0.287532687 0.0835378468 0.209075212 -0.138658658]\n",
      "[0.0462907553 -0.342950016 0.22330308 0.216890454 0.582392871 -0.526290298 -0.610656202 -0.54491812]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259417fe1a5f4b47ad3a874696d3b3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.71917 |\n",
      "Epoch: 01 | Loss: 0.71401 |\n",
      "Epoch: 02 | Loss: 0.70973 |\n",
      "Epoch: 03 | Loss: 0.70609 |\n",
      "Epoch: 04 | Loss: 0.70351 |\n",
      "Epoch: 05 | Loss: 0.70050 |\n",
      "Epoch: 06 | Loss: 0.69879 |\n",
      "Epoch: 07 | Loss: 0.69634 |\n",
      "Epoch: 08 | Loss: 0.69463 |\n",
      "Epoch: 09 | Loss: 0.69227 |\n",
      "Epoch: 10 | Loss: 0.69054 |\n",
      "Epoch: 11 | Loss: 0.68854 |\n",
      "Epoch: 12 | Loss: 0.68682 |\n",
      "Epoch: 13 | Loss: 0.68586 |\n",
      "Epoch: 14 | Loss: 0.68366 |\n",
      "Epoch: 15 | Loss: 0.68248 |\n",
      "Epoch: 16 | Loss: 0.68033 |\n",
      "Epoch: 17 | Loss: 0.67910 |\n",
      "Epoch: 18 | Loss: 0.67734 |\n",
      "Epoch: 19 | Loss: 0.67525 |\n",
      "Epoch: 20 | Loss: 0.67403 |\n",
      "Epoch: 21 | Loss: 0.67166 |\n",
      "Epoch: 22 | Loss: 0.67065 |\n",
      "Epoch: 23 | Loss: 0.66849 |\n",
      "Epoch: 24 | Loss: 0.66705 |\n",
      "Epoch: 25 | Loss: 0.66564 |\n",
      "Epoch: 26 | Loss: 0.66349 |\n",
      "Epoch: 27 | Loss: 0.66179 |\n",
      "Epoch: 28 | Loss: 0.66071 |\n",
      "Epoch: 29 | Loss: 0.65924 |\n",
      "Epoch: 30 | Loss: 0.65671 |\n",
      "Epoch: 31 | Loss: 0.65493 |\n",
      "Epoch: 32 | Loss: 0.65332 |\n",
      "Epoch: 33 | Loss: 0.65131 |\n",
      "Epoch: 34 | Loss: 0.64846 |\n",
      "Epoch: 35 | Loss: 0.64576 |\n",
      "Epoch: 36 | Loss: 0.64360 |\n",
      "Epoch: 37 | Loss: 0.64204 |\n",
      "Epoch: 38 | Loss: 0.63995 |\n",
      "Epoch: 39 | Loss: 0.63878 |\n",
      "Epoch: 40 | Loss: 0.63633 |\n",
      "Epoch: 41 | Loss: 0.63611 |\n",
      "Epoch: 42 | Loss: 0.63433 |\n",
      "Epoch: 43 | Loss: 0.63396 |\n",
      "Epoch: 44 | Loss: 0.63202 |\n",
      "Epoch: 45 | Loss: 0.63134 |\n",
      "Epoch: 46 | Loss: 0.62999 |\n",
      "Epoch: 47 | Loss: 0.62907 |\n",
      "Epoch: 48 | Loss: 0.62835 |\n",
      "Epoch: 49 | Loss: 0.62738 |\n",
      "Epoch: 50 | Loss: 0.62585 |\n",
      "Epoch: 51 | Loss: 0.62599 |\n",
      "Epoch: 52 | Loss: 0.62498 |\n",
      "Epoch: 53 | Loss: 0.62350 |\n",
      "Epoch: 54 | Loss: 0.62314 |\n",
      "Epoch: 55 | Loss: 0.62230 |\n",
      "Epoch: 56 | Loss: 0.62183 |\n",
      "Epoch: 57 | Loss: 0.62019 |\n",
      "Epoch: 58 | Loss: 0.62114 |\n",
      "Epoch: 59 | Loss: 0.62045 |\n",
      "Epoch: 60 | Loss: 0.61920 |\n",
      "Epoch: 61 | Loss: 0.61814 |\n",
      "Epoch: 62 | Loss: 0.61700 |\n",
      "Epoch: 63 | Loss: 0.61664 |\n",
      "Epoch: 64 | Loss: 0.61562 |\n",
      "Epoch: 65 | Loss: 0.61561 |\n",
      "Epoch: 66 | Loss: 0.61461 |\n",
      "Epoch: 67 | Loss: 0.61339 |\n",
      "Epoch: 68 | Loss: 0.61338 |\n",
      "Epoch: 69 | Loss: 0.61292 |\n",
      "Epoch: 70 | Loss: 0.61224 |\n",
      "Epoch: 71 | Loss: 0.61209 |\n",
      "Epoch: 72 | Loss: 0.61109 |\n",
      "Epoch: 73 | Loss: 0.61049 |\n",
      "Epoch: 74 | Loss: 0.60890 |\n",
      "Epoch: 75 | Loss: 0.60945 |\n",
      "Epoch: 76 | Loss: 0.60902 |\n",
      "Epoch: 77 | Loss: 0.60765 |\n",
      "Epoch: 78 | Loss: 0.60838 |\n",
      "Epoch: 79 | Loss: 0.60669 |\n",
      "Epoch: 80 | Loss: 0.60748 |\n",
      "Epoch: 81 | Loss: 0.60564 |\n",
      "Epoch: 82 | Loss: 0.60474 |\n",
      "Epoch: 83 | Loss: 0.60529 |\n",
      "Epoch: 84 | Loss: 0.60380 |\n",
      "Epoch: 85 | Loss: 0.60353 |\n",
      "Epoch: 86 | Loss: 0.60253 |\n",
      "Epoch: 87 | Loss: 0.60250 |\n",
      "Epoch: 88 | Loss: 0.60216 |\n",
      "Epoch: 89 | Loss: 0.60043 |\n",
      "Epoch: 90 | Loss: 0.60031 |\n",
      "Epoch: 91 | Loss: 0.59929 |\n",
      "Epoch: 92 | Loss: 0.59881 |\n",
      "Epoch: 93 | Loss: 0.59891 |\n",
      "Epoch: 94 | Loss: 0.60028 |\n",
      "Epoch: 95 | Loss: 0.59893 |\n",
      "Epoch: 96 | Loss: 0.59905 |\n",
      "Epoch: 97 | Loss: 0.59863 |\n",
      "Epoch: 98 | Loss: 0.59627 |\n",
      "Epoch: 99 | Loss: 0.59595 |\n",
      "Epoch: 100 | Loss: 0.59582 |\n",
      "Epoch: 101 | Loss: 0.59551 |\n",
      "Epoch: 102 | Loss: 0.59501 |\n",
      "Epoch: 103 | Loss: 0.59574 |\n",
      "Epoch: 104 | Loss: 0.59225 |\n",
      "Epoch: 105 | Loss: 0.59344 |\n",
      "Epoch: 106 | Loss: 0.59340 |\n",
      "Epoch: 107 | Loss: 0.59223 |\n",
      "Epoch: 108 | Loss: 0.59148 |\n",
      "Epoch: 109 | Loss: 0.59199 |\n",
      "Epoch: 110 | Loss: 0.59194 |\n",
      "Epoch: 111 | Loss: 0.59152 |\n",
      "Epoch: 112 | Loss: 0.58931 |\n",
      "Epoch: 113 | Loss: 0.59024 |\n",
      "Epoch: 114 | Loss: 0.58934 |\n",
      "Epoch: 115 | Loss: 0.58966 |\n",
      "Epoch: 116 | Loss: 0.58816 |\n",
      "Epoch: 117 | Loss: 0.58575 |\n",
      "Epoch: 118 | Loss: 0.58813 |\n",
      "Epoch: 119 | Loss: 0.58696 |\n",
      "Epoch: 120 | Loss: 0.58713 |\n",
      "Epoch: 121 | Loss: 0.58440 |\n",
      "Epoch: 122 | Loss: 0.58554 |\n",
      "Epoch: 123 | Loss: 0.58497 |\n",
      "Epoch: 124 | Loss: 0.58586 |\n",
      "Epoch: 125 | Loss: 0.58374 |\n",
      "Epoch: 126 | Loss: 0.58426 |\n",
      "Epoch: 127 | Loss: 0.58385 |\n",
      "Epoch: 128 | Loss: 0.58208 |\n",
      "Epoch: 129 | Loss: 0.58166 |\n",
      "Epoch: 130 | Loss: 0.58304 |\n",
      "Epoch: 131 | Loss: 0.58189 |\n",
      "Epoch: 132 | Loss: 0.58035 |\n",
      "Epoch: 133 | Loss: 0.57993 |\n",
      "Epoch: 134 | Loss: 0.58129 |\n",
      "Epoch: 135 | Loss: 0.58064 |\n",
      "Epoch: 136 | Loss: 0.57965 |\n",
      "Epoch: 137 | Loss: 0.58051 |\n",
      "Epoch: 138 | Loss: 0.57946 |\n",
      "Epoch: 139 | Loss: 0.57892 |\n",
      "Epoch: 140 | Loss: 0.57827 |\n",
      "Epoch: 141 | Loss: 0.57731 |\n",
      "Epoch: 142 | Loss: 0.57777 |\n",
      "Epoch: 143 | Loss: 0.57546 |\n",
      "Epoch: 144 | Loss: 0.57529 |\n",
      "Epoch: 145 | Loss: 0.57491 |\n",
      "Epoch: 146 | Loss: 0.57650 |\n",
      "Epoch: 147 | Loss: 0.57539 |\n",
      "Epoch: 148 | Loss: 0.57493 |\n",
      "Epoch: 149 | Loss: 0.57397 |\n",
      "Epoch: 150 | Loss: 0.57464 |\n",
      "Epoch: 151 | Loss: 0.57322 |\n",
      "Epoch: 152 | Loss: 0.57237 |\n",
      "Epoch: 153 | Loss: 0.57406 |\n",
      "Epoch: 154 | Loss: 0.57395 |\n",
      "Epoch: 155 | Loss: 0.57306 |\n",
      "Epoch: 156 | Loss: 0.57277 |\n",
      "Epoch: 157 | Loss: 0.56991 |\n",
      "Epoch: 158 | Loss: 0.57192 |\n",
      "Epoch: 159 | Loss: 0.57139 |\n",
      "Epoch: 160 | Loss: 0.57109 |\n",
      "Epoch: 161 | Loss: 0.57054 |\n",
      "Epoch: 162 | Loss: 0.57011 |\n",
      "Epoch: 163 | Loss: 0.56977 |\n",
      "Epoch: 164 | Loss: 0.57102 |\n",
      "Epoch: 165 | Loss: 0.57036 |\n",
      "Epoch: 166 | Loss: 0.56775 |\n",
      "Epoch: 167 | Loss: 0.56719 |\n",
      "Epoch: 168 | Loss: 0.56814 |\n",
      "Epoch: 169 | Loss: 0.56637 |\n",
      "Epoch: 170 | Loss: 0.56874 |\n",
      "Epoch: 171 | Loss: 0.56679 |\n",
      "Epoch: 172 | Loss: 0.56721 |\n",
      "Epoch: 173 | Loss: 0.56628 |\n",
      "Epoch: 174 | Loss: 0.56779 |\n",
      "Epoch: 175 | Loss: 0.56451 |\n",
      "Epoch: 176 | Loss: 0.56579 |\n",
      "Epoch: 177 | Loss: 0.56589 |\n",
      "Epoch: 178 | Loss: 0.56515 |\n",
      "Epoch: 179 | Loss: 0.56628 |\n",
      "Epoch: 180 | Loss: 0.56304 |\n",
      "Epoch: 181 | Loss: 0.56417 |\n",
      "Epoch: 182 | Loss: 0.56456 |\n",
      "Epoch: 183 | Loss: 0.56453 |\n",
      "Epoch: 184 | Loss: 0.56318 |\n",
      "Epoch: 185 | Loss: 0.56436 |\n",
      "Epoch: 186 | Loss: 0.56257 |\n",
      "Epoch: 187 | Loss: 0.56286 |\n",
      "Epoch: 188 | Loss: 0.56229 |\n",
      "Epoch: 189 | Loss: 0.56114 |\n",
      "Epoch: 190 | Loss: 0.56148 |\n",
      "Epoch: 191 | Loss: 0.56027 |\n",
      "Epoch: 192 | Loss: 0.56065 |\n",
      "Epoch: 193 | Loss: 0.56077 |\n",
      "Epoch: 194 | Loss: 0.56125 |\n",
      "Epoch: 195 | Loss: 0.56028 |\n",
      "Epoch: 196 | Loss: 0.56002 |\n",
      "Epoch: 197 | Loss: 0.56184 |\n",
      "Epoch: 198 | Loss: 0.56118 |\n",
      "Epoch: 199 | Loss: 0.55891 |\n",
      "Epoch: 200 | Loss: 0.55964 |\n",
      "Epoch: 201 | Loss: 0.55685 |\n",
      "Epoch: 202 | Loss: 0.55985 |\n",
      "Epoch: 203 | Loss: 0.56034 |\n",
      "Epoch: 204 | Loss: 0.55918 |\n",
      "Epoch: 205 | Loss: 0.56109 |\n",
      "Epoch: 206 | Loss: 0.55884 |\n",
      "Epoch: 207 | Loss: 0.55936 |\n",
      "Epoch: 208 | Loss: 0.55747 |\n",
      "Epoch: 209 | Loss: 0.55844 |\n",
      "Epoch: 210 | Loss: 0.55479 |\n",
      "Epoch: 211 | Loss: 0.55523 |\n",
      "Epoch: 212 | Loss: 0.55575 |\n",
      "Epoch: 213 | Loss: 0.55678 |\n",
      "Epoch: 214 | Loss: 0.55617 |\n",
      "Epoch: 215 | Loss: 0.55512 |\n",
      "Epoch: 216 | Loss: 0.55458 |\n",
      "Epoch: 217 | Loss: 0.55494 |\n",
      "Epoch: 218 | Loss: 0.55487 |\n",
      "Epoch: 219 | Loss: 0.55501 |\n",
      "Epoch: 220 | Loss: 0.55527 |\n",
      "Epoch: 221 | Loss: 0.55456 |\n",
      "Epoch: 222 | Loss: 0.55401 |\n",
      "Epoch: 223 | Loss: 0.55602 |\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "PyEval_EvalFrameEx returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_zeros\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m   \u001b[0;31m# Note: variants will use _zeros_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2619277/118804419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             verbosity=1)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_test_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dhdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2619277/4247248776.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, epochs, early_stopping_epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2619277/4247248776.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m#tf.print('predicted', predicted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m#tf.print('current_loss', current_loss, summarize=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaf_classes_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaf_classes_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m#tf.print('grads', grads, summarize=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1082\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/XAI/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m       \u001b[0;34m\"\"\"Process output gradients and call the backward function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162c6af1-9dd1-4709-bff8-4d8c1c92e340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:02:09.629198Z",
     "iopub.status.busy": "2022-06-04T15:02:09.629094Z",
     "iopub.status.idle": "2022-06-04T15:04:03.058815Z",
     "shell.execute_reply": "2022-06-04T15:04:03.058277Z",
     "shell.execute_reply.started": "2022-06-04T15:02:09.629186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10405615 -0.0325394273 -0.0746977776 0.00976884365 0.0798775554 -0.0226446092 0.199392498 -0.0884800851 -0.157206744 -0.210958183 -0.283841044 0.250602424 -0.0695579201 -0.157768637 0.203923196 0.07348001 0.08279562 0.12547636 -0.187254116 0.280626178 -0.0511720628 -0.0946338 0.0703919232 0.0186030269 0.0230322182 -0.0131372511 0.0924431384 0.149752051 -0.209223688 -0.232426018 0.0220719576 0.25241524 -0.0870755315 -0.163386226 -0.23524265]\n",
      "[-0.193196684 -0.147729188 -0.171274662 0.00118607283 0.0514366925 0.269179761 -0.0124358237 -0.198199391 0.201756209 -0.0928686559 0.209533036 0.285333335 -0.23075369 0.0342013538 0.0404126644 0.258410096 0.0399357677 -2.68816948e-05 0.18228367 -0.180432796 0.224031985 -0.0702865124 0.262633741 0.130812496 0.264650702 -0.112750098 0.14068687 0.110420108 0.264017045 -0.141295969 -0.276891381 -0.0761598498 0.0672431588 0.127154201 0.223339498]\n",
      "[0.579931796 -0.0352647901 0.427904189 -0.24518615 -0.0250062943 -0.300857484 0.514985263 0.309282303]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8dbe9c052440ec837e5ab80c472235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.63631 |\n",
      "Epoch: 01 | Loss: 0.62735 |\n",
      "Epoch: 02 | Loss: 0.61837 |\n",
      "Epoch: 03 | Loss: 0.60899 |\n",
      "Epoch: 04 | Loss: 0.60255 |\n",
      "Epoch: 05 | Loss: 0.59557 |\n",
      "Epoch: 06 | Loss: 0.58913 |\n",
      "Epoch: 07 | Loss: 0.58325 |\n",
      "Epoch: 08 | Loss: 0.57815 |\n",
      "Epoch: 09 | Loss: 0.57276 |\n",
      "Epoch: 10 | Loss: 0.56644 |\n",
      "Epoch: 11 | Loss: 0.56276 |\n",
      "Epoch: 12 | Loss: 0.55956 |\n",
      "Epoch: 13 | Loss: 0.55407 |\n",
      "Epoch: 14 | Loss: 0.55059 |\n",
      "Epoch: 15 | Loss: 0.54930 |\n",
      "Epoch: 16 | Loss: 0.54421 |\n",
      "Epoch: 17 | Loss: 0.54221 |\n",
      "Epoch: 18 | Loss: 0.54021 |\n",
      "Epoch: 19 | Loss: 0.53661 |\n",
      "Epoch: 20 | Loss: 0.53426 |\n",
      "Epoch: 21 | Loss: 0.53008 |\n",
      "Epoch: 22 | Loss: 0.52868 |\n",
      "Epoch: 23 | Loss: 0.52588 |\n",
      "Epoch: 24 | Loss: 0.52580 |\n",
      "Epoch: 25 | Loss: 0.52182 |\n",
      "Epoch: 26 | Loss: 0.51721 |\n",
      "Epoch: 27 | Loss: 0.51485 |\n",
      "Epoch: 28 | Loss: 0.51376 |\n",
      "Epoch: 29 | Loss: 0.51153 |\n",
      "Epoch: 30 | Loss: 0.50900 |\n",
      "Epoch: 31 | Loss: 0.50688 |\n",
      "Epoch: 32 | Loss: 0.50597 |\n",
      "Epoch: 33 | Loss: 0.50317 |\n",
      "Epoch: 34 | Loss: 0.50192 |\n",
      "Epoch: 35 | Loss: 0.49894 |\n",
      "Epoch: 36 | Loss: 0.49505 |\n",
      "Epoch: 37 | Loss: 0.49425 |\n",
      "Epoch: 38 | Loss: 0.49069 |\n",
      "Epoch: 39 | Loss: 0.49070 |\n",
      "Epoch: 40 | Loss: 0.48726 |\n",
      "Epoch: 41 | Loss: 0.48601 |\n",
      "Epoch: 42 | Loss: 0.48185 |\n",
      "Epoch: 43 | Loss: 0.48152 |\n",
      "Epoch: 44 | Loss: 0.47789 |\n",
      "Epoch: 45 | Loss: 0.47631 |\n",
      "Epoch: 46 | Loss: 0.47371 |\n",
      "Epoch: 47 | Loss: 0.47522 |\n",
      "Epoch: 48 | Loss: 0.47221 |\n",
      "Epoch: 49 | Loss: 0.46758 |\n",
      "Epoch: 50 | Loss: 0.46663 |\n",
      "Epoch: 51 | Loss: 0.46575 |\n",
      "Epoch: 52 | Loss: 0.46362 |\n",
      "Epoch: 53 | Loss: 0.46409 |\n",
      "Epoch: 54 | Loss: 0.45949 |\n",
      "Epoch: 55 | Loss: 0.45912 |\n",
      "Epoch: 56 | Loss: 0.45612 |\n",
      "Epoch: 57 | Loss: 0.45173 |\n",
      "Epoch: 58 | Loss: 0.44925 |\n",
      "Epoch: 59 | Loss: 0.44972 |\n",
      "Epoch: 60 | Loss: 0.44698 |\n",
      "Epoch: 61 | Loss: 0.44636 |\n",
      "Epoch: 62 | Loss: 0.44386 |\n",
      "Epoch: 63 | Loss: 0.44368 |\n",
      "Epoch: 64 | Loss: 0.44031 |\n",
      "Epoch: 65 | Loss: 0.43858 |\n",
      "Epoch: 66 | Loss: 0.43663 |\n",
      "Epoch: 67 | Loss: 0.43193 |\n",
      "Epoch: 68 | Loss: 0.43188 |\n",
      "Epoch: 69 | Loss: 0.42982 |\n",
      "Epoch: 70 | Loss: 0.42440 |\n",
      "Epoch: 71 | Loss: 0.42448 |\n",
      "Epoch: 72 | Loss: 0.42005 |\n",
      "Epoch: 73 | Loss: 0.41588 |\n",
      "Epoch: 74 | Loss: 0.41421 |\n",
      "Epoch: 75 | Loss: 0.41620 |\n",
      "Epoch: 76 | Loss: 0.41209 |\n",
      "Epoch: 77 | Loss: 0.41114 |\n",
      "Epoch: 78 | Loss: 0.40647 |\n",
      "Epoch: 79 | Loss: 0.40632 |\n",
      "Epoch: 80 | Loss: 0.40230 |\n",
      "Epoch: 81 | Loss: 0.39926 |\n",
      "Epoch: 82 | Loss: 0.39963 |\n",
      "Epoch: 83 | Loss: 0.39829 |\n",
      "Epoch: 84 | Loss: 0.39842 |\n",
      "Epoch: 85 | Loss: 0.39089 |\n",
      "Epoch: 86 | Loss: 0.39269 |\n",
      "Epoch: 87 | Loss: 0.39002 |\n",
      "Epoch: 88 | Loss: 0.39369 |\n",
      "Epoch: 89 | Loss: 0.38588 |\n",
      "Epoch: 90 | Loss: 0.38199 |\n",
      "Epoch: 91 | Loss: 0.38225 |\n",
      "Epoch: 92 | Loss: 0.37896 |\n",
      "Epoch: 93 | Loss: 0.37838 |\n",
      "Epoch: 94 | Loss: 0.37717 |\n",
      "Epoch: 95 | Loss: 0.37663 |\n",
      "Epoch: 96 | Loss: 0.37789 |\n",
      "Epoch: 97 | Loss: 0.37684 |\n",
      "Epoch: 98 | Loss: 0.37784 |\n",
      "Epoch: 99 | Loss: 0.37464 |\n",
      "Epoch: 100 | Loss: 0.37199 |\n",
      "Epoch: 101 | Loss: 0.37518 |\n",
      "Epoch: 102 | Loss: 0.37181 |\n",
      "Epoch: 103 | Loss: 0.37168 |\n",
      "Epoch: 104 | Loss: 0.36966 |\n",
      "Epoch: 105 | Loss: 0.36759 |\n",
      "Epoch: 106 | Loss: 0.36846 |\n",
      "Epoch: 107 | Loss: 0.37086 |\n",
      "Epoch: 108 | Loss: 0.37058 |\n",
      "Epoch: 109 | Loss: 0.36598 |\n",
      "Epoch: 110 | Loss: 0.36651 |\n",
      "Epoch: 111 | Loss: 0.36724 |\n",
      "Epoch: 112 | Loss: 0.36448 |\n",
      "Epoch: 113 | Loss: 0.36469 |\n",
      "Epoch: 114 | Loss: 0.36288 |\n",
      "Epoch: 115 | Loss: 0.36531 |\n",
      "Epoch: 116 | Loss: 0.36351 |\n",
      "Epoch: 117 | Loss: 0.36516 |\n",
      "Epoch: 118 | Loss: 0.35960 |\n",
      "Epoch: 119 | Loss: 0.36509 |\n",
      "Epoch: 120 | Loss: 0.36055 |\n",
      "Epoch: 121 | Loss: 0.35935 |\n",
      "Epoch: 122 | Loss: 0.36182 |\n",
      "Epoch: 123 | Loss: 0.36084 |\n",
      "Epoch: 124 | Loss: 0.35928 |\n",
      "Epoch: 125 | Loss: 0.35860 |\n",
      "Epoch: 126 | Loss: 0.36088 |\n",
      "Epoch: 127 | Loss: 0.35932 |\n",
      "Epoch: 128 | Loss: 0.35849 |\n",
      "Epoch: 129 | Loss: 0.35987 |\n",
      "Epoch: 130 | Loss: 0.35783 |\n",
      "Epoch: 131 | Loss: 0.35808 |\n",
      "Epoch: 132 | Loss: 0.35869 |\n",
      "Epoch: 133 | Loss: 0.35472 |\n",
      "Epoch: 134 | Loss: 0.35755 |\n",
      "Epoch: 135 | Loss: 0.35560 |\n",
      "Epoch: 136 | Loss: 0.35703 |\n",
      "Epoch: 137 | Loss: 0.35639 |\n",
      "Epoch: 138 | Loss: 0.35683 |\n",
      "Epoch: 139 | Loss: 0.35548 |\n",
      "Epoch: 140 | Loss: 0.35105 |\n",
      "Epoch: 141 | Loss: 0.35421 |\n",
      "Epoch: 142 | Loss: 0.35676 |\n",
      "Epoch: 143 | Loss: 0.35573 |\n",
      "Epoch: 144 | Loss: 0.35364 |\n",
      "Epoch: 145 | Loss: 0.35341 |\n",
      "Epoch: 146 | Loss: 0.35511 |\n",
      "Epoch: 147 | Loss: 0.35560 |\n",
      "Epoch: 148 | Loss: 0.35312 |\n",
      "Epoch: 149 | Loss: 0.35276 |\n",
      "Epoch: 150 | Loss: 0.35153 |\n",
      "Epoch: 151 | Loss: 0.35300 |\n",
      "Epoch: 152 | Loss: 0.35364 |\n",
      "Epoch: 153 | Loss: 0.35078 |\n",
      "Epoch: 154 | Loss: 0.35590 |\n",
      "Epoch: 155 | Loss: 0.35074 |\n",
      "Epoch: 156 | Loss: 0.35070 |\n",
      "Epoch: 157 | Loss: 0.35258 |\n",
      "Epoch: 158 | Loss: 0.35136 |\n",
      "Epoch: 159 | Loss: 0.35222 |\n",
      "Epoch: 160 | Loss: 0.35190 |\n",
      "Epoch: 161 | Loss: 0.35134 |\n",
      "Epoch: 162 | Loss: 0.35345 |\n",
      "Epoch: 163 | Loss: 0.35242 |\n",
      "Epoch: 164 | Loss: 0.34940 |\n",
      "Epoch: 165 | Loss: 0.34728 |\n",
      "Epoch: 166 | Loss: 0.35284 |\n",
      "Epoch: 167 | Loss: 0.35159 |\n",
      "Epoch: 168 | Loss: 0.34782 |\n",
      "Epoch: 169 | Loss: 0.34663 |\n",
      "Epoch: 170 | Loss: 0.34853 |\n",
      "Epoch: 171 | Loss: 0.34828 |\n",
      "Epoch: 172 | Loss: 0.35204 |\n",
      "Epoch: 173 | Loss: 0.35044 |\n",
      "Epoch: 174 | Loss: 0.34825 |\n",
      "Epoch: 175 | Loss: 0.34794 |\n",
      "Epoch: 176 | Loss: 0.35059 |\n",
      "Epoch: 177 | Loss: 0.35092 |\n",
      "Epoch: 178 | Loss: 0.34729 |\n",
      "Epoch: 179 | Loss: 0.34537 |\n",
      "Epoch: 180 | Loss: 0.34635 |\n",
      "Epoch: 181 | Loss: 0.35143 |\n",
      "Epoch: 182 | Loss: 0.34766 |\n",
      "Epoch: 183 | Loss: 0.34684 |\n",
      "Epoch: 184 | Loss: 0.34618 |\n",
      "Epoch: 185 | Loss: 0.34882 |\n",
      "Epoch: 186 | Loss: 0.34802 |\n",
      "Epoch: 187 | Loss: 0.34772 |\n",
      "Epoch: 188 | Loss: 0.34739 |\n",
      "Epoch: 189 | Loss: 0.34402 |\n",
      "Epoch: 190 | Loss: 0.34811 |\n",
      "Epoch: 191 | Loss: 0.34753 |\n",
      "Epoch: 192 | Loss: 0.34652 |\n",
      "Epoch: 193 | Loss: 0.34497 |\n",
      "Epoch: 194 | Loss: 0.34865 |\n",
      "Epoch: 195 | Loss: 0.34447 |\n",
      "Epoch: 196 | Loss: 0.34760 |\n",
      "Epoch: 197 | Loss: 0.34528 |\n",
      "Epoch: 198 | Loss: 0.34844 |\n",
      "Epoch: 199 | Loss: 0.34471 |\n",
      "Epoch: 200 | Loss: 0.34306 |\n",
      "Epoch: 201 | Loss: 0.34599 |\n",
      "Epoch: 202 | Loss: 0.34267 |\n",
      "Epoch: 203 | Loss: 0.34585 |\n",
      "Epoch: 204 | Loss: 0.34476 |\n",
      "Epoch: 205 | Loss: 0.34257 |\n",
      "Epoch: 206 | Loss: 0.34510 |\n",
      "Epoch: 207 | Loss: 0.34468 |\n",
      "Epoch: 208 | Loss: 0.34744 |\n",
      "Epoch: 209 | Loss: 0.34158 |\n",
      "Epoch: 210 | Loss: 0.34556 |\n",
      "Epoch: 211 | Loss: 0.34369 |\n",
      "Epoch: 212 | Loss: 0.34727 |\n",
      "Epoch: 213 | Loss: 0.34656 |\n",
      "Epoch: 214 | Loss: 0.34544 |\n",
      "Epoch: 215 | Loss: 0.34369 |\n",
      "Epoch: 216 | Loss: 0.34896 |\n",
      "Epoch: 217 | Loss: 0.34437 |\n",
      "Epoch: 218 | Loss: 0.34524 |\n",
      "Epoch: 219 | Loss: 0.34525 |\n",
      "Epoch: 220 | Loss: 0.34331 |\n",
      "Epoch: 221 | Loss: 0.34896 |\n",
      "Epoch: 222 | Loss: 0.34297 |\n",
      "Epoch: 223 | Loss: 0.34855 |\n",
      "Epoch: 224 | Loss: 0.34567 |\n",
      "Epoch: 225 | Loss: 0.34266 |\n",
      "Epoch: 226 | Loss: 0.34378 |\n",
      "Epoch: 227 | Loss: 0.34505 |\n",
      "Epoch: 228 | Loss: 0.34132 |\n",
      "Epoch: 229 | Loss: 0.34336 |\n",
      "Epoch: 230 | Loss: 0.34430 |\n",
      "Epoch: 231 | Loss: 0.34428 |\n",
      "Epoch: 232 | Loss: 0.34333 |\n",
      "Epoch: 233 | Loss: 0.34335 |\n",
      "Epoch: 234 | Loss: 0.34230 |\n",
      "Epoch: 235 | Loss: 0.33959 |\n",
      "Epoch: 236 | Loss: 0.34821 |\n",
      "Epoch: 237 | Loss: 0.34753 |\n",
      "Epoch: 238 | Loss: 0.34507 |\n",
      "Epoch: 239 | Loss: 0.34190 |\n",
      "Epoch: 240 | Loss: 0.34432 |\n",
      "Epoch: 241 | Loss: 0.34491 |\n",
      "Epoch: 242 | Loss: 0.34580 |\n",
      "Epoch: 243 | Loss: 0.34317 |\n",
      "Epoch: 244 | Loss: 0.34582 |\n",
      "Epoch: 245 | Loss: 0.34037 |\n",
      "Epoch: 246 | Loss: 0.34051 |\n",
      "Epoch: 247 | Loss: 0.34378 |\n",
      "Epoch: 248 | Loss: 0.34039 |\n",
      "Epoch: 249 | Loss: 0.34660 |\n",
      "Epoch: 250 | Loss: 0.34319 |\n",
      "Epoch: 251 | Loss: 0.34415 |\n",
      "Epoch: 252 | Loss: 0.34620 |\n",
      "Epoch: 253 | Loss: 0.34535 |\n",
      "Epoch: 254 | Loss: 0.33942 |\n",
      "Epoch: 255 | Loss: 0.34770 |\n",
      "Epoch: 256 | Loss: 0.33980 |\n",
      "Epoch: 257 | Loss: 0.34455 |\n",
      "Epoch: 258 | Loss: 0.34127 |\n",
      "Epoch: 259 | Loss: 0.34137 |\n",
      "Epoch: 260 | Loss: 0.33958 |\n",
      "Epoch: 261 | Loss: 0.34252 |\n",
      "Epoch: 262 | Loss: 0.34365 |\n",
      "Epoch: 263 | Loss: 0.33982 |\n",
      "Epoch: 264 | Loss: 0.33965 |\n",
      "Epoch: 265 | Loss: 0.34278 |\n",
      "Epoch: 266 | Loss: 0.34485 |\n",
      "Epoch: 267 | Loss: 0.34585 |\n",
      "Epoch: 268 | Loss: 0.34165 |\n",
      "Epoch: 269 | Loss: 0.34192 |\n",
      "Epoch: 270 | Loss: 0.34040 |\n",
      "Epoch: 271 | Loss: 0.33885 |\n",
      "Epoch: 272 | Loss: 0.34482 |\n",
      "Epoch: 273 | Loss: 0.34549 |\n",
      "Epoch: 274 | Loss: 0.34423 |\n",
      "Epoch: 275 | Loss: 0.34151 |\n",
      "Epoch: 276 | Loss: 0.34102 |\n",
      "Epoch: 277 | Loss: 0.34297 |\n",
      "Epoch: 278 | Loss: 0.34094 |\n",
      "Epoch: 279 | Loss: 0.34097 |\n",
      "Epoch: 280 | Loss: 0.34045 |\n",
      "Epoch: 281 | Loss: 0.33887 |\n",
      "Epoch: 282 | Loss: 0.34230 |\n",
      "Epoch: 283 | Loss: 0.33769 |\n",
      "Epoch: 284 | Loss: 0.34336 |\n",
      "Epoch: 285 | Loss: 0.34198 |\n",
      "Epoch: 286 | Loss: 0.33691 |\n",
      "Epoch: 287 | Loss: 0.34125 |\n",
      "Epoch: 288 | Loss: 0.33823 |\n",
      "Epoch: 289 | Loss: 0.34124 |\n",
      "Epoch: 290 | Loss: 0.34129 |\n",
      "Epoch: 291 | Loss: 0.34002 |\n",
      "Epoch: 292 | Loss: 0.33744 |\n",
      "Epoch: 293 | Loss: 0.34293 |\n",
      "Epoch: 294 | Loss: 0.34208 |\n",
      "Epoch: 295 | Loss: 0.34120 |\n",
      "Epoch: 296 | Loss: 0.34012 |\n",
      "Epoch: 297 | Loss: 0.34101 |\n",
      "Epoch: 298 | Loss: 0.34094 |\n",
      "Epoch: 299 | Loss: 0.34325 |\n",
      "Epoch: 300 | Loss: 0.34118 |\n",
      "Epoch: 301 | Loss: 0.34444 |\n",
      "Epoch: 302 | Loss: 0.33714 |\n",
      "Epoch: 303 | Loss: 0.33854 |\n",
      "Epoch: 304 | Loss: 0.33977 |\n",
      "Epoch: 305 | Loss: 0.34167 |\n",
      "Epoch: 306 | Loss: 0.33791 |\n",
      "Test Accuracy 0.878\n"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=40,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62620a38-33b3-4606-a14a-db6060ed97c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:04:03.060479Z",
     "iopub.status.busy": "2022-06-04T15:04:03.060365Z",
     "iopub.status.idle": "2022-06-04T15:05:34.477554Z",
     "shell.execute_reply": "2022-06-04T15:05:34.476602Z",
     "shell.execute_reply.started": "2022-06-04T15:04:03.060464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.230341315 -0.290073931 -0.265134513 0.0122451186 -0.234274849 0.0229974687 -0.0359121859 -0.160120264 -0.253641635 0.0109185278 0.264867 -0.0410477817 0.00220483541 0.255836725 -0.017172426 0.196480244 0.170074552 0.259053111 0.00863462687 0.206366539 -0.206700683 0.0341176689 -0.0680171251 -0.0647429228 0.120285153 -0.275357276 0.00239357352 -0.0682737082 0.102100372 0.288813055 0.0274158418 0.244256496 0.186001599 0.029227972 -0.226596504]\n",
      "[0.256135583 0.0570820272 -0.140065849 -0.265346497 -0.0383561552 -0.279242098 0.114590496 -0.160129622 0.250803411 -0.226013869 -0.100860551 -0.000622838736 -0.274319649 -0.0326150954 -0.00306400657 -0.243238807 -0.0329400897 0.135204017 -0.194108516 -0.114808559 -0.128485322 -0.20707804 -0.277358353 -0.236792535 0.107991159 -0.0141676664 -0.135069653 -0.000762105 0.000439673662 -0.0071644485 0.243114769 0.0424708426 -0.104558229 -0.0628411 0.212780595]\n",
      "[0.41184479 0.347100198 -0.502791405 0.129827738 0.468471467 -0.0732898712 0.269770801 0.351515889]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ad9dddcffe499b94b63c914bc57ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Loss: 0.74299 |\n",
      "Epoch: 01 | Loss: 0.72826 |\n",
      "Epoch: 02 | Loss: 0.71319 |\n",
      "Epoch: 03 | Loss: 0.69539 |\n",
      "Epoch: 04 | Loss: 0.67779 |\n",
      "Epoch: 05 | Loss: 0.66187 |\n",
      "Epoch: 06 | Loss: 0.64740 |\n",
      "Epoch: 07 | Loss: 0.63489 |\n",
      "Epoch: 08 | Loss: 0.62193 |\n",
      "Epoch: 09 | Loss: 0.60977 |\n",
      "Epoch: 10 | Loss: 0.59580 |\n",
      "Epoch: 11 | Loss: 0.58325 |\n",
      "Epoch: 12 | Loss: 0.57173 |\n",
      "Epoch: 13 | Loss: 0.56155 |\n",
      "Epoch: 14 | Loss: 0.54962 |\n",
      "Epoch: 15 | Loss: 0.54021 |\n",
      "Epoch: 16 | Loss: 0.53216 |\n",
      "Epoch: 17 | Loss: 0.52412 |\n",
      "Epoch: 18 | Loss: 0.51492 |\n",
      "Epoch: 19 | Loss: 0.50763 |\n",
      "Epoch: 20 | Loss: 0.50130 |\n",
      "Epoch: 21 | Loss: 0.49350 |\n",
      "Epoch: 22 | Loss: 0.48797 |\n",
      "Epoch: 23 | Loss: 0.48162 |\n",
      "Epoch: 24 | Loss: 0.47680 |\n",
      "Epoch: 25 | Loss: 0.47269 |\n",
      "Epoch: 26 | Loss: 0.46577 |\n",
      "Epoch: 27 | Loss: 0.46136 |\n",
      "Epoch: 28 | Loss: 0.45699 |\n",
      "Epoch: 29 | Loss: 0.45272 |\n",
      "Epoch: 30 | Loss: 0.44699 |\n",
      "Epoch: 31 | Loss: 0.44236 |\n",
      "Epoch: 32 | Loss: 0.43929 |\n",
      "Epoch: 33 | Loss: 0.43586 |\n",
      "Epoch: 34 | Loss: 0.43399 |\n",
      "Epoch: 35 | Loss: 0.42776 |\n",
      "Epoch: 36 | Loss: 0.42611 |\n",
      "Epoch: 37 | Loss: 0.42287 |\n",
      "Epoch: 38 | Loss: 0.41980 |\n",
      "Epoch: 39 | Loss: 0.41798 |\n",
      "Epoch: 40 | Loss: 0.41456 |\n",
      "Epoch: 41 | Loss: 0.41269 |\n",
      "Epoch: 42 | Loss: 0.40719 |\n",
      "Epoch: 43 | Loss: 0.40724 |\n",
      "Epoch: 44 | Loss: 0.40608 |\n",
      "Epoch: 45 | Loss: 0.40155 |\n",
      "Epoch: 46 | Loss: 0.40077 |\n",
      "Epoch: 47 | Loss: 0.39941 |\n",
      "Epoch: 48 | Loss: 0.39819 |\n",
      "Epoch: 49 | Loss: 0.39445 |\n",
      "Epoch: 50 | Loss: 0.39252 |\n",
      "Epoch: 51 | Loss: 0.39214 |\n",
      "Epoch: 52 | Loss: 0.38920 |\n",
      "Epoch: 53 | Loss: 0.38575 |\n",
      "Epoch: 54 | Loss: 0.38729 |\n",
      "Epoch: 55 | Loss: 0.38613 |\n",
      "Epoch: 56 | Loss: 0.38460 |\n",
      "Epoch: 57 | Loss: 0.38295 |\n",
      "Epoch: 58 | Loss: 0.38302 |\n",
      "Epoch: 59 | Loss: 0.38307 |\n",
      "Epoch: 60 | Loss: 0.37854 |\n",
      "Epoch: 61 | Loss: 0.37971 |\n",
      "Epoch: 62 | Loss: 0.37760 |\n",
      "Epoch: 63 | Loss: 0.37890 |\n",
      "Epoch: 64 | Loss: 0.37417 |\n",
      "Epoch: 65 | Loss: 0.37550 |\n",
      "Epoch: 66 | Loss: 0.37446 |\n",
      "Epoch: 67 | Loss: 0.37175 |\n",
      "Epoch: 68 | Loss: 0.37113 |\n",
      "Epoch: 69 | Loss: 0.37153 |\n",
      "Epoch: 70 | Loss: 0.36967 |\n",
      "Epoch: 71 | Loss: 0.37141 |\n",
      "Epoch: 72 | Loss: 0.36644 |\n",
      "Epoch: 73 | Loss: 0.36712 |\n",
      "Epoch: 74 | Loss: 0.36581 |\n",
      "Epoch: 75 | Loss: 0.36822 |\n",
      "Epoch: 76 | Loss: 0.36502 |\n",
      "Epoch: 77 | Loss: 0.36207 |\n",
      "Epoch: 78 | Loss: 0.36416 |\n",
      "Epoch: 79 | Loss: 0.36131 |\n",
      "Epoch: 80 | Loss: 0.36333 |\n",
      "Epoch: 81 | Loss: 0.36000 |\n",
      "Epoch: 82 | Loss: 0.35938 |\n",
      "Epoch: 83 | Loss: 0.36117 |\n",
      "Epoch: 84 | Loss: 0.35923 |\n",
      "Epoch: 85 | Loss: 0.35977 |\n",
      "Epoch: 86 | Loss: 0.35768 |\n",
      "Epoch: 87 | Loss: 0.35849 |\n",
      "Epoch: 88 | Loss: 0.35920 |\n",
      "Epoch: 89 | Loss: 0.35872 |\n",
      "Epoch: 90 | Loss: 0.35913 |\n",
      "Epoch: 91 | Loss: 0.35799 |\n",
      "Epoch: 92 | Loss: 0.35465 |\n",
      "Epoch: 93 | Loss: 0.35327 |\n",
      "Epoch: 94 | Loss: 0.35784 |\n",
      "Epoch: 95 | Loss: 0.35704 |\n",
      "Epoch: 96 | Loss: 0.35879 |\n",
      "Epoch: 97 | Loss: 0.35667 |\n",
      "Epoch: 98 | Loss: 0.35580 |\n",
      "Epoch: 99 | Loss: 0.35324 |\n",
      "Epoch: 100 | Loss: 0.35377 |\n",
      "Epoch: 101 | Loss: 0.35597 |\n",
      "Epoch: 102 | Loss: 0.35236 |\n",
      "Epoch: 103 | Loss: 0.35391 |\n",
      "Epoch: 104 | Loss: 0.35012 |\n",
      "Epoch: 105 | Loss: 0.34999 |\n",
      "Epoch: 106 | Loss: 0.35313 |\n",
      "Epoch: 107 | Loss: 0.35284 |\n",
      "Epoch: 108 | Loss: 0.35058 |\n",
      "Epoch: 109 | Loss: 0.35128 |\n",
      "Epoch: 110 | Loss: 0.35164 |\n",
      "Epoch: 111 | Loss: 0.35080 |\n",
      "Epoch: 112 | Loss: 0.34988 |\n",
      "Epoch: 113 | Loss: 0.35291 |\n",
      "Epoch: 114 | Loss: 0.34891 |\n",
      "Epoch: 115 | Loss: 0.35120 |\n",
      "Epoch: 116 | Loss: 0.35074 |\n",
      "Epoch: 117 | Loss: 0.34748 |\n",
      "Epoch: 118 | Loss: 0.35253 |\n",
      "Epoch: 119 | Loss: 0.34640 |\n",
      "Epoch: 120 | Loss: 0.35056 |\n",
      "Epoch: 121 | Loss: 0.34669 |\n",
      "Epoch: 122 | Loss: 0.34797 |\n",
      "Epoch: 123 | Loss: 0.34956 |\n",
      "Epoch: 124 | Loss: 0.34976 |\n",
      "Epoch: 125 | Loss: 0.34634 |\n",
      "Epoch: 126 | Loss: 0.35053 |\n",
      "Epoch: 127 | Loss: 0.34757 |\n",
      "Epoch: 128 | Loss: 0.34486 |\n",
      "Epoch: 129 | Loss: 0.34551 |\n",
      "Epoch: 130 | Loss: 0.34583 |\n",
      "Epoch: 131 | Loss: 0.34921 |\n",
      "Epoch: 132 | Loss: 0.34563 |\n",
      "Epoch: 133 | Loss: 0.34558 |\n",
      "Epoch: 134 | Loss: 0.34410 |\n",
      "Epoch: 135 | Loss: 0.34440 |\n",
      "Epoch: 136 | Loss: 0.34324 |\n",
      "Epoch: 137 | Loss: 0.34727 |\n",
      "Epoch: 138 | Loss: 0.34707 |\n",
      "Epoch: 139 | Loss: 0.34443 |\n",
      "Epoch: 140 | Loss: 0.34790 |\n",
      "Epoch: 141 | Loss: 0.34000 |\n",
      "Epoch: 142 | Loss: 0.33999 |\n",
      "Epoch: 143 | Loss: 0.33942 |\n",
      "Epoch: 144 | Loss: 0.34060 |\n",
      "Epoch: 145 | Loss: 0.33837 |\n",
      "Epoch: 146 | Loss: 0.33717 |\n",
      "Epoch: 147 | Loss: 0.33916 |\n",
      "Epoch: 148 | Loss: 0.33677 |\n",
      "Epoch: 149 | Loss: 0.33670 |\n",
      "Epoch: 150 | Loss: 0.33719 |\n",
      "Epoch: 151 | Loss: 0.33402 |\n",
      "Epoch: 152 | Loss: 0.33512 |\n",
      "Epoch: 153 | Loss: 0.33480 |\n",
      "Epoch: 154 | Loss: 0.33446 |\n",
      "Epoch: 155 | Loss: 0.33898 |\n",
      "Epoch: 156 | Loss: 0.33798 |\n",
      "Epoch: 157 | Loss: 0.33079 |\n",
      "Epoch: 158 | Loss: 0.33310 |\n",
      "Epoch: 159 | Loss: 0.33471 |\n",
      "Epoch: 160 | Loss: 0.33925 |\n",
      "Epoch: 161 | Loss: 0.33337 |\n",
      "Epoch: 162 | Loss: 0.33198 |\n",
      "Epoch: 163 | Loss: 0.33211 |\n",
      "Epoch: 164 | Loss: 0.33323 |\n",
      "Epoch: 165 | Loss: 0.33347 |\n",
      "Epoch: 166 | Loss: 0.32958 |\n",
      "Epoch: 167 | Loss: 0.33223 |\n",
      "Epoch: 168 | Loss: 0.33083 |\n",
      "Epoch: 169 | Loss: 0.32934 |\n",
      "Epoch: 170 | Loss: 0.33029 |\n",
      "Epoch: 171 | Loss: 0.32855 |\n",
      "Epoch: 172 | Loss: 0.33076 |\n",
      "Epoch: 173 | Loss: 0.32735 |\n",
      "Epoch: 174 | Loss: 0.33235 |\n",
      "Epoch: 175 | Loss: 0.32781 |\n",
      "Epoch: 176 | Loss: 0.32968 |\n",
      "Epoch: 177 | Loss: 0.33030 |\n",
      "Epoch: 178 | Loss: 0.32739 |\n",
      "Epoch: 179 | Loss: 0.32902 |\n",
      "Epoch: 180 | Loss: 0.32566 |\n",
      "Epoch: 181 | Loss: 0.32823 |\n",
      "Epoch: 182 | Loss: 0.32878 |\n",
      "Epoch: 183 | Loss: 0.32644 |\n",
      "Epoch: 184 | Loss: 0.32717 |\n",
      "Epoch: 185 | Loss: 0.33183 |\n",
      "Epoch: 186 | Loss: 0.32661 |\n",
      "Epoch: 187 | Loss: 0.32689 |\n",
      "Epoch: 188 | Loss: 0.32995 |\n",
      "Epoch: 189 | Loss: 0.32785 |\n",
      "Epoch: 190 | Loss: 0.32703 |\n",
      "Epoch: 191 | Loss: 0.32433 |\n",
      "Epoch: 192 | Loss: 0.32755 |\n",
      "Epoch: 193 | Loss: 0.32540 |\n",
      "Epoch: 194 | Loss: 0.32795 |\n",
      "Epoch: 195 | Loss: 0.32784 |\n",
      "Epoch: 196 | Loss: 0.32612 |\n",
      "Epoch: 197 | Loss: 0.32897 |\n",
      "Epoch: 198 | Loss: 0.33191 |\n",
      "Epoch: 199 | Loss: 0.32293 |\n",
      "Epoch: 200 | Loss: 0.32611 |\n",
      "Epoch: 201 | Loss: 0.32374 |\n",
      "Epoch: 202 | Loss: 0.32853 |\n",
      "Epoch: 203 | Loss: 0.33037 |\n",
      "Epoch: 204 | Loss: 0.32822 |\n",
      "Epoch: 205 | Loss: 0.32938 |\n",
      "Epoch: 206 | Loss: 0.32775 |\n",
      "Epoch: 207 | Loss: 0.32814 |\n",
      "Epoch: 208 | Loss: 0.32802 |\n",
      "Epoch: 209 | Loss: 0.32832 |\n",
      "Epoch: 210 | Loss: 0.32634 |\n",
      "Epoch: 211 | Loss: 0.32619 |\n",
      "Epoch: 212 | Loss: 0.32831 |\n",
      "Epoch: 213 | Loss: 0.32752 |\n",
      "Epoch: 214 | Loss: 0.32444 |\n",
      "Epoch: 215 | Loss: 0.32737 |\n",
      "Epoch: 216 | Loss: 0.32540 |\n",
      "Epoch: 217 | Loss: 0.32686 |\n",
      "Epoch: 218 | Loss: 0.32386 |\n",
      "Epoch: 219 | Loss: 0.32649 |\n",
      "Test Accuracy 0.8263333333333334\n"
     ]
    }
   ],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95cdf41-fba8-447f-838e-c8a6689590a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T15:05:34.478776Z",
     "iopub.status.busy": "2022-06-04T15:05:34.478657Z",
     "iopub.status.idle": "2022-06-04T15:05:36.166851Z",
     "shell.execute_reply": "2022-06-04T15:05:36.165931Z",
     "shell.execute_reply.started": "2022-06-04T15:05:34.478761Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2619277/1705769632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35199286-0e11-4701-88fc-14752b0f9397",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.167642Z",
     "iopub.status.idle": "2022-06-04T15:05:36.167943Z",
     "shell.execute_reply": "2022-06-04T15:05:36.167797Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.167780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=0)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a22db9-4fcd-406a-bd22-573ba8ecdd14",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.169075Z",
     "iopub.status.idle": "2022-06-04T15:05:36.169509Z",
     "shell.execute_reply": "2022-06-04T15:05:36.169355Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.169337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt = DHDT(\n",
    "            depth=3,\n",
    "            number_of_variables = 5,\n",
    "            learning_rate=1e-3,\n",
    "            squeeze_factor = 5,\n",
    "            loss='binary_crossentropy',#'binary_crossentropy',\n",
    "            random_seed=41,\n",
    "            verbosity=1)\n",
    "\n",
    "model_dhdt.fit(X_train, y_train, batch_size=64, epochs=500, early_stopping_epochs=20)\n",
    "\n",
    "y_test_model = model_dhdt.predict(X_test)\n",
    "score_dhdt = accuracy_score(y_test, np.round(y_test_model))\n",
    "\n",
    "print('Test Accuracy', score_dhdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0dfba-21f2-4c6b-9c34-e836e7dadf9d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.170282Z",
     "iopub.status.idle": "2022-06-04T15:05:36.170554Z",
     "shell.execute_reply": "2022-06-04T15:05:36.170419Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.170404Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "image = model_dhdt.plot()\n",
    "display(image)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(model_sklearn, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bce89-e3d1-474d-9df3-21cbc4f70194",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.171422Z",
     "iopub.status.idle": "2022-06-04T15:05:36.171777Z",
     "shell.execute_reply": "2022-06-04T15:05:36.171628Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.171610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a9767-c75c-46e9-ac03-6dc2831e602f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.172733Z",
     "iopub.status.idle": "2022-06-04T15:05:36.173124Z",
     "shell.execute_reply": "2022-06-04T15:05:36.172974Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.172956Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt.dt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46c63a-a485-4d91-9371-1e968aa5c0e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.173847Z",
     "iopub.status.idle": "2022-06-04T15:05:36.174110Z",
     "shell.execute_reply": "2022-06-04T15:05:36.173978Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.173964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.sigmoid(model_dhdt.dt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2019e-609d-4f52-93d6-29fa5934f320",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.174958Z",
     "iopub.status.idle": "2022-06-04T15:05:36.175229Z",
     "shell.execute_reply": "2022-06-04T15:05:36.175092Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.175077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dhdt.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba167628-4a58-47e8-92c6-d56274e650d6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-04T15:05:36.176112Z",
     "iopub.status.idle": "2022-06-04T15:05:36.176412Z",
     "shell.execute_reply": "2022-06-04T15:05:36.176271Z",
     "shell.execute_reply.started": "2022-06-04T15:05:36.176255Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "image = model_dhdt.plot()\n",
    "display(image)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(model_sklearn, fontsize=10) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32042bc9-1008-4311-8d2d-7896db8a3a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
